[{"id":0,"href":"/2025_ossfruehstueck/","title":"Netzwerkfrühstück: Open Source in der Verwaltung","section":"Inductive Bias","content":" Netzwerkfrühstück: Open Source in der Verwaltung # Networking breakfast: Open Source in public admin (at German \u0026ldquo;Smart country convention\u0026rdquo; Berlin).\nBack in August, in the middle of my vacation, I got an e-mail from plain schwarz asking if I would be willing to give a brief talk at an event they were organising about open source. All I needed to do was get back to them within a week and coordinate in a quick call. Luckily we were in the middle of no-where in Denmark - so internet access was easy. Luckily also, my family was very understanding about me taking a conference call in the middle of no-where in between time at the beach.\nCutting a long story short: Thank you plain schwarz for inviting me to give a ten minute brief mini talk about the three pillars of open source: How open source projects differ beyond license - and how informed use leads to participation leads to being an active player in the ecosystem.\nThe breakfast was a collaboration between City Lab Berlin and FOSS Backstage. For full disclosure: Several years ago, together with Stefan Rudnitzki I carried the vague nucleus of an idea to plain schwarz of what turned into an amazing conference on all things open source behind the scenes: Think governance, finance, legal, collaboration, corporate shenanigans, documentation, security etc.\nThe entire breakfast started with an incredibly helpful ice breaker question: All attendees were asked to stand up, sort themselves into one line ranging from \u0026ldquo;user of open source\u0026rdquo; via \u0026ldquo;contributor to open source\u0026rdquo; towards \u0026ldquo;shaping the ecosystem, e.g. as maintainer\u0026rdquo;. To be fully honest: Even though the event was bound to draw proponents of open source in public admin, I was surprised by the number of people who were not only using open source, but were also contributing to projects - up to the level of influencing the ecosystem. Sure, compared to the entire convention spanning multiple halls at Messe Berlin (the largest exhibition space in Berlin) the group was a small percentage. And still it was several dozen people who knew about the value and who are already very actively part of the ecosystem.\nI really appreciated hearing stories about first time contributions to Apache Airflow and Superset. It was awesome seeing the strategy of the city of Munich that couples donations, paying for features, giving time to people on their pay roll and handing out paid OSS sabbaticals for experts from the private sector and publishing custom software under free and open source licenses. Last but not least Citylab talked about the projects that they are working on.\nI\u0026rsquo;ll be sharing more about the ideas that I had to condense down to ten minutes in upcoming blog posts. So far I would like to say thank you for having me as a speaker, for a lot of inspiring conversations and looking forward to seeing those conversations continued at FOSS Backstage next year.\n"},{"id":1,"href":"/llm-tool-locally/","title":"Playing with LLMs locally","section":"Inductive Bias","content":" Playing with LLMs locally # After a talk by Nick Burch at Berlin Buzzwords I finally got started playing with LLMs locally. The easiest way to get started for me: llm tool by Simon Willison :)\nBackstory # Back when it came out in winter 2022 I started playing around with ChatGPT - mostly to generate texts from individual terms given as what teachers in German primary schools call \u0026ldquo;learning words\u0026rdquo; - usually they are handed out for pupils to get prepared for a dictation (written to check spelling skills): It\u0026rsquo;s a lot more fun to prepare for theses tests with fun texts than with mere lists of words :) Back then the results were impressive - but also frightning given the implications for ease of generating mis-information, fake social media profiles, spamming search indeces and more.\nSince then I\u0026rsquo;ve been on the edge with LLMs: Generated texts and images all looked like polished marketing material optimised for performance on social media - but lacked all human liveliness. Summaries on content was pretty much hit and miss: While for short videos it looked good, for longer ones models quickly went off the rails mixing in content that seemed to come from the training corpus but not from the piece to summarize. Asked to provide a summary of a book providing only the book title seemed to work OK for very popular English language literature, but fails for obvious reasons for niche German stuff - with the usual tendency of models to make things up rather than display a warning about the lower liklyhood of the generated text.\nSwitching to locally on the commandline # Nick\u0026rsquo;s talk did a great job providing an easy to understand intro to the ins and outs of LLMs for running them locally as an engineer. The most helpful slide for me: The one pointing to llm tool - a commandline tool, written in Python, so easy to install e.g. through pipx. Using that several popular APIs can be accessed from the commandline, in addition several popular local LLMs have been integrated, so switching back and forth really is a matter of changing commandline options.\n"},{"id":2,"href":"/2025_04_pyconde/","title":"PyCon/ PyData DE - Invited to give a keynote","section":"Inductive Bias","content":" How I was invited to give a keynote in Darmstadt # It was autumn 2009 at an ApacheCon in Oakland/ California: I asked one of my fellow ASF people what it would take to get him to visit Berlin. The flippant, not quite serious, joking answer: \u0026ldquo;Someone to fund for my flight tickets.\u0026rdquo; Fast forward six months. I got back to that lovely person with a newly created Berlin Buzzwords conference and enough sponsorship budget to pay for keynote speaker travel. The result? A first time keynote given at an inaugural Berlin Buzzwords for that person.\nFast forward 15 years. That same person reaches out to me to connect me with one of their mentees. After several lovely conversations, that same mentee Anja Pilz asks me if I would be open to give a keynote at PyCon/PyData 2025 in Darmstadt. My flippant, no quite serious, joking answer? \u0026ldquo;The conference is in the middle of Berlin Easter holidays. Can\u0026rsquo;t you find someone to host a Python workshop for kids?\u0026rdquo; The instant answer: \u0026ldquo;We\u0026rsquo;ve always wanted to do that, let me check what we can do.\u0026rdquo; And low and behold: First-Ever Kids\u0026rsquo; Coding Workshop at PyCon DE \u0026amp; PyData 2025: Explore the World of Zümi!. Karma?\nBut there\u0026rsquo;s a tiny lesson in there that relates to the content of my keynote: Never underestimate the impact that you personally can have to make a difference out there. And never underestimate the willingness of others to help you if they understand and support your cause.\nWith that a huge THANK YOU to the organisers of PyCon/PyData DE 2025 - in particular Anja Pilz who sent the invitation and gracefully dealt with my answer. Thank you also to inovex who kindly sponsored the Mini Pythonistas. Thank you to the workshop educators. And thank you to everyone else who made the event a lovely experience!\n"},{"id":3,"href":"/bundestag/","title":"Visiting the Bundestag","section":"Inductive Bias","content":" Visiting (and speaking at) the German Bundestag # Many, many years ago, I invited Berlin Buzzwords keynote speakers to join me for lunch pre-conference. For the view – and the impact of the location – I reserved a table at the restaurant that is located atop the Reichstags-building here in Berlin. If back then anyone had told me that one day I’d get an invitation to speak as an expert in the committee for digital issues of the German Bundestag, I wouldn’t have believed a single word!\nThe invitation # Fast forward some 15 years: I’m sitting at my desk, checking my personal mails some time in November 2024 – only to find an invitation letter to the hearing on Open Source. The instructions: Answer a catalog of questions around Open Source (“Yeah, sure, no problem with that.”), attend the hearing at the Bundestag (“Oook – that’s half an hour by public transport, not problem”), give a 5 minute verbal statement (“Wait, what? Are you kidding me?”), and afterwards answer questions of the representatives (“Sure, happy to”). After reading that (and making sure it’s not spam or phishing or else) I needed a bit of fresh air. I asked who told them my name – the answer confirmed the hunch I had: .oO(Ok, if that person thinks, I’m the right one to do that, …). I consulted my family in the evening, receiving a “You should totally do that mommy” – putting me in the “great, now you’re a role model and your behaviour will influence at least one child’s view” position.\nCutting the story short: I sent my acceptance, received the questionnaire and formal invitation shortly after that – it took me another two days to figure out that the most important information of that invitation was contained in the attached pdf documents, not in the message text itself – oh, yeah, that already had been weird when booking a slot to visit the Bundestag dome as a visitor.\nBackground # So what was the hearing about? In early 2024 during FOSS Backstage Jacob Green had asked me, what I expected public admin to do wrt. Open Source. Could be procuring more software under open source licenses, could be providing funding to software infrastructure especially that which is underfunded by private corporations.\nFor the first task, think public money, public code by the FSFE. Or in a more child friendly way, think of the book Ada und Zangemann – if you haven’t done so already, go do that now – I’ll wait here in the meantime until you come back.\n…\n…\n“Ausschuss für Digitales” you said? If you’re not from Germany but speak a tiny bit of German (or can make your AI translate videos): The Bundestag has a lovely explanation of how that construct operates on the site Kuppelkucker for kids. (Kids pages have the added benefit that not only do they start from the basics requiring no prior knowledge – they also come with very easy to understand German language ;) ).\nPreparation # For the catalogue of questions: It took me about two evenings to get to a first rough draft of the answers. Polished and refined in the following days. Thanks to every expert I met in the past 20 years (Yiiiiks – 20?!? I’m not getting old, not at all, nope!) - and in particular thanks to everyone at FOSS Backstage who shared their wisdom! Having access to the recordings of FOSS Backstage also meant that for questions who were outside of my own area of expertise I could share pointers to presentations given by people much more knowledgable than me.\nReaching out # On Thursday ahead of the meeting I sent my draft to a bunch of friends, some of which pointed me to a few other usual suspects that I had forgotten to include. As a result, this invitation gave me the opportunity to again work with people I hadn’t met for ages.\nAfter a lot of silence, comments started to fly in on Friday evening, peaking on Saturday evening – on the weekend that I had reserved for baking cookies with my mom and daughter. The cookie baking exercise at least made me take regular breaks of an hour or two – after which waves of comments often settled down, so I could address them top to bottom without more flying in at the same time from the top :)\nSunday evening, back home in Berlin, I finalized everything and submitted it. Confirmation came on Monday, obviously not during the weekend.\nLogistics # At some point in time a kind human pointed out that my child would be allowed to get onto the guest gallery of the room if accompanied by an adult (not me for obvious reasons, not her Dad – he had another group of Open Source policy experts in his office that same day). During the flurry of trying to find another kind and brave human with time at their hands on short notice there’s two things I learnt:\na) Getting a “no” quickly is much more helpful than getting a “I’ll try” followed by a “sorry, can’t make it” days later. The quick “no” responses helped me to quickly move to the next person.\nb) I went through people in my Berlin tech bubble, narrowed down to those I trust for obvious reasons. Feedback from those who did join (adults who had last seen our Bundestag as kids, even though working literally next door): The experience did have an impact on them, did make them think about how they can become more active. Occasionally we pride ourselves as having nothing to do with politics – that’s not only a very privileged thing to say, it’s also one cause why on the other hand we tend to find reason to roll our eyes when looking at the state of digital in public admin.\nThe evening itself # To watch the recording of the evening yourself, checkout the official page online.\nA choreographed ballet dance of a meeting # If you’ve watched the kids movie above on Kuppelkucker you already know roughly what the room the meeting takes place in looks like. You also know roughly the outline of the meeting. To recap:\nEssentially the entire meeting is very strictly organised to allow for sticking to the schedule and to allow for giving air time to participants according to the will of voters. That’s where my comparison to a ballet choreography comes from: The frame is very well defined, it’s brought to life by the people participating.\nFor a public hearing there’s transparency for the people in that visitors can participate in person from the visitors’ gallery – no chance to speak or ask questions from there, following the discussion in person does make a difference though. People from all over the world can participate online because the entire meeting is streamed. In the background notetakers are taking care that everything that’s being said and done ends up on the protocol. Side note: You remember that my child was sitting up on the visitors’ gallery? One of my friends who went there with her was really nervous, the little toy cat she brought would decide to jump down onto the desk underneath – hearing about the people writing the protocol she told me after a school trip to the Bundestag, the first question to pop up in my head: “I wonder, if the cat “jumping” down would also have made it into the protoc… “ ;)\nPutting jokes aside: The agenda of the meeting is pretty much as follows:\nIn a first round, all experts are asked to give a five minute statement. In our case in alphabetical order by surname. There’s a big cube in the center of the room with four screens attached to call remote people in. Those five minutes are being counted down for everyone to see on the lower right corner of each screen for everyone to see. One minute before the end of the slot a bell rings. If the speaker goes over the five minutes, the counter turns red. If they still don’t stop shortly after that the meeting chair will intervene.\nIn the second round representatives of elected parties can ask questions. Order is determined by size of elected members of that party in parliament. Each one has a five minute slot for question-answer interactions. Typically those questions first go to the expert they invited (if they did invite one), then to others. The answers to the questionnaire submitted before are one basis for formulating questions. Another one is knowledge about the area of expertise and experience of each of the experts.\nThe third round is a repeat of the second one. Experts in my case were expected to be available to answer questions not only from the party that invited them but provide their expertise to the entire room.\nThe coordination that is not visible on the recording: In many cases, answers to the questions are the result of the input of more than the expert invited. It is not unlikely that experts know each other - if they do, it is likely that the communicate before the hearing - and go to dinner afterwards, even if they disagree on details. The points and references made do not necessarily reflect the personal opinion of the speaker, but may mirror the way topics are discussed in groups deemed relevant by the speaker.\nLearnings # Democracy is about arguing about the best way forward – there are ways to structure discourse such that it happens in a productive, civilized manner. At least some of the experts in the room were there not as individuals but representing their organisation. There is no way they could possibly argue differently – they are there to represent their role. Mixing that up would only complicate the message. Each role is important. Often we only hear quotations taken out of the context of the entire meeting and get emotional about those. Watching the entire meeting unfold though tells very complex and interesting story about not only the topic, but also interests that participating individuals and organisations have. Final invitation # Political leaders also have a day that has only 24 hours. We can mock them not getting what we do on a daily basis – but that won’t change anything. If we expect different outcomes, we need to participate – not only at the level of the Bundestag but also at local levels. It took me ages to write an e-mail to local admin with a question I had – I got instant feedback, and a friendly smile for the warm thank you for the speedy response.\nAnd, if you speak German: Checkout the Bundestag resources – even if just as an educational exercise – or as a test how good your command of the German language really is :)\nParting thoughts # A couple days after the meeting I got feedback that explaining instead of expecting people to know even an Open Source foundation as old, large and impactful as Apache was a positive surprise. I\u0026rsquo;ve been a part of that foundation for long enough to know that no matter how large you think you are, there will always be people who didn\u0026rsquo;t hear about it before - or hear about it in a much smaller context than it\u0026rsquo;s active in. If you bump into people who still divvy people up into tech and non-tech I would like to urge you to watch the recording of a talk that Leslie Hawthorn gave many, many years ago - but that I personally still find highly valuable.\n"},{"id":4,"href":"/learning-english/","title":"Learning German in Berlin","section":"Inductive Bias","content":" Learning German in Berlin # So you have been living in Berlin for a while, maybe even for years – but you fail to speak German fluently. It may feel weird switching to German even with friends, as your English gives you many more options to speak proficiently, to be precise when talking. People around you speak better English than you speak German.\nOne thing to remember: While Berlin is easy to navigate speaking only English, if you stay here for more than a couple of weeks you really truly do want to start training your German language skills: Emergency information – typically first available and more detailed in German. Dealing with public admin tasks – you’ll need German. You have kids – you need German language skills for medical appointments, for public admin, for Kindergarten and school. Even just getting quick access to news articles requires at least a basic understanding of the local language. And that is true if you want to relocate here, or if you consider yourself a digital nomad.\nSo here’s a writeup of things I’ve sent to too many friends already:\nOne warning at the very beginning: You must tell people that you want to learn German – find people who are patient enough to stay in German even if you are proficient in English. Otherwise a lot of people will quickly switch back to English which doesn’t help you.\nOne encouragement at the beginning: Don\u0026rsquo;t be afraid to make mistakes. The only way to learn the language is to practice speaking and talking with people. It will feel strange, you may feel less competent, but the only way to get better is to push through that and get better. Try it not only with close friends but also in everyday situations - especially in calmer situations. And don\u0026rsquo;t worry if the people you interact with seem somewhat unfriendly, less chatty than expected - chances are very high that\u0026rsquo;s not because of you but average north-German mentality.\nIf you\u0026rsquo;re looking to combine your language learning with doing good: You might want to consider leaving tourist hotspot or expat heavy areas - a general rule of thumb is to leave the S-Bahn ring, watch out for oportunities to help people go for a walk with their dog, help elderly people stay in touch with the younger generation and combine playing cards/ board games with learning German. Chances are high you\u0026rsquo;ll find people with a combination of great German language skills, low English language skills and lots of time and patience.\nSome additions to this article from social media # I published this blog post on social media (read: Mastodon, Bluesky and LinkedIn) - back came several additions to the proposals already in the post:\nIn case your employer does not cover language courses, the ones offered by public Volkshochschulen are really good and affordable - as a result though, you need to be quick as they fill up relatively fast. As an addition to the recommendation around moving out of your Kiez: In addition to helping elderly people, you can also see if there\u0026rsquo;s a hobby you can follow and join an existing group - think pottery, knitting, playing games - anything that encourages chatting. For online video content a friend recommended the Easy German YouTube channel. Online resources to get you started # Goethe Institut operates globally, also has online resources for people learning German. Tagesschau in simple German is a simple German language version of the public (meaning independent) daily news show. Deutsche Welle has several radio shows and podcasts for German learners. Berlin library gives you access to both, paper books, online access to local newspapers, Libby and Onleihe for digital books for 10 Euro each year. All you need to do is visit a library in Berlin, get your access card. For a lot of fun stories check out the kids show Die Sendung mit der Maus. For book recommendations – also for new books – checkout Stiftung Lesen. There’s a ton of podcasts that also public news agencies make available. Take your favourite podcast app to find them.\nFor kids # Having grown up in east Germany until I was nine I realized that there’s a ton of traditional kids literature that many people take for granted, that in some cases was new to me. So here’s me list of authors you should check out and a more general page on must read books.\nStiftung Lesen above covers a ton of book recommendations also for children of different ages. You want to checkout things around the yearly book fairs.\nAda und Zangemann … a personal favorite on open source and ice cream. I’m not biased at all ;) Kindermann Verlag has lovely kids versions of famous literature. Annette Betz has a lovely collection of books to make classical music accessible to kids. Cornelia Funke … has books for all ages and is a must read. Otfried Preußler Erich Kästner Märchen der Gebrüder Grimm Das Urmel aus dem Eis Michael Ende Das Sams For more recommendations there’s an enitre list of classical kids books.\nFor more recent titles:\nDie Schule der magischen Tiere Das Neinhorn … so hilarious Die drei ??? … detective boys Die drei !!! … detective girls Was ist was … non fiction books for a ton of interesting topics. Wieso, weshalb, warum … same as above for younger children. One final parting note: One thing you can try is finding a language buddy - that could even be someone at work with the deal being you speak German on every other day, a language that you speak at native level the remaining days :)\n"},{"id":5,"href":"/notes_on_meetup_org/","title":"Notes on meetups","section":"Inductive Bias","content":" Notes on meetup organisation # What feels like decades ago I went to a talk at FOSDEM. One in a dev-room. And one of the very few that proved relevant for many years after. I have no idea what the title was, nor what year it was given, I do believe it ran in the MySQL dev room - in terms of topic it provided an overview on what to do and what to watch out for when organising community meetups. A lot of the learnings shared made me nod in the audience.\nI organised my own meetup before this term was used - instead mine was called \u0026ldquo;get together\u0026rdquo;. I started not knowing how many people would attend. I also started in a time when there were barely any meetups in Berlin, so it quickly grew from little over a dozen to several dozen attendees. I also started before hosting meetups was a cool thing to do for companies - so I was very lucky to have a location like newthinking store available for free - a bit like a co-working space, before that term was invented.\nOver time I shared my learnings on do\u0026rsquo;s and don\u0026rsquo;ts - however usual off the record on deniable communication media. I guess it\u0026rsquo;s time to write those learnings down:\nDon\u0026rsquo;t worry whether someone will attend - one or two people commit to participate is more than enough, if you start transparently with a \u0026ldquo;if nobody attends we\u0026rsquo;ll just move to a restaurant close by and exchange notes\u0026rdquo;.\nKeep costs low. One cost driver tends to be location rent. You can avoid that if you can host the meetup at your employer\u0026rsquo;s premises, alternatively companies hiring often provide space for free. A third option is to book a restaurant table or meet at university.\nAnother cost driver can be catering. You can try to only provide for something to drink, for food moving to another place after the meetup is an option in that case. Sometimes companies providing space also provide food - that needs a good attendee estimate though.\nSpeaking of providing drinks: I\u0026rsquo;m not a fan of providing alcoholic drinks for free. I have heard too many stories involving free alcohol and male attendees unable to abide by basic rules of social conduct.\nSpeaking of people unable to behave professionally: It\u0026rsquo;s common to have a code of conduct these days. Unfortunately rules printed and published are useless if not enforced. If you yourself are not comfortable doing the enforcing, seek support for that.\nSpeaking of seeking support: Never walk alone! I was literally hit by a car on my way to the second or third meetup I had organised myself and broke my knee. I was lucky that nothing worse happened - I was also lucky to quickly find someone else to take over moderation. I was also lucky location availability was not tied to me being present.\nSpeaking of backup plans: I met speakers who agreed to give a talk, but when reminded a week in advance told me they had mixed up dates and booked the flight a month later (having more than one talk, and maybe having a backup speaker in town or a discussion topic for the audience helps). I met speakers who forgot to bring their laptop/ were unable to hook their presentation to the projector/ forgot whatever device they needed to get their Mac attached to the projector (having another laptop for presentations helps for that, so does urging speakers to test equipment, so does having common adaptors handy). I had speakers who would triple their presentation time unless stopped (signs telling remaining talk time are helpful for that).\nSpeaking of growing: Make sure to minimize risk. Running a 70 person company hosted meetup that is essentially free because there are not location costs can work well. As soon as money (and sponsorship) is involved: Don\u0026rsquo;t put your personal account in between sponsor and benefit provider. Newthinking always was kind enough to directly bill those who offered to cover talk recording costs. From organising meetups and conferences I know that there are sponsors who will fail to pay they promised - typically not out of malice but either because there are no processes for that yet (think startups) or because processes in place are too complicated for tiny sums like a couple hundred Euros (think large corporations). If the thought of \u0026ldquo;In the worst case I will have to cover the cost myself\u0026rdquo; gets to scary it\u0026rsquo;s time to think about how to change organisation model.\nSpeaking of growing take 2: As your meetup grows - keep a record. Pictures (taken with consent) will help tell the story later one. Slides collected and published are not only valuable to have to make available to attendees but also to reflect on how topics have shifted over the years. Recordings kept over decades helps share important lessons - and unfortunately you only know years later what the really impactful talks have been. If you pay for just one thing: Having one memorable URL to publish information about your meetup is really helpful. Platforms can go away, make sure that moving is easy. Having one dedicated domain that is not linked to other personal stuff also makes it easy to hand over organisation should you ever want to do that.\nAnother thing that helped me: Talking with other meetup organisers. Exchange best practices, learnings, recommendations. You are not the only one - try to stand on the shoulders of others.\nOh - and the most important thing last: Often meetup organisation tends to be something that dedicated people do as a side project. If that is the case: Make sure that it\u0026rsquo;s fun for yourself. Invite people, you want to hear speak yourself. Invite talks about topics you yourself are interested in. Make sure that people you yourself want to meet are in the audience. And hand the ball back to whoever complains: I got a lot of complaints about the date or time chosen for the meetup. In my case, the rule was simple: First speaker gets to set the date and time. So whoever complains, there\u0026rsquo;s an easy way out to fix their issue themselves: Submit the next talk. (Sneak behind the scenes: I cannot remember a single complainer who ended up submitting a talk, but I remember all complainers stopping those complaints after being told that rule.) To keep things fun, also make sure to set boundaries: There is no need to answer requests instantly, mails can wait. Even if this is not a side project: There are things that are more important (think health, family\u0026hellip;) - no meetup related request is so important that it has to be answered at once. Putting messages into a separate inbox can help with that.\n"},{"id":6,"href":"/cra-isc/","title":"Open Source, InnerSource, CRA?","section":"Inductive Bias","content":" Open Source, InnerSource, CRA? # A couple days ago on Mastodon I read a post from by Robert Sander (here translated from German): \u0026ldquo;At FrOSCon I got the impression the FLOSS community is aging together. Where are people in their mid-twenties, new people entering the community?\u0026rdquo;\n\u0026ldquo;Entering the open source community\u0026rdquo; - what does that even mean? Does it mean creating a public GitHub repo and publishing source code there? For years when employers wanted to show how active employees are in Open Source they would point to projects on GitHub under their own org. Often employer branding was the most important motivation for a lot of these companies - and the weakest reason when faced with economic challenges.\nIn addition taking that path implies that developers do not benefit from being part of and learning from a community that spans multiple corporations, cultures and timezones. It also does not add more hands to the group of people supporting existing and wide spread existing open source projects.\n\u0026ldquo;But telling my colleagues to participate upstream didn\u0026rsquo;t work\u0026rdquo; - for what reason does upstream participation fail? Developers and management both lack a deep understanding of the benefits of investing working hours into a project they believed they could use for free. Developers are afraid to participate due to the public nature of open source: All mistakes, every little question will be made in public, where it\u0026rsquo;s archived, where it will surface in web searches. There are open source projects - in particular those operating according to an open core model - where outside collaboration is limited to submitting issues. For patches often only those are accepted that are in line with the business strategy of the backing entity. In addition participation there tends to end where strategy and roadmaps are decided, inherently limiting the level of impact an outsider can have - and limiting the motivation to invest time and energy.\nLooking at each of these three reasons in turn:\nBetter understanding # Outside of core open source circles - how well does your average software engineer understand the business reasons for investing precious time in open source? I would argue that very few people using open source would be able to tell even for their core dependencies who is backing these projects. How sustainable financing of the people backing these projects is setup. What the difference is between single vendor, VC backed open source vs. single vendor, bootstrapped vs. vendor neutral, foundation backed - and what that means in terms of risk assessment for anyone downstream using projects in any of these models.\nSo far other than the occasional high impact security issue being blissfully unaware of these details didn\u0026rsquo;t pose more than minor annoyances at least for your average downstream user. At the very worst case it meant having to switch to another dependency in case of a license change or end of life of a project.\nWith the CRA this changes to some extend: With a higher level of liability for sold software products there is a higher motivation to have security issues in downstream projects fixed. However maintaining those fixes independently is a very costly matter: Instead getting them integrated in the upstream project does prove a lot cheaper in the long run. As a result there\u0026rsquo;s now one more reason to be able to participate in upstream open source projects.\nGaining experience and learning # More than a decade ago I learnt that learning needs an environment where students feel safe, where they understand that mistakes will not be held against them. What a contrast to how work in open source is organised - with all communication happening in public, on channels that are archived, searchable and linkable - preferably for a very long time.\nWhat if instead developers could practise open source ways of working internally - after all even withing corporations there are lots of teams, working on components that are not all fully independent. InnerSource translates collaboration patterns often found in open source and translates those into patterns that help solve challenges commonly found in cross team collaboration within corporations.\nInterestingly people that have been practising InnerSource for several years over time were able to dig ever deeper - and noticed how a lot of the discussion points in open source around governance, communication, expectation management and the like are exactly the same as they already know from their InnerSource experience. As a result, becoming active upstream turns out to be a whole lot easier.\nGaining agency # Bringing the two aspects together: With growing discussion around re-licensing I do believe teams are starting to be aware of how open source projects are different from each other - not only in terms of license but also in terms of governance.\nInnerSource - in particular the pattern about governance levels - helps teams get more hands on experience what different levels of openess and neutrality mean for daily project decisions. In addition InnerSource teaches that the walls between teams are fairly small and can easily be overcome - giving agency back to engineers, teaching them just how easy and helpful it is to get involved in all of the things they depend on - both, internally (using InnerSource) as well as externally (participating naturally in Open Source projects).\nSummarizing # "},{"id":7,"href":"/ospo-links/","title":"useful ospo links","section":"Inductive Bias","content":" Some useful OSPO links # https://todogroup.org/ \u0026hellip; TODO group of Linux Foundation generally collecting material around OSPOs and getting people interested in the topic together https://github.com/github/github-ospo/?tab=readme-ov-file \u0026hellip; OSPO policy templates as published by GitHub\u0026rsquo;s OPSO Documentation skeleton \u0026hellip; skeleton for things to document in an OSS project. Quilin \u0026hellip; a starter project template for Open Source, and likely InnerSource, projects Keeping the above here to not loose them when cleaning my open tabs.\n"},{"id":8,"href":"/open-source-contributions/","title":"How hard can it be: Open Source contributions","section":"Inductive Bias","content":" How hard can it be: Open Source contributions # \u0026ldquo;Sharing is caring - if only more downstream users simply contributed to the open source software that they depend upon.\u0026rdquo; - a thought that has crossed my mind more than once in the past decades. Except - for your average software engineer in industry \u0026ldquo;contributing to open source\u0026rdquo; is anything but simple. In addition it\u0026rsquo;s anything but obvious why - on top of daily work - one should commit time to open source. Where should that time come from if deadlines already are tight?\nWhy should I care? # So unless you\u0026rsquo;ve been living under a rock and pretending to be a lone hero coding away in the dark most likely a lot of your daily work relies on open source: That starts with your operating system (at least the one that your production workload is deployed on). It continues with your programming language. Most likely build tooling and package management as well. How about logging - both, the libraries in your code as well as the data analysis stack in your backend? Is there a search box anywhere in your application - chances are it\u0026rsquo;s backed by Apache Lucene. How about data storage? I\u0026rsquo;m not even trying to dig into stuff that happens in the browser.\n\u0026ldquo;But I can\u0026rsquo;t be involved in all of them.\u0026rdquo; I hear you say. As a first step, trim down the list you created above to components your business relies on - the ones you cannot remove or replace in a matter of days. Those are the ones where you want to learn quickly if there\u0026rsquo;s a security issue, or worse if the project is in the process of shutting down or being superseded by something else. Those are candidates to dig deeper. Dig deeper here means figuring out where the community of the project generally meets to discuss future developments, roadmap, project strategy. These days, a lot happens on GitHub, GitLab and the likes. However there are still projects relying on good old mailing lists, IRC, but also Slack, Discourse or similar systems. Often you will find out where the community meets in their README, on their webpage or on a page dedicated to new contributors. (Caveat: Not all of that may be open for public discussion if the open source project you are looking at is single vendor.)\nSo your first reason to be involved: Your business vitally depends on an open source project.\n\u0026ldquo;So I just sit down for an hour, checkout the project, make a modification and send it as a pull request, right?\u0026rdquo; I hear you say. Don\u0026rsquo;t! That will only lead to frustration. Chances are the project uses a language you are not familiar with, a build system you have to get used to and the modification you are making isn\u0026rsquo;t even welcome - leading to you having spent a week for nothing. Instead: Figure out where the project\u0026rsquo;s community meets. Chances are in addition to the source repo you have already found, there is an issue tracker you can follow - more likely on their contributing docs you will find links to mailing lists (did I mention that I have grey hair?), IRC (more grey hair?), Slack, community forums, Discord, Mattermost etc. There you will find all of the unstructured communication that happens outside of issue trackers. People helping users get up and running. People helping contributors getting up and running. People discussin strategy and roadmap. Even job boards if the project is popular enough to get more than the occasional yearly job ad on their communication channels. Dedicate an hour or two each week to just follow along. Very soon you will notice that you are able to answer at least basic questions of people who joined after you. Do that and help the project get time back by responding to these questions on their channels. As a result you will better remember what you learned through having used active recall. In addition over time people will remember your name.\nYour second reason to be involved: Learn more about both, project and community and have your name known.\nPro-tip: Larger projects are often very well organised including offering onboarding support, office hour calls, dedicated staff for helping newcomers. On the flip side tiny projects tend to be a lot more relaxed with people both, happy to help you and happy about your help. Which to prefer is up to you.\nYour third reason to be involved: Not only will you learn more about the project itself - you will also be exposed to different ways of building, testing, documenting and enforcing coding guidelines. A lot of what teams typically cover in onboarding has to scale in your average open source project to much higher numbers: Out of 100 users, only one will turn in a contributor. Out of 100 contributors, only one will turn into someone who sticks around for long turning into a regular committer - as in someone committed to the project. As a result a lot of what is implicit in company teams tends to be written down explicitly for open source projects to the point where it is automatically enforced, documented to a level where a lot of questions can be answered in a self-service kind-of way or automated to free up precious time. A ton of these practices over time have been carried over to professional software development teams. A ton is still left to carry over.\nYour forth reason to be involved: Not only will you learn new technical ways of working. Often open source communities span multiple continents, cultures, timezones, companies. \u0026ldquo;Let\u0026rsquo;s have a quick call to discuss this.\u0026rdquo; doesn\u0026rsquo;t scale. Neither does any kind of coffee-machine/ desk-bound conversation. Instead methods had to be found to build bridges across these gaps - and find a way to collaborate that builds a tide that lifts the boats of everyone. As a result not only will you see communication that puts a strong emphasis on async. You will also see very intentional use of in-person communication. You will also see very intentional way of negotiating features rooted in the economics of vendor neutrality.\nSo how exactly do I get started again? # Here\u0026rsquo;s your checklist:\nBased on your business interest and risk - but also based on your personal interest - chose an open source project that you want to dedicated an hour or two per week to. Then:\nFigure out where to find your favourite project: Where do they host version control? Where\u0026rsquo;s the issue tracker? Where\u0026rsquo;s the webpage? Is there a \u0026ldquo;how to contribute\u0026rdquo; document anywhere in the sourcecode or on the webpage? Based on the information gathered above, figure out how to best join the conversation of your project. Most likely only knowing where PRs and issues come in is far from enough. Follow the conversations for a couple weeks. Notice how you naturally start answering questions of others. Notice how after a bit of time potential for modifications is obvious. Notice also how friction to send a ping when a PR falls through the cracks reduces substantially because you know the people involved as well as their constraints. Finally: Watch out for chances to meet in person - or over a virtual cup of tea :)\nGaining by sharing and building bridges # As a result I tend to emphasize the chance to gain by sharing in open source. In addition through participation you will be able to build bridges - not only to the project itself. By practising to change your perspective that will also become a lot easier when communicating across the limits of your own local team.\n"},{"id":9,"href":"/reboot-blogging/","title":"Reboot blogging","section":"Inductive Bias","content":" Reboot blogging? # March 2009 - according to the history of the posts that\u0026rsquo;s when I started writing entries for my own blog. Twitter was three years old and of little more use than sharing where people were at the moment, which food they were eating.YouTube was just four years old. It was also the year I started the Apache Hadoop Get Together Berlin, back then in the famous Newthinking Store - a meetup before meetups were called meetups in a co-working space long before the concept of a coworking space gained traction. All I needed the blog for was as a place to easily publish the meetup schedule, share summaries of those events, slides and later recordings of the meetup. Back then the blog was hosted in a Wordpress installation - with trackbacks/pingbacks enabled and occasionally coming in. With comments enabled - and no need to moderate them. People learnt about new blog posts simply by subscribing to the blog\u0026rsquo;s RSS feed.\nA few years later comments started generating more noise than value, so I turned them off. Those who had left comments, knew me in person - so those messages would still reach me, just not in public.\nOne advantage of writing a blog not to maximise readership or reach but simply to stay in touch with friends and family: This blog never was under anything I would call monitoring and alerting in any way. If it went down, typically someone I knew would get in touch by e-mail telling me: \u0026ldquo;Hey, your blog is down.\u0026rdquo; - as a side effect telling me that there are still people reading my stuff. At around 2013 those messages got too annoying: For whatever reason everytime a search engine crawler went over my little blog, the database in the backend got overloaded. Adjusting caching in Apache httpd did help quite a bit, but didn\u0026rsquo;t remove the issues entirely. So instead, because anything dynamic had been turned off already anyway, I moved to generating static pages with nikola instead. That made the page a lot more stable - and needed way less resources. There I was finally understanding why for a very long time projects at the ASF were very much encouraged to use static pages (potentially generated from content sources checked into version control) instead of all those new-fangled dynamic CMS stuff :)\nFast forward another ten years: Social media is no longer the little innocent experiment it once was. Most popular RSS feed readers from ten years ago have been starving or are dead. However there are still things I would like to write about - and in a location that I can easily backup, easily move. Time to breath a little life back into this little blog. With a little help from t-lo moved to a combination of GitHub pages and Hugo. With a little help from Raphael got a CSS issue with dark mode fixed - so apparently the \u0026ldquo;hey, your blog is down\u0026rdquo; kind of alerting still works - even ten years later. Maybe there is hope for a future where the distributed nature of the web survives the trend of centralization, how for rewilding the internet. Through F Droid I finally managed to find a usable feed reader. First impression of the look and feel: It looks a lot more usable to follow other people\u0026rsquo;s content than simply subscribing to blogs through mail. Other than Fernanda Weiden on Substack - wondering who else should I definitely add to my list of feeds to follow?\n"},{"id":10,"href":"/kids-in-tech/","title":"An update to the 'Children tinkering' after \u003e10years","section":"Inductive Bias","content":" Children tinkering, part 2 # Roughly a decade ago I wrote a short post about children tinkering. A lot of the advice from back then is still valid today. I just noticed that ten years later there are a few more things one could look into:\nInstead of only being a subtrack of FrOSCon, Froglabs have turned into Teckids e.V. - including online fora, a yearly week for an off-site and several side tracks of Chemnitzer Linuxtage, FrOSCon and FOSS events in Graz.\nIn terms of conferences with sub-tracks, in addition to CCC events and FrOSCon, there\u0026rsquo;s now Chemnitzer Linuxtage with several kids tracks and a soldering workshop room. There\u0026rsquo;s also a kids track as an aside to FOSDEM.\nThere is a lovely, actively maintained list of resources over in the FSFE Wikipage.\nSpeaking of FSFE:\nDid I mention that I love their book Ada und Zangemann. Available in all sorts of languages. With stickers in addition and pages for coloring in. With slides and all you may ever wish for to do a book reading in your local library or for the friends of your child.\nSpeaking of books:\nI regularly bump into nerds and geeks at a loss of motivating their kids - both, boys and girls, to loook into science.\nAs a result I have a collection of books I tend to recommend for people looking for stories about role models:\nThere\u0026rsquo;s the entire Rebel girls book series. In Germany, Hanser Verlag has similar books for boys. The Computer History Museum does have a lovely collection of materials and books. In particular check out the Women who launched the computer age.\nIf you need a video to convince you to read these books to your four year old: There\u0026rsquo;s a lovely one on inspiring the future. It was back at the Natural History Museum in London that I learnt that compliance with expectations and stereotypical behaviour starts to develop at the age of 4.\nIf you speak German you may want to check out Meine Freundin Roxy for a nice introduction to machine learning.\nAlso for people speaking Germans there\u0026rsquo;s a lovely #kids #digital #genial book to cover all sorts of online and digital first topics that go way beyond coding. Digital Courage also has several other pages on going online in a responsible way related to children.\nSpeaking of other resources beyond coding:\nFor German speakers there are things like Frag Finn and Klexikon to guide younger humans towards content that is easy to understand.\nIn terms of projects:\nStart with the learning to code. Alternatively try out light bot. Continue with scratch. There are tons of great books, tpyically even local libraries have them. If playing with hardware, both, Calliope and Micro Bit are lovely. For getting apps on your phone, try out app inventor - again, comes with a ton of books, we love Become an app inventor because it comes with a lot of motivational stories.\nTry to find like-minded people in your city, e.g. by joining a coder dojo. There are several in Berlin. If there are none in your city, create one :)\n\u0026ldquo;But given code generating AI, nobody will be writing source code when today\u0026rsquo;s children are grown up\u0026rdquo; I hear people say.\nTrue, neither did I need Amiga Basic in my job, nor Turbo Pascal. I didn\u0026rsquo;t even need the Assembler that I learnt at university. However I do still need loops, conditions and switches. I do still need the strength to think algorithmically. I also still need the understanding of how a CPU works, including branch prediction and loading data from registers, memory, disc or through the network to do performance optiomisation and understand specific security attack vectors.\nI\u0026rsquo;d much rather see a generation who understands at least the basics of the devices they use on a day to day basis. And maybe, just maybe, they\u0026rsquo;ll also be more motivated for math, if they know that all recommendations seen in any online social media site at the end of the day is nothing more than a few matrix multiplications ;)\nAnd at the end of the day, even those new and shiny Large Language Models are nothing but a bit of algebra and statistics :)\n\u0026ldquo;But I have no background in technology myself\u0026rdquo; I hear you say. Guess what? A lot of these offerings are very easy to grasp for little people with a hacker\u0026rsquo;s mindset who love exploring and tinkering. The best way to help is to avoid stopping their creativity. For cherries on top show them what you yourself do on a day to day basis. Show them your own workplace and let them understand all of the things that exist beyond what they learn in school - or phrased another way how what they learn in school relates to the world that exists beyond.\nSo what if someone asks you whether tech would even be interesting for girls? I\u0026rsquo;ll quote a lovely lady who gave a lightning talk together with me during FOSS Backstage 2024 on how to draw kids - and in particular girls - into technology in general and into open source in particular: \u0026ldquo;I would answer to look at Ada Lovelace. To look towards Grace Hopper. To look towards all the cool girls in technology. And then I\u0026rsquo;d tell them to think again.\u0026rdquo;\n"},{"id":11,"href":"/spam-spam-spam/","title":"A short, incomprehensive history of spam and counter measures","section":"Inductive Bias","content":" A short, incomprehensive history of spam and counter measures # \u0026ldquo;\u0026hellip; it should be clear that improvements in communication tend to divide mankind \u0026hellip;\u0026rdquo; by Harold Innis in Changing Concepts of Time\nThis post was triggered by multiple conversations in my big data circle of friends. All conversations agreed on some important topics: Social media sites these days are influential on the daily life of people globally. With that influence comes an incentive to use these sites to influence behaviour and public opinion. Oftentimes counter measures to these influencing activities - if implemented - are considered the secret sauce of each individual site: Even if malicious users will have a laundry list of sites to spread their message to.\nBack in 2022 we had a keynote at Berlin Buzzwords on the influence of targeted advertisement on public opinion and on it\u0026rsquo;s detrimental effect on democracy if used in democratic voting processes due to its nature of being invisible to public inspection. After seeing the talk, I\u0026rsquo;ve shared the recording over and over again. I believe it is a great talk to explain the field that recommendations and personalised ranking has moved into: From merely suggesting other books or boots to buy based on purchase history, or suggesting other songs to play from play history we have entered a time where recommender algorithm implementations are being targeted by entities actively trying to figure out ranking factors from the outside in order to maximise the impact of their messages on political outcomes.\nThe fun and encouraging observation though: Malicious actors trying to guess from the outside how training machine models work from the outside - that\u0026rsquo;s nothing new. There\u0026rsquo;s an entire research field called Adversarial Learning dedicated to that topic. I first encountered it talking with people working on e-mail spam decades ago.\nHowever attackes on social media sites aren\u0026rsquo;t trivial cases of mails being sent out. There\u0026rsquo;s entire sub graphs of synthetically created user accounts used to amplify messages. But wait a minute: Again, this is no news for those who remember things like Google Bombs from decades ago. There\u0026rsquo;s a ton of wisdom in the search engine ranking community about combating search engine spam and boosting organic looking content. A lot of that is based on making sure pages develop organically over time, but also based on properties of the web graph - where pages are nodes, links are edges. Social graphs aren\u0026rsquo;t that different. Well, except if you check the literature on social graphs you will find several features that will set these graphs and sub-graphs apart from mechanically created graphs.\nLooking at how trust and safety teams used to work what strikes me as obvious is that there needs to be way more integration with those teams working to improve rankings. There are several naive assumptions in machine learning models for learning to rank and for recommending content that are not true - in particular when faced with actors intend on using the ranking signals to their advantage. Knowing these influences though can be used by platforms to demote content they do not wish to promote.\nMalicious influences though do not happen in an isolated way. Much like in security - knowing that attacks happen in a coordinated way where players essentially have a laundry list of platforms to target, we need to shift to collaboration in order to gain speed by sharing learnings, both about the nature of attacks but also about working counter measures.\nSo what does it take, to spread behavioural changes through social networks? Actually quite easily, so in different ways then you would see with deseases spreading in the same networks. You can play around with such dynamics easily online.\nNaturally I would love to see this collaboration happen rather sooner than later - unless it\u0026rsquo;s already taking place, in which case: Awesome! (Even if I\u0026rsquo;d love to hear a talk about that at Berlin Buzzwords analogous to the one on security at FOSS Backstage ;)\nOne final parting note: A lot of the engineers working in ranking and recommendation may not have witnessed the early days of search engine spam so, maybe it\u0026rsquo;s time to share a few fireside stories as well.\n"},{"id":12,"href":"/isc-board-of-directors-retrospective/","title":"InnerSource Commons Board of Directors - a retrospective","section":"Inductive Bias","content":" InnerSource Commons Board of Directors - a retrospective # In the last years I had the honour of serving on the InnerSource Commons board of directors. I was one of the founding board members. Thank you to Danese Cooper for inviting me to that: I still remember sitting in the subway on my way to work years ago reading her question of whether I wanted to be founding member of the InnerSource Commons. I agreed. And only read the request carefully when I had finally arrived at work - only to find out that what I had been asked was whether I wanted to be a founding board member \u0026hellip; That was at a time where I had been board member of the ASF for two years (with a break of one year) in the past. So I only knew the time and energy intensive but also rewarding experience. I told myself the ISC would be much smaller, and decided to stick to my original decision. So far I never regretted that decision.\nThe past twelve months was the first time where we had people join the ISC board who did have a lot of corporate leadership experience - but not that much experience with the nudging, inviting, decentralized type of leadership that is so common in open source. We had people who were not part of the foundation from it\u0026rsquo;s very beginnings. Based on their experience learning the ropes of the ISC board I would like to publicly share my retrospective on what it means to be an ISC board member. Maybe it helps others make a better decision on whether this is a path they want to take.\nI\u0026rsquo;ll link to other directors\u0026rsquo; texts below as soon as they decide to make them publicly available.\nFor context:\nMuch like the ASF the InnerSource Commons foundation is a public charity, read: 501(c)3, serving the public good, mostly as an educational organization. Much like the ASF the InnerSource Commons foundation was incorporated in the US. Much like the ASF the InnerSource Commons foundation is built on the idea that individuals supporting the movement are more important than corporations and other legal entities supporting it. As a result influence in the foundation lies with it\u0026rsquo;s members, is bound to contributing to the foundation and is meant for life. As a result we do have members that remain active even despite changes in their employer. We do have members who carry our ideas to multiple corporations and public institutions. One thing that was important to the founding members of the foundation was to allow people to remain part of the community, even when their resources to be involved ebb and flow over time. It was also important to us to build rotation into the system of setting up the board - making it easy for people to step down and take a break - returning back when they have collected new and fresh energy.\nHow did being on the ISC board differ from being on the board of The ASF? The first time I became director at The ASF that foundation was already well established. It was large and influential. It had gained industry influence to a level that really only became clear to me when I got the birds eye view of a director. It also had well established processes - though dated, several pillars I knew from my project work did continue to hold.\nAt the ISC we decided early on that The ASF should be our role model - except where from history we knew that we would need to deviate to avoid certain challenges. We started with people that had ASF experience, with people that had listened to those stories for a very long time already, with people that had extensive open source experience. But we also added people who had been involved with open source only tangentially and really only touched base with those concepts through InnerSource.\nThat mixture meant that we had a good template to draw best practices from. But we also needed to get down to the reasoning behind processes at The ASF to explain how and why they work. Often that needed quite some historic knowledge and an understanding of why other alternatives that were being proposed would not work out.\nBeing so small meant that there was more space for experimentation. Discussions on changes were much less contentious. Being a small group of people also meant that it was easier than in the large group of the ASF membership to come to agreement and move forward.\nOn the other hand I did miss certain processes that people just followed automatically. Certain things that I know work very well in volunteer first settings where not everyone is on the same timetable and not everyone has time to join sync meetings were a challenge to explain to a group that hadn\u0026rsquo;t gone through ASF training. That\u0026rsquo;s a bit like what projects go through when the join the ASF incubator - except there the only way out and up is to learn the existing way of collaborating, instead of building a new way on top of that existing template.\nWhich areas were a lot of fun for you? Seeing small suggestions turn into major areas for contribution and outgrow my initial idea certainly are things that are a lot of fun. I know from experience but also from wide spread guidance that one way to pull in new contributors is to tell them all the easy tasks you need help with. I also know that translations are an easy first step into communities. Now, what our working groups offer are a lot of educational material - be it our Learning Path that people can go through to get a self paced InnerSource training. Or be it our pattern collection. I remember suggesting months ago that we provide contributors a means to translate the Learning Path. What came out of that simple idea is way bigger than I hwould have imagined:\nAt first the learning path segments that were English (but written by a German speaking contributor) were translated to German by one of the colleagues of said contributor. Over time more and more people stepped up translating the Learning Path into all sorts of languages. As a next step volunteers stepped up to do the same with our patterns. Finally there were people stepping up to not only run local community groups, but also help translate the entire web site.\nChanneling that work certainly has been an interesting question. In particular as some of the target languages are not spoken by any of the foundation founders.\nAre there any parts of being a board member that you could imagine helping with even after stepping down? I have been told that people appreciate my OSS experience as well as my knowledge of the inner workings of the ASF. Whenever I step down, know that I will not leave - at least not immediately. I will still be around, happy to share knowledge and answer questions.\nWhich areas were particularly time costly for you? During the past year in addition to being director I also took on the role of president. While it\u0026rsquo;s a tiny bit of work - I do look forward to not needing to write a monthly board report :) I\u0026rsquo;m glad everyone did enjoy reading the ones I did write. It certainly was a new experience for me to write those - in particular given my limited experience with corporate English.\nOverall though time invest was really a lot less than at a foundation the size of The ASF.\nIn early years the tasks that did eat up time were related to setting up process (think meeting cadence, preparation, agenda handling, reporting cadence etc. but also think membership voting), setting up infrastructure and access (think GitHub access).\nWhich areas were energy costly for you - didn't necessarily take a lot of time but were definitely not fun to deal with? One thing that is energy costly is reminding everyone that we are dealing with volunteers, that we want a decentralized organisation where working groups are the places where you want to be involved to get things done - and where the foundation only exists to serve those working groups.\nFor someone that grew up inside the ASF it\u0026rsquo;s natural to understand that the foundation doesn\u0026rsquo;t pay for working group work, it does not meddle with strategy or technology produced by those groups - but it does meddle with how those groups are run. Getting that same understanding into a new organisation with people lacking the ASF background is something where questions will pop up over and over again.\nAlso switching perspective from \u0026ldquo;this is what we produce and sell\u0026rdquo; to \u0026ldquo;this is where we ask those benefitting from our material to invest time to help us improve\u0026rdquo; has been a challenge to communicate in a way for everyone to fully understand.\n\"I wish I had known this before joining the board\" Looking back at that one moment of panic when I realized what I had agreed to, I would tell myself that this time it truely will be different: Less time consuming. But on the other hand in a place where being alert is needed so discussions and decisions don\u0026rsquo;t move in the wrong direction early on.\nI would tell myself that while not much has been written down there are still a ton of presentations recorded on several services that will help understand why the ASF does certain things the way it does them.\nIn your opinion - what are the strengths of the ISC board? The ISC board builds on the strenghts of its members. It builds on the experience of those members - coming with a lot of leadership experience but also with a lot of open source experience. All that aside everyone involved does have a lot of humor but also brings there entire self - including experience from other organisations, including family, including stories from other contexts.\nIn your opinion - what are areas for potential improvement for the ISC board? We are still young, there are only a limited amount of topics to discuss during each board meeting. But I miss the flurry of discussions happening between board meetings, I miss consensus building between board meetings.\nBeing still young one of the things I would like to see us better from the get go is explain to everyone involved - including people who are just getting started to use our material - how we operate. With more and more open source projects pushed forward by well financed benevolent dictators has meant that people have started to see open source projects essentially as equivalent to commercial products. As some piece in your supply chain. Something that we teach as a core part of InnerSource is to understand that you are not passively consuming - but you are expected to contribute yourself to mae things work better. I believe as a foundation we can better communicate that it is those who participate, those who give us their time that make the entire organisation work out well.\nIn your opinion - what changes should be made to the way the ISC board operates, interacts with communities, interacts with the wider ISC ecosystem, interacts with the public? I do think we could make sure that our working groups are even more clearly perceived as what they are: The heart of the InnerSource Commons foundation.\nAny advice for new board members - where to look first, what legal implications to keep in mind, what PR implications to keep in mind etc.? So far we are still relatively small compared to the ASF at least, so a word on social media that you haven\u0026rsquo;t thought about for a very long time is bound to have less potential for damage than at an org like the ASF.\nWe are a 501(c)3 - I would urge you to do the boring work of reading through the bylaws and ask any questions you run into. If you have seen the work and offerings of the Eclipse foundation or the Linux foundation be prepared that we are similar - but subtly and intentionally different in some areas. So don\u0026rsquo;t be shy to ask existing members for their perspective on the exact aspects that we differ in.\nWe are incorporated in the US. While I at least hope our working groups are wide open to each and every contributor, while I hope they are culturally diverse at the corporate level you will need to read up on US regulations: In particular for Germans - a lot of dictionaries will translate our incorporation form as \u0026ldquo;e.V.\u0026rdquo; - while this is largely correct, there are several differences in the details. So beware of false equivalences there.\nWhat are the tasks and time commitment? You will have to provide community oversight for our working groups - making sure they are alive and humming along. You will be asked to help out with publicity work, speak at our events. Apart from that general structure, budget allocation, establishing connection to prospective sponsors are all relevant tasks.\nTell us about a moment from your time on the ISC board that is most precious to you. Seeing InnerSource make its way into organisations has been awesome to watch. Being on the inside it sometimes feels like things have always been the way they are just now. Looking back several years though the progress does become very obvious: That includes feedback how our work on InnerSource is very important coming from ASF members who have been part of the Open Source movement for longer than I am working as a software engineer :)\n"},{"id":13,"href":"/when-it-takes-a-pandemic/","title":"When it takes a pandemic ...","section":"Inductive Bias","content":" When it takes a pandemic \u0026hellip; # to understand the speed of innovation. 2020 was a special year for all of us - with \u0026ldquo;us\u0026rdquo; here meaning the entire world: Faced with a truly urgent global problem that year was a learning opportunity for everyone.\nFor me personally the year started like any other year - except that news coming out of China were troubling. Little did I know how fast those news would reach the rest of the world - little did I know the impact that this would have.\nI started the year with FOSDEM in Brussels in February - like every other year, except it felt decidedly different going to this event with thousands of attendees, crammed into overfull university rooms.\nNot a month later, travel budgets in many corporations had been frozen. The last in person event that I went to was FOSS Backstage - incapable of imagining just for how long this would be the last in person event I would go to. To this date I\u0026rsquo;m grateful for Bertrand for teaching the organising team just how much can be transported with video calls - and I\u0026rsquo;m still grateful for the technicians onsite that made speaker-attendee interaction seamless - across several hundred miles.\nOne talk from FOSS Backstage that I went back to over and over during 2020 and 2021 was the one given by Emmy Tsang on Open Science.\nReferencing the then new pandemic she made a very impressive case for open science, for open collaboration - and for the speed of innovation that comes from that open collaboration. More than anything else I had heard or read before it made it clear to me what everyone means by explaining how Open Source (and by extension internally InnerSource) increases the speed of innovation substantially:\nInstead of having everyone start from scratch, instead of wasting time and time again to build the basic foundation - instead we can all collaborate on that technological foundation and focus on key business differentiators. Or as Danese Cooper would put it: \u0026ldquo;All the boats must rise.\u0026rdquo; The one thing that I found most amazing during this pandemic were moments during which we saw scientists from all sorts of disciplines work together - and doing so in a very open and accessible way. Among all the chaos and misinformation voices for reliable and dependable information emerged. We saw practitioners add value to the discussion.\nWith information shared in pre-print format, groups could move much faster than the usual one year innovation cycle. Yes, it meant more trash would make it as well. And still we wouldn\u0026rsquo;t be where we are today if humans across the globe, no matter their nationality or background would have had a chance to collaborate and move faster as a result.\nSomehow that\u0026rsquo;s at a very large scale the same effect seen in other projects:\nRoboCup only moved as fast as it did by opening up the solution of winning teams each year. As a result new teams would get a head start with designs and programs readily available. Instead of starting from scratch, they can stand on the shoulders of giants. Open source helps achieve the same on a daily basis. There's a very visible sign for that: Perseverance on Mars is running on open source software. Every GitHub user, who in their life has contributed to software running on Perseverance today has a badge on their GitHub profile - there are countless badges serving as proof just how many hands it took, how much collaboration was necessary to make this project work. For me one important learning during this pandemic was just how much we can achieve by working together, be collaborating and building bridges. In that sense, what we have seen is how it is possible to gain so much more by sharing - as Niels Basjes put it so nicely when explaining the Apache Way in one sentence: Gaining by Sharing. In a sense this is what brought me to the InnerSource Commons Foundation - it\u0026rsquo;s a way for all of us to experience the strenght of collaboration. It\u0026rsquo;s a first step towards bringing more people and more businesses to the open source world, joining forces to solve issues ahead of us.\n"},{"id":14,"href":"/apache-board-of-directors-a-retrospective/","title":"Apache Board of Directors - a retrospective","section":"Inductive Bias","content":" Apache Board of Directors - a retrospective # Last term I had the honour of serving on the ASF board of directors, better explained in context of the ASF governance structure. As quite a few directors (myself included) declined to run for the board this year again, I thought it would be a good idea to think about the past term, write that down and publish those thoughts. As I wanted to give every board member a chance to respond, I shared some guiding questions but left it to board members to choose the channel they deemed most appropriate to share their responses on.\nSo far, two decided to make their answers publicly available:\nThe board member experience at Apache by Shane Curcuru Reflections on the ASF board, and advise to new directors by Rich Bowen When posting the questions, I intented to share my own answers as well - but refrain from publishing them for as long as possible to not influence others. So here we go:\nFor a bit of context where I\u0026rsquo;m coming from: When I was first nominated for board I didn\u0026rsquo;t see that nomination coming at all. This was back in 2016. I did accept, I was voted in. Back then I was still in a technical role, even though what I had been doing in various teams and open source projects was much more on the leadership and management side oftentimes. As a matter of fact, after that one year it was time to make a shift in roles in my dayjob as well.\nI took a one year break in 2017, was elected as board member again in 2018. Seems like twelve months is just about the timeframe that works for me - maybe some of the answers below will explain why that is the case.\nWhich areas were a lot of fun for you? What is truely the most fun part is working with excellent people who have a ton of experience and know what they are doing - even though they themselves are sometimes in a mode like \"we just need to make this up as we go because there really is nobody who has been faced with these challenges before\". The other part that is really fun is that even besides the fact that there is quite a bit to do, there's always someone who keeps the morale up, who brings their humour to the team. Be it on the board members meeting IRC backchannel or other places. If you are not subscribed to board@: Every quarter projects are supposed to submit a quarterly board report. To guide their reporting and make it easier for those parts that can be derived from project communication statistics, there's a report generation tool. It will generate a project board report template that includes headings for some sections that can only be filled by a human. Those sections come with default text - text that is entirely non-sensical - but fun to read, should projects forget to fill in the real content. Trying to remember from memory, as I'm offline for the coming weeks, it was something along the lines of ==Project health narrative Minions took over your project and are having a party. The last part is the chance to bring people together, work together, trust each other across large distances - to the point of sharing family stories. While face to face meetings are awesome for bonding with people, those I've encountered so far as directors were easy to work with across distances even if I had met them just a couple times for a few days years ago at an ApacheCon. An \"assume best intent\" kind of attitude is certainly helpful there. On a tangent that also means that typically I'm communicating with people on eye level, only to get scared for a split second weeks and months later when coming across their LinkedIn profile and reading what they do or have done for a living... Which parts were particularly educational for you? The way the Apache board runs board meetings with several dozen agenda points in very well under two hours by moving as many decisions to asynchronous channels as possible. This makes it possible to participate from many more time zones than I have seen at any other organisation. It also means that people don't have to align on when everyone has time to participate in the meeting and can do most of the discussion and decision work whenever they happen to have time for it spread out over many days. On the flip side this means the entire process feels like it's slowed down a lot - except it wouldn't be possible any other way as those know who have tried to get all nine directors in one room at one date at one time during that day. I had no idea how budgeting is done at the ASF's scale before being part of that discussion. I was very glad to know that there are other directors who have experience with that topic. In every board meeting we go over project board reports: Everytime there are some where I'm like: \"You should totally talk about what you're doing there to a wider audience than just your project.\" - it's great when that advise is actually followed up on - it's even better as so far everytime such learnings were written down behind a stable URL, that URL became very popular as a reference for other going forward. Are there any parts of being a board member that you could imagine helping with even after stepping down? Pretty much all tasks the board does can be done by any member. One thing I seriously would like to continue doing is to build more bridges between the board of directors and projects (inviting new PMC chairs to join the board meeting and take away their fear that it might last hours is just one possibility). If asked I will also continue answering questions on best practices, how to get involved and generally try to pull people in - both into projects as well as into the foundation itself. With a bit of hind-sight: A few months after leaving the board of directors, what I did continue doing was make time to explain the Apache Way to other people. I'm also interested in keeping the discussion on how to get involved - and how to convince your employer to donate your time to the foundation alive. Another thing I would really love to find best practices and patterns for is how to balance full-time open source contributors with those working on projects occasionally. This isn't much different to the challenge of integrating part-time and full-time employees - something that at least in some German companies has been solved for a long time already. Time to also find a solution for Apache projects taking project neutrality one step further. Which areas were particularly time costly for you? We have well over 200 projects. Each is required to submit a board report on a quarterly basis. While most aren\u0026rsquo;t particularly long, the content does add up. While in theory, checking private lists, following a few discussions and listening to project contributors (be they users, contributors, committers or pmc members) would be needed to spot governance issues early, the current number of projects to check for each shepherd means that issues will be discovered too late for precise, scalpel like patches. Somehow that entire process is something that in my opinion could benefit from a better collective understanding of what constitutes a healthy projects, including an understanding of just how much freedom there is in configuring your project governance best practices. Maybe the Apache maturity model would be a good starting point for spotting best practices. Maybe it would be helpful to also come up with project anti patterns to watch out for to take the guess-work out of the process of writing a board report. Which areas were energy costly for you - didn't necessarily take a lot of time but were definitely not fun to deal with? There\u0026rsquo;s just one example that\u0026rsquo;s standing out for this one: Occasions where the board needed to delegate tasks to a new role. The main reason here was that typically the discussions quickly dissolved into mixing role name, role accountabilities, person to fill the role and budget for the role all into one discussion. While for the final result all four perspectives need to come together, in my experience, it helps to first think about the actual accountabilities that should be delegated. Only after that look for a name that fits, otherwise people will associate accountabilities because of the name, believe that those accountabiities aren\u0026rsquo;t needed and reject the need for the role entirely. Only after having exact accountabilities start discussing budget - it\u0026rsquo;s much easier to talk about money, once it\u0026rsquo;s clear exactly what the purpose looks like that the money is going to be spent on. Once that is out of the way, start looking for a human to fill the role. Even if there is an obvious choice popping up, make your need for that role to be filled known and go looking for volunteers. You\u0026rsquo;d be surprised to hear how many new faces we\u0026rsquo;ve seen speak up and get active - lowering the load on any usual suspects. \"I wish I had known this before joining the board\" The ASF is really good at hiding where it was incorporated in it's daily operations. Being a director that suddenly moves a lot closer: It's becomes much more likely that your communication becomes of interest in a suppeona (luckily, there's an archive for pretty much everything, so the relevant communication can be handed over fairly quickly). For me this lesson moved the ASF a whole lot closer to a regular company with a BYOD policy though. In terms of how to run a US Delaware law incorporated 501.c3 foundation I was lucky insofar as there were still several long term directors as well as former directors around that knew about and were happy to talk about the constraints of what could be done and what couldn't be done - and why so. In your opinion - what are the strengths of the ASF board? They have developed an awesome model of collaboration with an asynchronous decision making process that does scale. The people I had the honour to serve with remained human beings, including the humor and including their private family background. The ones I have seen serve in their individual capacity leaving corporate interests at the door - or announcing when discussions are started because of what they observed at dayjob. They bring a ton of experience from all sorts of backgrounds - both, culturally but also in terms of which companies they've seen from the inside - trying to learn from all of that experience to build a better world that is much more flexible and light weight than your average large scale enterprise. In your opinion - what are areas for potential improvement for the ASF board? A lot of the things that I think of reading that question really are things that could be improved even without being a director: Documentation on how the foundation works at the project, and at the operational level; documentation of expectations of projects, making purpose and actual content of previous decsions easier to discover... The only thing that comes to my mind that the board could watch out for is to have discussions in such a way as to keep all relevant people in the loop, facilitating a dialogue with people instead of about them. For contentious discussions it might also help to have a set of moderation techniques that have proven helpful in asynchronous communcation handy. In your opinion - what changes should be made to the way the ASF board operates, interacts with communities, interacts with the wider ASF ecosystem, interacts with the public? While at the ASF we are very good at thinking about inward facing communication, I believe we can become much better at communicating to the non-ASF world. As we grow, I believe that perspective becomes ever more important - we cannot rely on people to just \u0026ldquo;get\u0026rdquo; how we work from simply following our projects and becoming ever more involved.\nBeing community over code to me implies counting the user/customer of our projects as part of that community. They are the ones who have the fresh eyes to spot flaws in our argumentation of why we are doing things the way we do - and spot advantages. However they can only do that by understanding what goal and purpose we want to work towards.\nAny advice for new board members - where to look first, what legal implications to keep in mind, what PR implications to keep in mind etc.? While being a director can be done even with little time available, there are others on the board that dedicate a lot more time to the task than you will have available. Don't let that stress you out. For some discussions it helps to wait a day before writing an answer - either someone else will have writting what you wanted to say after that - or you'll be in a much better state of mind to make your statement without it being hidden behind a cloud of emotions that stop you from thinking clearly. While being an ASF director may mean little to the people you work with - even if your dayjob builds their entire business on top of an Apache project - it does bear a lot of weight to many other people worldwide: ASF internal, or ASF external (e.g. press representatives). With that power comes a call for using it responsibly: The statements you make, even if they were only meant as a basis for discussions, will have the power to cause fear if not marked as such. People will seek you out with the issues they are having in their community. Instead of jumping in yourself which does not scale long term, try to figure out how to help them in a sustainable way - e.g. by helping to find the right more public space to talk about the issues they have encountered and discuss a solution in public. Remember that the board at the ASF is designed to move slowly. As a result do not expect to make ground breaking changes within just one board term. However do expect that the little changes you do make will make a big difference in the long run. Essentially that's a side effect of a foundation that's build on the goal of longevity. One thing that is different at the director level (as well as the operational level): While in a lot of positions we have setup process that allow for people to take a break without announcement for any amount of time, director and operational positions do mean that there is a need for dependability and reliability - you will need to make time for that position regularly for at least one year. What are the tasks and time commitment? That totally depends on how much time and energy you want to commit. At the bare minimum if it's only about oversight a couple hours a week and maybe two days running up to board meetings could be sufficient (that's where projects submitting reports early make life easier for directors). Tell us about a moment from your time on the ASF board that is most precious to you. I tried to think of one - except there wasn't. While it does take time and energy, it's also a great source of positive inspiration, including tons of chances to learn from others. "},{"id":15,"href":"/froscon-2018/","title":"FrOSCon 2018","section":"Inductive Bias","content":" FrOSCon 2018 # A more general summary: https://tech.europace.de/froscon-2018/ of the conference written in German. Below a more detailed summary of the keynote by Lorena Jaume-Palasi.\nIn her keynote \u0026ldquo;Blessed by the algorithm - the computer says no!\u0026rdquo; Lorena detailed the intersection of ethics and technology when it comes to automated decision making systems. As much as humans with a technical training shy away from questions related to ethics, humans trained in ethics often shy away from topics that involve a technical layer. However as technology becomes more and more ingrained in everyday life we need people who understand both - tech and ethical questions.\nLorena started her talk detailing how one typical property of human decision making involves inconsistency, otherwise known as noise: Where machine made decisions can be either accurate and consistent or biased and consistent, human decisions are either inconsistent but more or less accurate or inconsistent and biased. Experiments that showed this level of inconsistency are plenty, ranging from time estimates for tasks being different depending on weather, mood, time of day, being hungry or not up to judges being influenced by similar factors in court.\nOne interesting aspect: While in order to measure bias, we need to be aware of the right answer, this is not necessary for measuring inconsistency. Here\u0026rsquo;s where monitoring decisions can be helpful to palliate human inconsistencies.\nIn order to understand the impact of automated decision making on society one needs a framework to evaluate that - the field of ethics provides multiple such frameworks. Ethics comes in three flavours: Meta ethics dealing with what is good, what are ethical requests? Normative ethics deals with standards and principles. Applied ethics deals with applying ethics to concrete situations.\nIn western societies there are some common approaches to answering ethics related questions: Utilitarian ethics asks which outputs we want to achieve. Human rights based ethics asks which inputs are permissible - what obligations do we have, what things should never be done? Virtue ethics asks what kind of human being one wants to be, what does behaviour say about one\u0026rsquo;s character? These approaches are being used by standardisation groups at e.g. DIN and ISO to answer ethical questions related to automation.\nFor tackling ethics and automation today there are a couple viewpoints, looking at questions like finding criteria within the context of designing and processing of data (think GDPR), algorithmic transparency, prohibiting the use of certain data points for decision making. The importance of those questions is amplified now because automated decision making makes it\u0026rsquo;s way into medicine, information sharing, politics - often separating the point of decision making from the point of acting. One key assumption in ethics is that you should always be able to state why you took a certain action - except for actions taken by mentally ill people, so far this was generally true. Now there are many more players in the decision making process: People collecting data, coders, people preparing data, people generating data, users of the systems developed. For regulators this setup is confusing: If something goes wrong, who is to be held accountable? Often the problem isn\u0026rsquo;t even in the implementation of the system but in how it\u0026rsquo;s being used and deployed. This confusion leads to challenges for society: Democracy does not understand collectives, it understands individuals acting. Algorithms however do not understand individuals, but instead base decisions on comparing individuals to collectives and inferring how to move forward from there. This property does impact individuals as well as society.\nFor understanding which types of biases make it into algorithmic decision making systems that are built on top of human generated training data one needs to understand where bias can come from:\nThe uncertainty bias is born out of a lack of training data for specific groups amplifying outlier behaviour, as well as the risk for over-fitting. One-sided criteria can serve to reinforce a bias that is generated by society: Even ruling out gender, names and images from hiring decisions a focus on years of leadership experience gives an advantage to those more likely exposed to leadership roles - typically neither people of colour, nor people from poorer districts. One-sided hardware can make interaction harder - think face recognition systems having trouble identifying non-white humans, having trouble identifying non-male humans.\nIn the EU we focus on the precautionary principle where launching new technology means showing it\u0026rsquo;s not harmful. This though proves more and more complex as technology becomes entrenched in everyday life.\nWhat other biases do humans have? There\u0026rsquo;s information biases, where humans tend to reason based on analogy, based on the illusion of control (overestimating oneself, downplaying risk, downplaying uncertainty), there\u0026rsquo;s an escalation of committment (a tendency to stick to a decision even if it\u0026rsquo;s the wrong one), there are single outcome calculations.\nFor cognitive biases are related to framing, criteria selection (we tend to value quantitative criteria over qualitative criteria), rationality. There\u0026rsquo;s risk biases (uncertainties about positive outcomes typically aren\u0026rsquo;t seen as risks, risk tends to be evaluated by magnitude rather than by a combination of magnitude and probability). There\u0026rsquo;s attitude based biases: In experiments senior managers considered risk taking as part of their job. The level of risk taken depended on the amount of positive performance feedback given to a certain person: The better people believe they are, the more risk they are willing to take. Uncertainty biases relate to the difference between the information I believe I need vs. the information available - in experiments humans made worse decisions the more data and information was available to them.\nGeneral advise: Beware of your biases\u0026hellip;\n"},{"id":16,"href":"/dataworkssummit-berlin-wednesday-morning/","title":"DataworksSummit Berlin - Wednesday morning","section":"Inductive Bias","content":" DataworksSummit Berlin - Wednesday morning # Data strategy - cloud strategy - business strategy: Aligning the three was one of the main themes (initially put forward in his opening keynote by CTO of Hortonworks Scott Gnau) thoughout this weeks Dataworks Summit Berlin kindly organised and hosted by Hortonworks. The event was attended by over 1000 attendees joining from 51 countries.\nThe inspiration hat was put forward in the first keynote by Scott was to take a closer look at the data lifecycle - including the fact that a lot of data is being created (and made available) outside the control of those using it: Smart farming users are using a combination of weather data, information on soil conditions gathered through sensors out in the field in order to inform daily decisions. Manufacturing is moving towards closer monitoring of production lines to spot inefficiencies. Cities are starting to deploy systems that allow for better integration of public services. UX is being optimized through extensive automation.\nWhen it comes to moving data to the cloud, the speaker gave a nice comparison: To him, explaining the difficulties that moving to the cloud brings is similar to the challenges that moving \u0026ldquo;stuff\u0026rdquo; to external storage in the garage brings: It opens questions of \u0026ldquo;Where did I put this thing?\u0026rdquo;, but also about access control, security. Much the same way, cloud and on-prem integration means that questions like encryption, authorization, user tracking, data governance need to be answered. But also questions like findability, discoverability and integration for analysis purposes.\nThe second keynote was given by Mandy Chessell from IBM introducing Apache Atlas for metadata integration and governance.\nIn the third keynote, Bernard Marr talked about the five promises of big data:\nInforming decisions based on data: The goal here should be to move towards self service platforms to remove the \"we need a data scientist for that\" bottleneck. That in turn needs quite some training and hand-holding for those interested in the self-service platforms. Understanding customers and customer trends better: The example given was a butcher shop that would install a mobile phone tracker in his shop window in order to see which advertisement would make more people stop by and look closer. As a side effect he noticed an increase in people on the street in the middle of the night (coming from pubs nearby). A decision was made to open at that time, offer what people were searching for at that time according to Google trends - by now that one hour in the night makes a sizeable portion of the shop's income. The second example given was Disney already watching all it's Disney park visitors through wrist bands, automating line management at popular attractions - but also deploying facial recognition watching audiences watch shows in figure out how well those shows are received. Improve the customer value proposition: The example given was the Royal Bank of Scotland moving closer to it's clients, informing them through automated means when interest rates are dropping, or when they are double insured - thus building trust and transparency. The other example given was that of a lift company building sensors into lifts in order to be able to predict failures and repair lifts when they are least used. Automate business processes: Here the example was that of a car insurance that would offer dynamic rates if people would let themselves monitor during driving. Those adhering to speed limits, avoiding risky routes and times would get lower rates. Another example was that of automating the creation of sports reports e.g. for tennis matches based on sensors deployed, or that of automating Forbes analyst reports some of which get published without the involvement of a journalist. Last but not least the speaker mentioned the obvious business case of selling data assets - e.g. selling aggregated and refined data gathered through sensors in the field back to farmers. Another example was the automatic detection of events based on sounds detected - e.g. gun shots close to public squares and selling that back to the police. After the keynotes were over breakout sessions started - including my talk about the Apache Way. It was good to see people show up to learn how all the open source big data projects are working behind the scences - and how they themselves can get involved in contributing and shaping these projects. I\u0026rsquo;m looking forward to receiving pictures of feather shaped cookies. During lunch there was time to listen in on how Santander operations is using data analytics to drive incident detection, as well as load prediction for capacity planning. After lunch I had time for two more talks: The first explained how to integrate Apache MxNet with Apache NiFi to bring machine learning to the edge. The second one introduced Apache Beam - an abstraction layer above Apache Flink, Spark and Google\u0026rsquo;s platform. Both, scary and funny: Walking up to the Apache Beam speaker after his talk (having learnt at DataworksSummit that he is PMC Chair of Apache Beam) - only to be greeted with \u0026ldquo;I know who you are\u0026rdquo; before even getting to introduce oneself\u0026hellip;\n"},{"id":17,"href":"/apache-breakfast/","title":"Apache Breakfast","section":"Inductive Bias","content":" Apache Breakfast # In case you missed it but are living in Berlin - or are visiting Berlin/ Germany this week: A handful of Apache people (committers/ members) are meeting over breakfast on Friday morning this week. If you are interested in joining, please let me know (or check yourself - in the archives of the mailing list party@apache.org)\n"},{"id":18,"href":"/foss-backstage-schedule-online/","title":"FOSS Backstage - Schedule online","section":"Inductive Bias","content":" FOSS Backstage - Schedule online # In January the CfP for FOSS Backstage opened. By now reviews have been done, speakers notified and a schedule created. I\u0026rsquo;m delighted to find both - a lot of friends from the Apache Software Foundation but also a great many speakers that aren\u0026rsquo;t affiliated with the ASF among the speakers. If you want to know how Open Source really works, if you want to get a glimpse behind the stage, do not wait for too long to grab your ticket now and join us in summer in Berlin/ Germany. If project management is only partially of your interest, we have you covered as well: For those interested in storing, searching and scaling data analysis, Berlin Buzzwords is scheduled to take place in the same week. For those interested in Tomcat, httpd, cloud and iot, Apache Roadshow is scheduled to happen on the same days as FOSS Backstage - and your FOSS Backstage ticket grants you access to Apache Roadshow as well. If you\u0026rsquo;re still not convinced - head over to the conference website and check out the talks available yourself.\n"},{"id":19,"href":"/my-board-nomination-statement-2018/","title":"My board nomination statement 2018","section":"Inductive Bias","content":" My board nomination statement 2018 # Two days ago the Apache Software Foundation members meeting started. One of the outcomes of each members meeting is an elected board of directors. The way that works is explained here: Annual Apache members meeting. As explained in the linked post, members accepting their nomination to become a director are supposed to provide a nomination statement. This year they were also asked to answer a set of questions so members could better decide who to vote for.\nAs one of my favourite pet peeves is to make the inner workings of the foundation more transparent to outsiders (and have said so in the nomination statement) - I would like to start by publishing my own nomination statement here for others to read who don\u0026rsquo;t have access to our internal communication channels:\nBoard statement: Two years ago I was put on a roller coaster by being nominated as Apache board member which subsequently meant I got to serve on the board in 2016. Little did I know what kind of questions were waiting for me. Much like back then I won\u0026rsquo;t treat this position statement as a voting campaign. I don\u0026rsquo;t claim to have answers to all the questions we face as we grow larger - however I believe being a board member even at our size should be something that is fun. Something that is lightweight enough so people don\u0026rsquo;t outright decline their nominations just for lack of time.\nOne thing I learnt the hard way is scalability needs two major ingredients: Breaking dependencies and distribution of workload. Call me old-fashioned (even though chemistry can hide my gray hair, my preference for mutt as a mail client betrays my age), but I believe we already have some of the core values to achieve just that:\n\"Community over code\" to me includes rewarding contributions that aren't code. I believe it is important to get people into the foundation that are committed to both our projects as well as the foundation itself - helping us in all sorts of ways, including but not limited to coding, documenting, marketing, mentoring, legal, education and more. \"What didn't happen on the mailing list didn't happen\" to me means communicating as publicly as possible (while keeping privacy as needed) to enable others to better understand where we are, how we work, what we value and ultimately how to help us. I would like for us to think twice before sending information to private lists - both at the project and at the operational level. I believe we can do better in getting those into the loop who have a vested interest in seeing that our projects are run in a vendor neutral way: Our downstream users who rely on Apache projects for their daily work. I am married to a Linux kernel geek working for the Amazon kernel and operating systems team - I've learnt a long time ago that the Open Source world is bigger than just one project, bigger than just one foundation. Expect me to keep the bigger picture in mind during my work here that is not ASF exclusive. Much like Bertrand I\u0026rsquo;m a European - that means I do see value in time spent offline, in being disconnected. I would like to urge others to take that liberty as well - if not for yourselves, then at least to highlight where we are still lacking in terms of number of people that can take care of a vital role.\nAs you may have guessed from the time it took for me to accept this nomination, I didn\u0026rsquo;t take the decision lightly. For starters semi-regularly following the discussion on board@ to me feels like there are people way more capable than myself. Seeing just how active people are feels like my time budget is way too limited.\nSo what made me accept? I consider myself lucky seeing people nominated for the Apache board who are capable leaders that bring very diverse skills, capabilities and knowledge with them that taken together will make an awesome board of directors.\nI know that with FOSS Backstage one other \u0026ldquo;pet project of mine\u0026rdquo; is in capable hands, so I don\u0026rsquo;t need to be involved in it on a day-to-day basis.\nLast but not least I haven\u0026rsquo;t forgotten that back in autumn 2016 Lars Trieloff* told me that I am a role model: Being an ASF director, while still working in tech, with a today three year old at home. As the saying goes \u0026ldquo;Wege entstehen dadurch, dass man sie geht\u0026rdquo; - free-form translation: \u0026ldquo;paths are created by walking them.\u0026rdquo; So instead of pre-emptively declining my nomination I would like to find a way to make the role of being a Director at the Apache Software Foundation something that is manageable for a volunteer. Maybe along that way we\u0026rsquo;ll find a piece in the puzzle to the question of who watches the watchmen - how do we reduce the number of volunteers that we burn through, operating at a sustainable level, enabling people outside of the board of directors to take over or help with tasks.\nWhom I know through the Apache Dinner/ Lunch Berlin that I used to organise what feels like ages ago. We should totally re-instate that again now that there are so many ASF affiliated people in or close to Berlin. Any volunteers? The one who organises gets to choose date and location after all ;) Answers to questions to the board nominees: On Thu, Mar 15, 2018 at 01:57:07PM +0100, Daniel Gruno wrote:\n\u0026gt; Missions, Visions\u0026hellip;and Decisions:\n\u0026gt; - The ASF exists with a primary goal of \u0026ldquo;providing open source\n\u0026gt; software to the public, at no charge\u0026rdquo;. What do you consider to be\n\u0026gt; the foundation\u0026rsquo;s most important secondary (implicit) goal?\nI learnt a lot about what is valuable to us in the following discussion:\nhttps://s.apache.org/hadw\n(and the following public thread over on dev@community with the same subject. My main take-away from there came from Bertrand: The value we are giving back to projects is by providing \u0026ldquo;A neutral space where they can operate according to our well established best practices.\u0026rdquo;\nThe second learning I had just recently when I had the chance of thinking through some of the values that are encoded in our Bylaws that you do not find in those of other organisations: At the ASF you pay for influence with time (someone I respect a lot extended that by stating that you actually pay with time and love).\n\u0026gt; - Looking ahead, 5 years, 10 years\u0026hellip;what do you hope the biggest\n\u0026gt; change (that you can conceivably contribute to) to the foundation\n\u0026gt; will be, if any? What are your greatest concerns?\nOne year ago I had no idea that little over two months from now we would have something like FOSS Backstage here in Berlin: One thing the ASF has taught me is that predicting the future is futile - the community as a whole will make changes in this world that are way bigger than the individual contributions taken together.\n\u0026gt; \u0026lt; - Which aspect(s) (if any) of the way the ASF operates today are you \u0026gt; least satisfied with? What would you do to change it?\nThose are in my position statement already.\n\u0026gt; #######################################\n\u0026gt; Budget and Operations:\n\u0026gt; - Which roles do you envision moving towards paid roles. Is this the\n\u0026gt; right move, and if not, what can we do to prevent/delay this?\n\u0026gt; Honestly I cannot judge what\u0026rsquo;s right and wrong here. I do know that burning through volunteers to me is not an option. What I would like to hear from you as a member is what you would need to step up and do operational tasks at the ASF.\nSome random thoughts:\nDo we have the right people in our membership that can fill these operational roles? Are we doing a good enough job in bringing people in with all sorts of backgrounds, who have done all sorts of types of contributions? Are we doing a good enough job at making transparent where the foundation needs operational help? Are those roles small enough to be filled by one individual? This question could be read like today work at the ASF is not paid for. This is far from true - both at the project as well as at the operational level. What I think we need is collective understanding of what the implications of various funding models are: Even if the ASF doesn\u0026rsquo;t accept payment for development doesn\u0026rsquo;t directly imply that projects are more independent as a result. I would assume the same to be true at the operational level.\n\u0026gt; #######################################\n\u0026gt;\n\u0026gt; Membership and Governance:\n\u0026gt; - Should the membership play a more prominent role in\n\u0026gt; decision-making at the ASF? If so, where do you propose this be?\nI may be naive but I still believe in the \u0026ldquo;those who do the work are those who take decisions\u0026rdquo;. There only close to a dozen people who participated in the \u0026ldquo;ask the members questionaire\u0026rdquo; I sent around - something that was troubling for me to see was how pretty much everyone wanted\n\u0026gt; - What would be your take on the cohesion of the ASF, the PMCs, the\n\u0026gt; membership and the communities. Are we one big happy family, or\n\u0026gt; just a bunch of silos? Where do you see it heading, and where do\n\u0026gt; we need to take action, if anywhere?\nIf \u0026ldquo;one big happy family\u0026rdquo; conjures the picture of people with smiling faces only, than that is a very cheesy image of a family that in my experience doesn\u0026rsquo;t reflect reality of what families typically look like.\nThis year at FOSDEM in Brussels we had a dinner table of maybe 15 people (while I did book the table, I don\u0026rsquo;t remember the exact number - over-provisioning and a bit of improvisation helped a lot in making things scale) from various projects, who joined at various times. I do remember a lot of laughter at that table. If anything I think we need the help people to bump into each other face to face independently of their respective project community more often.\n\u0026gt; - If you were in charge of overall community development (sorry,\n\u0026gt; Sharan!), what would you focus on as your primary and secondary\n\u0026gt; goal? How would you implement what you think is needed to achieve\n\u0026gt; this?\nI\u0026rsquo;m not in charge in that - nor would I want to be, nor should I be. The value I see in the ASF is that we rely very heavily on self organisation, so this foundation is what each individual in it makes out of it - and to me those individuals aren\u0026rsquo;t limited to foundation members, PMC members or even committers. In each Apache Way talk I\u0026rsquo;ve seen (and everytime I explain the Apache Way to people) the explanation starts with our projects\u0026rsquo; downstream users.\n\u0026gt; Show and Tell:\nI\u0026rsquo;m not much of a show and tell person. At ApacheCon Oakland I once was seeking help with getting a press article about ApacheCon reviewed. It was easy finding a volunteer to proof-read the article. The reason for that ease given by the volunteer themselves? What they got out of their contributions to the ASF was much bigger than anything they put into it. That observation holds true for me as well - and I do hope that this is true for everyone here who is even mildly active.\n"},{"id":20,"href":"/proxies-considered-harmful/","title":"An argument against proxies","section":"Inductive Bias","content":" An argument against proxies # Proxies? In companies getting started with an upstream first concept this is what people are called who act as the only interface between their employer and an open source project: All information from any project used internally flows through them. All bug reports and patches intended as upstream contribution also flows through them - hiding entire teams producing the actual contributions.\nAt Apache projects I learnt to dislike this setup of having proxies act in place of the real contributors. Why so?\nApache is built on the premise of individuals working together in the best interest of their projects. Over time, people who prove to commit themselves to a project get added to that project. Work contributed to a project gets rewarded - in a merit doesn\u0026rsquo;t go away kind-of sense working on an Apache project is a role independent of other work committments - in the \u0026ldquo;merit doesn\u0026rsquo;t go away\u0026rdquo; sense this merit is attached to the individual making contributions, not to the entity sponsoring that individual in one way or another.\nThis mechanism does not work anymore if proxy committers act as gateway between employers and the open source world: While proxied employees are saved from the tax that working in the public brings by being hidden behind proxies, they will also never be able to accrue the same amount of merit with the project itself. They will not be rewarded by the project for their committment. Their contributions do not end up being attached to themselves as individuals.\nFrom the perspective of those watching how much people contribute to open source projects the concept of proxy committers often is neither transparent nor clear. For them proxies establish a false sense of hyper productivity: The work done by many sails under the flag of one individual, potentially discouraging others with less time from participating: \u0026ldquo;I will never be able to devote that much work to that project, so why even start?\u0026rdquo;\nFrom an employer point of view proxies turn into single point of failure roles: Once that person is gone (on vacation, to take care of a relative, found a new job) they take the bonds they made in the open source project with them - including any street cred they may have gathered.\nLast but not least I believe in order to discuss a specific open source contribution the participants need a solid understanding of the project itself. Something only people in the trenches can acquire.\nAs a result you\u0026rsquo;ll see me try and pull those actually working with a certain project to get active and involved themselves, to dedicate time to the core technology they rely on on a daily basis, to realise that working on these projects gives you a broader perspective beyond just your day job.\n"},{"id":21,"href":"/fosdem-2018-recap/","title":"FOSDEM 2018 - recap","section":"Inductive Bias","content":" FOSDEM 2018 - recap # Too crowded, too many queues, too little space - but also lots of friendly people, Belgian waffles, ice cream, an ASF dinner with grey beards and new people, a busy ASF booth, bumping into friends every few steps, meeting humans you see only online for an entire year or more: For me, that\u0026rsquo;s the gist of this year\u0026rsquo;s FOSDEM.\nNote: German version of the article including images appeared in my employer\u0026rsquo;s tech blog. To my knowledge FOSDEM is the biggest gathering of free software people in Europe at least. It\u0026rsquo;s free of charge, kindly hosted by ULB, organised by a large group of volunteers. Every year early February the FOSS community meets for two one weekend in Brussels to discuss all sorts of aspects of Free and Open Source Software Development - including community, legal, business and policy aspects. The event features more than 600 talks as well as several dozen booths by FOSS projects and FOSS friendly companies. There\u0026rsquo;s several FOSDEM fringe events surrounding the event that are not located on campus. If you go to any random bar or restaurant in Brussels that weekend you are bound to bump into FOSDEM people.\nFortunately for those not lucky enough to have made it to the event, video recordings (unfortunately in varying quality) are available online at video.fosdem.org. Some highlights you might want to watch:\nExploiting modern microarchitectures - Meltdown, Spectre, and other hardware attacks by Jon Masters in case you want to understand what the Meltdown and Spectre and the underlying causes really mean for computer security. Passing the Baton: Succession Planning for FOSS leadership by VM (Vicky) Brasseur on why you should build hand over into your open source project before it's too late including some hints on how to best do that. Too young to Rock'n'Roll by Dominik George and Niels Hradek on what legal hurdles minors encounter when trying to contribute to open source projects. State of OpenJDK by Mark Reinhold if you're interested in the state of OpenJDK, both in terms of community as well as in terms of technical challenges ahead. One highlight for me personally this year: I cannot help but believe that I met way more faces from The Apache Software Foundation than at any other FOSDEM before. The booth was crowded at all times - Sharan Foga did a great job explaining The ASF to people. Also it\u0026rsquo;s great to hear The ASF mentioned in several talks as one of the initiatives to look at to understand how to run open source projects in a sustainable fashion with an eye on longevity. It was helpful to have at least two current Apache board members (Bertrand Delacretaz as well as Rich Bowen) on site to help answer tricky questions. Last but not least it was lovely meeting several of the Apache Grey Beards (TM) for an Apache Dinner on Saturday evening. Luckily co-located with the FOSDEM HPC speaker dinner - which took a calendar conflict out of the Apache HPC people\u0026rsquo;s calendar :)\nMe personally, I hope to see many more ASF people later this year in Berlin for FOSS Backstage - the advertisement sign that was located at the FOSDEM ASF booth last weekend already made it here, will you follow?\n"},{"id":22,"href":"/foss-backstage-cfp-open/","title":"FOSS Backstage - CfP open","section":"Inductive Bias","content":" FOSS Backstage - CfP open # It\u0026rsquo;s almost ten years ago that I attended my first ApacheCon EU in Amsterdam. I wasn\u0026rsquo;t entirely new to the topic of open source or free software. I attended several talks on Apache Lucene, Apache Solr, Hadoop, Tomcat, httpd (I still remember that the most impressive stories didn\u0026rsquo;t necessarily come from the project members, but from downstream users. They were the ones authorized to talk publicly about what could be done with the project - and often became committers themselves down the road. With \u0026ldquo;community over code\u0026rdquo; being one of the main values at Apache, ApacheCon also hosted several non-technical tracks: Open source and business, Open Development (nowadays better known as Inner Source), Open Source project management, project governance, an Apache Way talk. Over the past decade one learning survived any wave of tech buzzword: At the end of the day, success in Open Source (much like in any project) is defined by how well the project is run (read: managed). Reflecting on that the idea was born to create a space to discuss just these topics: What does it take to be \u0026ldquo;Leading the wave of open source\u0026rdquo;?\nAs announced on Berlin Buzzwords we (that is Isabel Drost-Fromm, Stefan Rudnitzki as well as the eventing team over at newthinking communications GmbH) are working on a new conference in summer in Berlin. The name of this new conference will be \u0026ldquo;FOSS Backstage\u0026rdquo;. Backstage comprises all things FOSS governance, open collaboration and how to build and manage communities within the open source space.\nSubmission URL: Call for Presentations\nThe event will comprise presentations on all things FOSS governance, decentralised decision making, open collaboration. We invite you to submit talks on the topics: FOSS project governance, collaboration, community management. Asynchronous/ decentralised decision making. Vendor neutrality in FOSS, sustainable FOSS, cross team collaboration. Dealing with poisonous people. Project growth and hand-over. Trademarks. Strategic licensing. While it\u0026rsquo;s primarily targeted at contributions from FOSS people, we would love to also learn more on how typical FOSS collaboration models work well within enterprises. Closely related topics not explicitly listed above are welcome.\nImportant Dates (all dates in GMT +2)\nSubmission deadline: February 18th, 2018.\nConference: June, 13th/14th, 2018\nHigh quality talks are called for, ranging from principles to practice. We are looking for real world case studies, background on the social architecture of specific projects and a deep dive into cross community collaboration. Acceptance notifications will be sent out soon after the submission deadline. Please include your name, bio and email, the title of the talk, a brief abstract in English language.\nWe have drafted the submission form to allow for regular talks, each 45 min in length. However you are free to submit your own ideas on how to support the event: If you would like to take our attendees out to show them your favourite bar in Berlin, please submit this offer through the CfP form. If you are interested in sponsoring the event (e.g. we would be happy to provide videos after the event, free drinks for attendees as well as an after-show party), please contact us.\nSchedule and further updates on the event will be published soon on the event web page.\nPlease re-distribute this CfP to people who might be interested.\nContact us at:\nnewthinking communications GmbH\nSchoenhauser Allee 6/7\n10119 Berlin, Germany\ninfo@foss-backstage.de\nLooking forward to meeting you all in person in summer :)\n"},{"id":23,"href":"/trust-and-confidence/","title":"Trust and confidence","section":"Inductive Bias","content":" Trust and confidence # One of the main principles at Apache (as in The Apache Software Foundation) is \u0026ldquo;Community over Code\u0026rdquo; - having the goal to build projects that survive single community members loosing interest or time to contribute. In his book \u0026ldquo;Producing Open Source Software\u0026rdquo; Karl Fogel describes this model of development as Consensus-based Democracy (in contrast to benevolent dictatorship): \u0026ldquo;Consensus simply means an agreement that everyone is willing to live with. It is not an ambiguous state: a group has reached consensus on a given question when someone proposes that consensus has been reached and no one contradicts the assertion. The person proposing consensus should, of course, state specifically what the consensus is, and what actions would be taken in consequence of it, if those are not obvious.\u0026rdquo;\nWhat that means is that not only one person can take decisions but pretty much anyone can declare a final decision was made. It also means decisions can be stopped by individuals on the project.\nThis model of development works well if what you want for your project is resilience to people, in particular those high up in the ranks, leaving at the cost of nobody having complete control. It means you are moving slower, at the benefit of getting more people on board and carrying on with your mission after you leave.\nThere are a couple implications to this goal: If for whatever reason one single entity needs to retain control over the project, you better not enter the incubator like suggested here. Balancing control and longevity is particularly tricky if you or your company believes they need to own the roadmap of the project. It\u0026rsquo;s also tricky if your intuitive reaction to hiring a new engineer is to give them committership to the project on their first day - think again keeping in mind that Money can\u0026rsquo;t buy love. If you\u0026rsquo;re still convinced they should be made committer, Apache probably isn\u0026rsquo;t the right place for your project.\nOnce you go through the process of giving up control with the help from your mentors you will learn to trust others - trust others to pick up tasks you leave open, trust others they are taking the right decision even if you would have done things otherwise, trust others to come up with solutions where you are lost. Essentially like Sharan Foga said to Trust the water.\nEven coming to the project at a later stage as an individual contributor you\u0026rsquo;ll go through the same learning experience: You\u0026rsquo;ll learn to trust others with the patch you wrote. You\u0026rsquo;ll have to learn to trust others to take your bug report seriously. If the project is well run, people will treat you as an equal peer, with respect and with appreciation. They\u0026rsquo;ll likely treat you as part of the development team with as many decisions as possible - after all that\u0026rsquo;s what these people want to recruit you for: For a position as volunteer in their project. Doing that means starting to Delegate like a Pro as Deb Nicholson once explained at ApacheCon. It also means training your capability for Empathy like Leslie Hawthorn explained at FOSDEM. It also means treating all contributions alike.\nThere\u0026rsquo;s one pre-requesite to all of this working out though: Working in the open (as in \u0026ldquo;will be crawled, indexed and made visible by the major search engine of the day\u0026rdquo;), giving control to others over your baby project and potentially over what earns your daily living means you need a lot of trust not onnly in others but also in yourself. If you\u0026rsquo;re in a position where you\u0026rsquo;re afraid that missteps will have negative repercussions on your daily life you won\u0026rsquo;t become comfortable with all of that. For projects coming to the incubator as well as companies paying contributors to become open source developers in their projects in my personal view that\u0026rsquo;s an important lesson: Unless committers already feel self confident and independent enough of your organisation as well as the team they are part of to take decisions on their own, you will run into trouble walking towards at least Apache.\n"},{"id":24,"href":"/open-source-summit-day-3/","title":"Open Source Summit - Day 3","section":"Inductive Bias","content":" Open Source Summit - Day 3 # Open source summit Wednesday started with a keynote by members of the Banks family telling a packed room on how they approached raising a tech family. The first hurdle that Keila (the teenage daughter of the family) talked about was something I personally had never actually thought about: Communication tools like Slack that are in widespread use come with an age restriction excluding minors. So by trying to communicate with open source projects means entering illegality. A bit more obivious was their advise to help raise kids\u0026rsquo; engagement with tech: Try to find topics that they can relate to. What works fairly often are reverse engineering projects that explain how things actually work.\nThe Banks are working with a goal based model where children get ten goals to pursue during the year with regular quarterly reviews. An intersting twist though: Eight of these ten goals are choosen by the children themselves, two are reserved for parents to help with guidance. As obvious as this may seem, having clear goals and being able to influence them yourselves is something that I believe is applicable in the wider context of open source contributor and project mentoring as well as employee engagement.\nThe speakers also talked about embracing children\u0026rsquo;s fear. Keila told the story of how she was afraid to talk in front of adult audiences - in particular at the keynote level. The advise that her father gave that did help her: You can trip on the stage, you can fall, all of that doesn\u0026rsquo;t matter for as long as you can laugh at yourself. Also remember that every project is not the perfect project - there\u0026rsquo;s always something you can improve - and that\u0026rsquo;s ok. This is fairly in line with the feedback given a day earlier during the Linux Kernel Panel where people mentioned how today they would never accept the first patch they themselves had once written: Be persistant, learn from the feedback you get and seek feedback early.\nLast but not least, the speakers advised to not compare your family to anyone, not even to yourself. Everyone arrives at tech via a different route. It can be hard to get people from being averse to tech to embrace it - start with a tiny little bit of motivation, from there on rely on self motivation.\nThe family\u0026rsquo;s current project turned business is to support L.A. schools to support children get a handle on tech.\nThe TAO of Hashicorp In the second keynote Hashimoto gave an overview of the Tao of Hashicorp - essentially the values and principles the company is built on. What I found interesting about the talk was the fact that these values were written down very early in the process of building up Hashicorp when the company didn\u0026rsquo;t have much more than five employees, comprised vision, roadmap and product design pieces and has been applied to every day decisions ever since.\nThe principles themselves cover the following points:\nWorkflows - not technologies. Essentially describing a UX first approach where tools are being mocked and used first before diving deeper into the architecture and coding. This goes as far as building a bash script as a mockup for a command line interface to see if it works well before diving into coding. Simple, modular and Comosable. Meaning that tools built should have one clear purpose instead of piling features on top of each other for one product. Communicating sequential processes. Meaning to have standalone tools with clear APIs. Immutability. Versioning through Codification. When having a question, the answer \"just talk to X\" doesn't scale as companies grow. There are several fixes to this problem. The one that Hashicorp decided to go for was to write knowledge down in code - instead of having a README.md detailing how startup works, have something people can execute. Automate. Resilient systems. Meaning to strive for systems that know their desired state and have means to go back to it. Pragmatism. Meaning that the principles above shouldn't be applied blindly but adjusted to the problem at hand. While the content itself differs I find it interesting that Hashicorp decided to communicate in terms of their principles and values. This kind of setup reminds me quite a bit about the way Amazon Leadership principles are being applied and used inside of Amazon.\nIntegrating OSS in industrial environments - by Siemens The third keynote was given by Siemens, a 170 year old, 350k employees rich German corporation focussed on industrial appliances. In their current projects they are using OSS in embedded projects related to power generation, rail automation (Debian), vehicle control, building automation (Yocto), medical imaging (xenomai on big machines).\nTheir reason for tapping into OSS more and more is to grow beyond their own capabilities.\nA challenge in their applications relates to long term stability, meaning supporiting an appliance for 50 years and longer. Running there appliances unmodified for years today is not feasible anymore due to policies and corporate standards that requrire updates in the field.\nTrouble they are dealing with today is in the cost of software forks - both, self inflicted and supplier caused forks. The amount of cost attached to these is one of the reasons for Siemens to think upstream-first, both internally as well as when choosing suppliers.\nAnother reason for this approach is to be found in trying to become part of the community for three reasons: Keeping talent. Learning best practices from upstream instead of failing one-self. Better communication with suppliers through official open source channels.\nOne project Siemens is involved with at the moment is the so-called Civil Infrastructure Platform project.\nAnother huge topic within Siemens is software license compliance. Being a huge corporation they rely on Fossology for compliance checking.\nLinus Torvalds Q\u0026A The last keynote of the day was an on stage interview with Linus Torvalds. The introduction to this kind of format was lovely: There's one thing Linus doesn't like: Being on stage and giving a pre-created talk. Giving his keynote in the form of an interview with questions not shared prior to the actual event meant that the interviewer would have to prep the actual content. :) The first question asked was fairly technical: Are RCs slowing down? The reason that Linus gave had a lot to do with proper release management. Typically the kernel is released on a time-based schedule, with one release every 2.5 months. So if some feature doesn\u0026rsquo;t make it into a release it can easily be integrated into the following one. What\u0026rsquo;s different with the current release is Greg Kroah Hartman having announced it would be a long term support release, so suddenly devs are trying to get more features into it.\nThe second question related to a lack of new maintainers joining the community. The reasons Linus sees for this are mainly related to the fact that being a maintainer today is still fairly painful as a job: You need experience to quickly judge patches so the flow doesn\u0026rsquo;t get overwhelming. On the other hand you need to have shown to the community that you are around 24/7, 365 days a year. What he wanted the audience to know is that despite occasional harsh words he loves maintainers, the project does want more maintainers. What\u0026rsquo;s important to him isn\u0026rsquo;t perfection - but having people that will stand up to their mistakes.\nOne fix to the heavy load mentioned earlier (which was also discussed during the kernel maintainers\u0026rsquo; panel a day earlier) revolved around the idea of having a group of maintainers responsible for any single sub-system in order to avoid volunteer burnout, allow for vacations to happen, share the load and ease hand-over.\nAsked about kernel testing Linus admitted to having been sceptical about the subject years ago. He\u0026rsquo;s a really big fan of random testing/ fuzzing in order to find bugs in code paths that are rarely if ever tested by developers.\nAsked about what makes a successful project his take was the ability to find commonalities that many potential contributors share, the ability to find agreement, which seems easier for systems with less user visibility. An observation that reminded my of the bikeshedding discussions.\nAlso he mentioned that the problem you are trying to solve needs to be big enough to draw a large enough crowd. When it comes to measuring success though his insight was very valuable: Instead of focussing too much on outreach or growth, focus on deciding whether your project solves a problem you yourself have.\nAsked about what makes a good software developer, Linus mentioned that the community over time has become much less homogenuous compared to when he started out in his white, male, geeky, beer-loving circles. The things he believes are important for developers are caring about what they do, being able to invest in their skills for a long enough period to develop perfection (much like athletes train a long time to become really sucessful). Also having fun goes a long way (though in his eyes this is no different when trying to identify a successful marketing person).\nWhile Linus isn\u0026rsquo;t particularly comfortable interacting with people face-to-face, e-mail for him is different. He does have side projects beside the kernel. Mainly for the reason of being able to deal with small problems, actually provide support to end-users, do bug triage. In Linux kernel land he can no longer do this - if things bubble up to his inbox, they are bound to be of the complex type, everything else likely was handled by maintainers already.\nHis reason for still being part of the Linux Kernel community: He likes the people, likes the technology, loves working on stuff that is meaningful, that people actually care about. On vacation he tends to check his mail three times a day to not loose track and be overwhelmed when he gets back to work. There are times when he goes offline entirely - however typically after one week he longing to be back.\nAsked about what further plans he has, he mentioned that for the most part he doesn\u0026rsquo;t plan ahead of time, spending most of his life reacting and being comfortable with this state of things.\nSpeaking of plans: It was mentioned that likely Linux 5.0 is to be released some time in summer 2018 - numbers here don\u0026rsquo;t mean anything anyway.\nNobody puts Java in a container J\u0026ouml;rg Schad from Mesosphere gave an introduction to how container technolgies like Docker really work and how that applies to software run in the JVM. He started off by explaining the advantages of containers: Isolating what\u0026rsquo;s running inside, supplying standard interfaces to deployed units, sort of the write once, run anywhere promise.\nCompared to real VMs they are more light weight, however with the caveat of using the host kernel - meaning that crashing the kernel means crashing all container instances running on that host as well. In turn they are faster to spin up, need less memory and less storage.\nSo which properties do we need to look at when talking about having a JVM in a container? Resource restrictions (CPU, memory, device visibility, blkio etc.) are being controlled by cgroups. Process spaces for e.g. pid, net, ipc, mnt, users and hostnames are being controlled through libcontainer namespaces.\nLooking at cgroups there are two aspects that are very obviously interesting for JVM deployments: For memory settings one can set hard and soft limits. However much in contrast to the JVM there is no such thing as an OOM being thrown when resources are exhausted. For CPUs available there are two ways to configure limits: cpushares lets you give processes a relative priority weighting. Cpusets lets you pin groups to specific cpus.\nGeneral advise is to avoid cupsets as it removes one level of freedom from scheduling, often leads to less efficiency. However it\u0026rsquo;s a good tool to avoid cup-bouncing, and to maximise cache usage.\nWhen trying to figure out the caveats of running JVMs in containers one needs to understand what the memory requirements for JVMs are: In addition to the well known, configurable heap memory, each JVM needs a bit of native JRE memory, perm get/ meta space, JIT bytecode space, JNO and NIO space as well as additional native space for threads. With permgen space turned native meta space that means that class loader leaks are capable of maxing out the memory of the entire machine - one good reason to lock JVMs in containers.\nThe caveats of putting JVMs into containers are related to JRE intialisation defaults being influenced by information like the number of cores available: It influences the number of JIT compilation threads, hotspot thresholds and limits.\nOne extreme example: When running ten JVM containers in a 32 core box this means that:\nEach JVM believes it's alone on the machine configuring itself to the maximally availble CPU count. pre-Java-9 the JVM is not aware of cpusets, meaning it will think that it can use all 32 cores even if configured to use less than that. Another caveat: JVMs typically need more resources on startup, leading to a need for overprovisioning just to get it started. Jörg promised a blog post to appear on how to deal with this question on the DC/OS blog soon after the summit.\nAlso for memory Java9 provides the option to look at memory limits set through cgroups. The (still experimental) option for that: -XX:+UseCGroupMemLimitForHeap\nAs a conclusion: Containers don\u0026rsquo;t hide the underlying hardware - which is both, good and bad.\nGoal - question - metric approach to community measurement In his talk on applying goals question metrics to software development management Jose Manrique Lopez de la Fuente explained how to successfully choose and use metrics in OSS projects. He contrasted the OKR based approach to goal setting with the goal question metric approach. In the latter one first thinks about a goal to achieve (e.g. \u0026ldquo;We want a diverse community.\u0026rdquo;), go from there to questions to help understand the path ot that goal better (\u0026ldquo;How many people from underrepresented groups do we have.\u0026rdquo;), to actual metrics to answer that question.\nKey to applying this approach is a cycle that integrates planning, making changes, checking results and acting on them.\nGoals, questions and metrics need to be in line with project goals, involve management and involve contributors. Metrics themselves are only useful for as long as they are linked to a certain goal.\nWhat it needs to make this approach successful is a mature organisation that understands the metrics\u0026rsquo; value, refrains from gaming the system. People will need training on how to use the metrics, as well as transparency about metrics.\nProjects dealing with applying more metrics and analytics to OSS projects include Grimoire Lab, CHAOSS (Community Health Analytics for OSS).\nThere\u0026rsquo;s a couple interesting books: Managing inner source projects. Evaluating OSS projects as well as the Grimoire training which are all available freely online.\nContainer orchestration - the state of play In his talk Michael Bright gave an overview of current container orchestration systems. In his talk he went into some details for Docker Swarm, Kubernetes, Apache Mesos. Technologies he left out are things like Nomad, Cattle, Fleet, ACS, ECS, GKE, AKS, as well as managed cloud. What became apparent from his talk was that the high level architecture is fairly similar from tool to tool: Orchestration projects make sense where there are enough microservices to be unable to treat them like pets with manual intervention needed in case something goes wrong. Orchestrators take care of tasks like cluster management, micro service placement, traffic routing, monitoring, resource management, logging, secret management, rolling updates.\nOften these systems build a cluster that apps can talk to, with masters managing communication (coordinated through some sort of distributed configuration management system, maybe some RAFT based consensus implementation to avoid split brain situations) as well as workers that handle requests.\nGoing into details Michael showed the huge takeup of Kubernetes compared to Docker Swarm and Apache Mesos, up the point where even AWS joined CNCF.\nFor Thursday I went to see Rich Bowen\u0026rsquo;s keynote on the Apache Way at MesosCon. It was great to hear how people were interested in the greater context of what Apache provides to the Mesos project in terms of infrastructure and mentoring. Also there were quite a few questions on what that thing called The Apache Software Foundation actually is at their booth at MesosCon.\nHopefully the initiative started on the Apache Community development mailing list on getting more information out on how things are managed at Apache will help spread the word even further.\nOverall Open Source Summit, together with it\u0026rsquo;s sister events like e.g. KVM forum, MesosCon as well as co-located events like the OpenWRT summit was a great chance to meet up with fellow open source developers and project leads, learn about technologies and processes both familiar was well as new (in my case the QEMU on UEFI talk clearly was above my personal comfort zone understanding things - here it\u0026rsquo;s great to be married to a spouse who can help fill the gaps after the conference is over). There was a fairly broad spectrum of talks from Linux kernel internals, to container orchestration, to OSS licensing, community management, diversity topics, compliance, and economics.\n"},{"id":25,"href":"/open-source-summit-day-2/","title":"Open source summit - Day 2","section":"Inductive Bias","content":" Open source summit - Day 2 # Day two of Open Source summit for me started a bit slow for lack of sleep. The first talk I went to was on \u0026ldquo;Developer tools for Kubernetes\u0026rdquo; by Michelle Noorali and Matt Butcher. Essentially the two of them showed two projects (Draft and Brigade to help ease development apps for Kubernetes clusters. Draft here is the tool to use for developing long running, daemon like apps. Brigade has the goal of making event driven app development easier - almost like providing shell script like composability to Kubernetes deployed pipelines.\nKubernetes in real life In his talk on K8s in real life Ian Crosby went over five customer cases. He started out by highlighting the promise of magic from K8s: Jobs should automatically be re-scheduled to healthy nodes, traffic re-routed once a machine goes down. As a project it came out of Google as a re-implementation of their internal, 15 years old system called Borg. Currently the governance of K8s lies with the Cloud Native Foundation, part of the Linux Foundation. So what are some of the use cases that Ian saw talking to customers: \"Can you help us setup a K8s cluster?\" - asked by a customer with one monolithic application deployed twice a year. Clearly that is not a good fit for K8s. You will need a certain level of automation, continuous integration and continuous delivery for K8s to make any sense at all. There were customers trying to get into K8s in order to be able to hire talent interested in that technology. That pretty much gets the problem the wrong way around. K8s also won't help with organisational problems where dev and ops teams aren't talking with each other. The first question to ask when deploying K8s is whether to go for on-prem, hosted externally or a mix of both. One factor pulling heavily towards hosted solution is the level of time and training investment people are willing to make with K8s. Ian told the audience that he was able to migrate a complete startup to K8s within a short period of time by relying on a hosted solution resulting in a setup that requires just one ops person to maintain. In that particular instance the tech that remained on-prem were Elasticsearch and Kafka as services. Another client (government related, huge security requirements) decided to go for on-prem. They had strict requirements to not connect their internal network to the public internet resulting in people carrying downloaded software on USB sticks from one machine to the other. The obvious recommendation to ease things at least a little bit is to relax security requirements at least a little bit here. In a third use case the customer tried to establish a prod cluster, staging cluster, test cluster, one dev cluster per developer - pretty much turning into a maintainance nightmare. The solution was to go for a one cluster architecture, using shared resources, but namespaces to create virtual clusters, role based access control for security, network policies to restrict which services can talk to each other, service level TLS to get communications secure. Looking at CI this can be taken one level furter even - spinning up clusters on the fly when they are needed for testing. In another customer case Java apps were dying randomly - apparently because what was deployed was using the default settings. Lesson learnt: Learn how it works first, go to production after that. Rebuilding trust through blockchains and open source Having pretty much no background in blockchains - other than knowing that a thing like bitcoin exists - I decided to go to the introductory \"Rebuilding trust through blockchains and open source\" talk next. Marta started of by explaining how societies are built on top of trust. However today (potentially accelerated through tech) this trust in NGOs, governments and institutions is being eroded. Her solution to the problem is called Hyperledger, a trust protocol to build an enterprise grade distributed database based on a permissioned block chain with trust built-in. Marta went on detailing eight use cases: Cross border payments: Currently, using SWIFT, these take days to complete, cost a lot of money, are complicated to do. The goal with rolling out block chains for this would be to make reconcillation real-time. Put information on a shared ledger to make it audible as well. At the moment ANZ, WellsFargo, BNP Paribas and BNY Mellon are participating in this POC. Healthcare records: The goal is to put pointers to medical data on a shared ledger so that procedures like blood testing are being done just once and can be trusted across institutions. Interstate medical licensing: Here the goal is to make treatment re-imbursment easier, probably even allowing for handing out fixed-purpose budgets. Ethical seafood movement: Here the goal is to put information on supply chains for seafood on a shared ledger to make tracking easier, audible and cheaper. The same applies for other supply chains, think diamonds, coffee etc. Real estate transactions: The goal is to keep track of land title records on a shared ledger for easier tracking, auditing and access. Same could be done for certifications (e.g. of academic titles etc.) Last but not least there is a POC to how how to use shared ledgers to track ownership of creative works in a distributed way and take the middleman distributing money to artists out of the loop. Kernel developers panel discussion For the panel discussion Jonathan Corbet invited five different Linux kernel hackers in different stages of their career, with different backgrounds to answer audience questions. The panel featured Vlastimil Babka, Arnd Bergmann, Thomas Gleixner, Narcisa Vasile, Laura Abbott. The first question revolved around how people had gotten started with open source and kernel development and what advise they would have for newbies. The one advise shared by everyone other than scratch your own itch and find something that interests you: Be persistant. Don\u0026rsquo;t give up. Talking about release cycles and moving too fast or too slow there was a comment on best practice to get patches into the kernel that I found very valuable: Don\u0026rsquo;t get started coding right away. A lot of waste could have been prevented if people just shared their needs early on and asked questions instead of diving right into coding. There was discussion on the meaning of long time stability. General consensus seemed to be that long term support really only includes security and stability fixes. No new features. Imaging adding current devices to a 20 year old kernel that doesn\u0026rsquo;t even support USB yet. There was a lovely quote by Narcisa on the dangers and advantages of using C as a primary coding language: With great power come great bugs. There was discussion on using \u0026ldquo;new-fangled\u0026rdquo; tools like github instead of plain e-mail. Sure e-mail is harder to get into as a new contributor. However current maintainer processes heavily rely on that as a tool for communication. There was a joke about implementing their own tool for that just like was done with git. One argument for using something less flexible that I found interesting: Aparently it\u0026rsquo;s hard to switch between subsystems just because workflows differ so much, so agreeing on a common workflow would make that easier.\nAsked for what would happen if Linus was eaten by a shark when scuba diving the answer was interesting: Likely at first there would be a hiding game because nobody would want to take up his work load. Next there would likely develop a team of maintainers collaborating in a consensus based model to keep up with things. In terms of testing - that depend heavily on hardware being available to test on. Think like the kernel CI community help a lot with that. I closed the day going to Zaheda Bhorat\u0026rsquo;s talk on \u0026ldquo;Love would you do - everyday\u0026rdquo; on her journey in the open source world. It\u0026rsquo;s a great motiviation for people to start contributing to the open source community and become part of it - often for life changing what you do in ways you would never have imagined before. Lots of love for The Apache Software Foundation in it.\n"},{"id":26,"href":"/open-source-summit-prague-2017-part-1/","title":"Open Source Summit Prague 2017 - part 1","section":"Inductive Bias","content":" Open Source Summit Prague 2017 - part 1 # Open Source Summit, formerly known as LinuxCon, this year took place in Prague. Drawing some 2000 attendees to the lovely Czech city, the conference focussed on all things Linux kernel, containers, community and governance. The first day started with three crowded keynotes: First one by Neha Narkhede on Keynotes Apache Kafka and the Rise of the Streaming Platform. Second one by Reuben Paul (11 years old) on how hacking today really is just childs play: The hack itself might seem like toying around (getting into the protocol of children's toys in order to make them do things without using the app that was intended to control them). Taken into the bigger context of a world that is getting more and more interconnected - starting with regular laptops, over mobile devices to cars and little sensors running your home the lack of thought that goes into security when building systems today is both startling and worrying at the same time. The third keynote of the morning was given by Jono Bacon on what it takes to incentivise communities - be it open source communities, volunteer run organisations or corporations. According to his perspective there are four major factors that drive human actions:\nPeople thrive for acceptance. This can be exploited when building communities: Acceptance is often displayed by some form of status. People are more likely to do what makes them proceed in their career, gain the next level in a leadership board, gain some form of real or artificial title. Humans are a reciprocal species. Ever heart of the phrase \"a favour given - a favour taken\"? People who once received a favour from you are more likely to help in the long run. People form habits through repetition - but it takes time to get into a habit: You need to make sure people repeat the behaviour you want them to show for at least two months until it becomes a habit that they themselves continue to drive without your help. If you are trying to roll out peer review based, pull request based working as a new model - it will take roughly two months for people to adapt this as a habit. Humans have a fairly good bullshit radar. Try to remain authentic, instead of automated thank yous, extend authentic (I would add qualified) thank you messages. When it comes to the process of incentivising people Jono proposed a three step model: From hook to reason to reward.\nHook here means a trigger. What triggers the incentivising process? You can look at how people participate - number of pull requests, amount of documentation contributed, time spent giving talks at conferences. Those are all action based triggers. What\u0026rsquo;s often more valuable is to look out for validation based triggers: Pull requests submitted, reviewed and merged. He showed an example of a public hacker leaderboard that had their evaluation system published. While that\u0026rsquo;s lovely in terms of transparency IMHO it has two drawbacks: It makes it much easier to evaluate known wanted contributions than what people might not have thought about being a valuable contribution when setting up the leadership board. With that it also heavily influences which contribtions will come in and might invite a \u0026ldquo;hack the leadership board\u0026rdquo; kind of behaviour.\nWhen thinking about reason there are two types of incentives: The reason could be invisible up-front, Jono called this submarine rewards. Without clear prior warning people get their reward for something that was wanted. The reason could be stated up front: \u0026ldquo;If you do that, then you\u0026rsquo;ll get reward x\u0026rdquo;. Which type to choose heavily depends on your organisation, the individual giving out the reward as well as the individual receiving the reward. The deciding factor often is to be found in which is more likely authentic to your organisation.\nIn terms of reward itself: There are extrinsic motivators - swag like stickers, t-shirts, give-aways. Those tend to be expensive, in particular if shipping them is needed. Something that in professional open source projects is often overlooked are intrinsic rewards: A Thank You goes a long way. So does a blog post. Or some social media mention. Invitations help. So do referrals to ones own network. Direct lines to key people help. Testimonials help.\nOverall measurement is key. So is concentrating on focusing on incentivising shared value.\nLimux - the loss of a lighthouse In his talk, Matthias Kirschner gave an overview of Limux - the Linux rolled out for the Munich administration project. How it started, what went wrong during evaluation, which way political forces were drawing.\nWhat I found very interesting about the talk were the questions that Matthias raised at the very end:\nDo we suck at desktop? Are there too many depending apps? Did we focus too much on the cost aspect? Is the community supportive enough to people trying to monetise open source? Do we harm migrations by volunteering - as in single people supporting a project without a budget, burning out in the process instead of setting up sustainable projects with a real budget? Instead of teaching the pros and cons of going for free software so people are in a good position to argue for a sustainable project budget? Within administrations: Did we focus too much on the operating system instead of freeing the apps people are using on a day to day basis? Did we focus too much on one star project instead of collecting and publicising many different free software based approaches? As a lesson from these events, the FSFE launched an initiative to drive developing code funded by public money under free licenses.\nDude, Where's My Microservice In his talk Dude, Where's My Microservice? - Tomasz Janiszewski from Allegro gave an introduction to what projects like Marathon on Apache Mesos, Docker Swarm, Kubernetes or Nomad can do for your Microservices architecture. While the examples given in the talk refer to specific technologies, they are intented to be general purpose. Coming from a virtual machine based world where apps are tied to virtual machines who themselves are tied to physical machines, what projects like Apache Mesos try to do is to abstract that exact machine mapping away. Is a first result from this decision, how to communicate between micro services becomes a lot less obvious. This is where service discovery enters the stage.\nWhen running in a microservice environment one goal when assigning tasks to services is to avoid unhealthy targets. In terms of resource utilization instead of overprovisioning the goal is to use just the right amount of your resources in order to avoid wasting money on idle resources. Individual service overload is to be avoided.\nLooking at an example of three physical hosts running three services in a redundant matter, how can assigning tasks to these instances be achieved?\nOne very simple solution is to go for a proxy based architecture. There will be a single point of change, there aren't any in-app dependencies to make this model work. You can implement fine-grained load balancing in your proxy. However this comes at the cost of having a single point of failure, one additional hop in the middle, and usually requires using a common protocol that the proxy understands. Another approach would be to go for a DNS based architecture: Have one registry that holds information on where services are located, but talking to these happens directly instead of through a proxy. The advantages here: No additional hop once the name is resolved, no single point of failure - services can work with stale data, it's protocol independent. However it does come with in-app dependencies. Load balancing has to happen local to the app. You will want to cache name resolution results, but every cache needs some cache invalidation strategy. In both solutions you will also still have logic e.g. for de-registrating services. You will have to make sure to register your service only once is successfully booted up.\nEnter the Service Mesh architecture, e.g. based on Linker.d, or Envoy. The idea here is to have what Tomek called a sidecar added to each service that talks to the service mesh controller to take care of service discovery, health checking, routing, load balancing, authn/z, metrics and tracing. The service mesh controller will hold information on which services are available, available load balancing algorithms and heuristics, repeating, timeouts and circuit breaking, as well as deployments. As a result the service itself no longer has to take care of load balancing, ciruict breaking, repeating policies, or even tracing.\nAfter that high level overview of where microservice orchestration can take you, I took a break, following a good friend to the Introduction to SoC+FPGA talk. It\u0026rsquo;s great to see Linux support for these systems - even if not quite as stable as would be an ideal world case.\nTrolling != Enforcement The afternoon for me started with a very valuable talk by Shane Coughlan on how Trolling doesn't equal enforcement. This talk was related to what was published on LWN earlier this year. Shane started off by explaining some of the history of open source licensing, from times when it was unclear if documents like the GPL would hold in front of courts, how projects like gplviolations.org proofed that indeed those are valid legal contracts that can be enforced in court.\nWhat he made clear was that those licenses are the basis for equal collaboration: They are a common set of rules that parties not knowing each other agree to adhere to. As a result following the rules set forth in those licenses does create trust in the wider community and thus leads to more collaboration overall.\nOn the flipside breaking the rules does erode this very trust. It leads to less trust in those companies breaking the rules. It also leads to less trust in open source if projects don\u0026rsquo;t follow the rules as expected.\nHowever when it comes to copyright enforcement, the case of Patrick McHardy does imply the question if all copyright enforcement is good for the wider community. In order to understand that question we need to look at the method that Patrick McHardy employs: He will get in touch with companies for seemingly minor copyright infringements, ask for a cease and desist to be signed and get a small sum of money out of his target. In a second step the process above repeats, except the sum extracted increases.\nUnfortunately with this approach what was shown is that there is a viable business model that hasn\u0026rsquo;t been tapped into yet. So while the activities by Patrick McHardy probably aren\u0026rsquo;t so bad in and off itself, they do set a precedent that others might follow causing way more harm.\nClearly there is no easy way out. Suggestions include establishing common norms for enforcement, ensuring that hostile actors are clearly unwelcome. For companies steps that can be taken include understanding the basics of legal requirements, understanding community norms, and having processes and tooling to address both. As one step there is a project called Open Chain publishing material on the topic of open source copyright, compliance and compliance self certification. Kernel live patching Following Tomas Tomecek's talk on how to get from Dockerfiles to Ansible Containers I went to a talk that was given by Miroslav Benes from SuSE on Linux kernel live patching. The topic is interesting for a number of reasons: As early as back in 2008 MIT developed something called Ksplice which uses jumps patched into functions for call redirection. The project was aquired by Oracle - and discontinued.\nIn 2014 SuSE came up with something called kGraft for Linux live patching based on immediate patching but lazy migration. At the same time RedHat developed kpatch based on an activeness check.\nIn the case of kGraft the goal was to be able to apply limited scope fixes to the Linux kernel (e.g. for security, stability or corruption fixes), require only minimal changes to the source code, have no runtime cost impact, no interruption to applications while patching, and allow for full review of patch source code. The way it is implemented is fairly obvious - in hindsight: It\u0026rsquo;s based on re-useing the ftrace framework. kGraft uses the tracer for inception but then asks ftrace to return back to a different address, namely the start of the patched function. So far the feature is available for x86 only.\nNow while patching a single function is easy, making changes that affect multiple funtions get trickier. This means a need for lazy migration that ensures function type safety based on a consistency model. In kGraft this is based on a per-thread flag that marks all tasks in the beginning and makes waiting for them to be migrated possible.\nFrom 2014 onwards it took a year to get the ideas merged into mainline. What is available there is a mixture of both kGraft and kpatch. What are the limitations of the merged approach? There is no way right now to deal with data structure changes, in particular when thinking about spinlocks and mutexes. Consistency reasoning right now is done manually. Architectures other than X86 are still an open issue. Documentation and better testing are open tasks.\n"},{"id":27,"href":"/open-development-and-inner-source-for-fun-and-profit/","title":"Open development and inner source for fun and profit","section":"Inductive Bias","content":" Open development and inner source for fun and profit # Last in a row if interesting talks at Adobe Open Source Summit was on Open Development/ Inner Source and how it benefits internal projects given by Michael Marth. Note: He knows there\u0026rsquo;s subtle differences between inner source and open development, but mentioned to use the terms interchangeably in his talk. So what is inner source all about? Essentially: Use all the tools and processes that already work for open source projects, just internally. (Company) public mailing lists, documentation, chat software, issue trackers. Taken to it\u0026rsquo;s core this is very simplistic though. The more interesting aspects are waiting when looking at the people interaction patterns that emerge.\nFirst off: The goal of making all interaction public and easy to follow for anyone is to attract more contributors. The richest source of contributors can be tapped into if your users are tech savvy as well. Being based on this assumption inner source works best when dealing with infrastructure software, or platform software where downstream users are developers themselves.\nAs a general rule of thumb: From 100 users, 10 will contribute, one will stick around and become a long term committer. This translates into a lot of effort for gaining additional hands for your project.\nSo - assuming what you want is a wildly successful open source project: You put your code on Github (or whatever the code hosting site of the day is), start some marketing effort - but still not magic happens, no stars, no unicorns, maybe there\u0026rsquo;s 10 pull requests, but that\u0026rsquo;s about it. What happened?\nArchitecting a community around an open source project is a long term investment: Over time you\u0026rsquo;ll end up training numerous newbies, help people getting started and convince some of those to contribute back.\nAccording to the speaker Michael Marth where that works best is for infrastructure projects: Where users can be turned into contributors and where projects can be turned into platform software that lasts for a decade and longer. In his opinion what is key are two factors: Enabling distributed decision making to let others participate, and a management style that lets the community take their own decisions instead of having one entity control the project. Usually what emerges from that is a distributed, peer-peer networked organisational structure with distributed teams, no calls, no standups, consensus based decision making.\nIn Michaels experience what works best it to adopt an open source working model from the very start. His recommendation for projects coming from comercial entities is to go to the Apache Software Foundation: There, proven principles and rules have been identified already. In addition going there gives the project much more credibility when it comes to convincing partners that decisions are actually being made in a distributed fashion without being controllable by one single entity. So telling a customer \u0026ldquo;We have to check this with the community first\u0026rdquo; as an answer to a feature request becomes much more credible.\nThe result of this approach are projects that under his guidance gained ten times as many people contributing to the project outside of the original entity than inside of it. The result Michael observed were partners that were much more likely to stick with the technology by means of co-owning it. Partners were participating in development. Also the project made for a lovely recruiting pipeline filled with people already familiar with the technology.\n"},{"id":28,"href":"/note-to-self-slides-for-staying-sane-when-maintaining-a-popular-open-source-project/","title":"Note to self - slides for staying sane when maintaining a popular open source project","section":"Inductive Bias","content":" Note to self - slides for staying sane when maintaining a popular open source project # For further reference - Simon MacDonald has a great collection of good advise on how to stay sane when running and maintaining a popular open source project. Link here: http://s.apache.org/sanity\nSome things he mentioned:\nInclude a README. It should tell people what the project is about but also what the project is not about. It should have some sort of getting started guide. Potentially link to a CONTRIBUTING doc.\nContribution guidelines should outline social rules like a code of conduct, technical instructions like how to submit a pull request, a style guide, information on how to build the project and make changes etc.\nAdd a LICENSE file - any OSS license really, because by default it won\u0026rsquo;t be open source in no jurisdiction. Add file headers to each file you publish.\nDecide how to handle questions vs. issues: Either both in the issue tracker, or in separate venues.\nAdd an issue template that asks the user if they searched for the issue already, asks for expected behaviour, actual behaviour, logs, reproduction code, version number used. A note on issues: Having many issues is a good thing - it shows your project is popular. Having only many stale issues is a bad thing - nobody is caring for the project anymore.\nClose issues that don\u0026rsquo;t follow the template. Close issues that are duplicates. Close issues that are non active after asking for user input a while ago. Repeated issues asking for seamingly obvious things: Turn those into additional documentation. Asks for easy to add functionality: Let it sit for a while to give others a chance to do it and get involved. Same for bugs that are easy to fix.\nOverall people are more difficult than code. Expect trolls to show up. Remain empathetic, respectful but firm in your communication. Don\u0026rsquo;t be afraid to say no to external requests even if they are urgent for the requester.\nAdd a pull request template that asks for a description, related issue, type tag. Remember that you don\u0026rsquo;t have to merge every pull request.\nBuild a community: Make it easy to contribute, identify beginner bugs, document the hell out of your project, turn contributors into maintainers, thank people for their effort.\nHave tests but keep build times low.\nAdd documentation, at the very least a README file, a how to contribute file, break those files into a separate website once they grow too large.\nAs for releasing: Automate as much as you can. Three options: time based release schedule, release on every commit, release \u0026ldquo;when it\u0026rsquo;s done\u0026rdquo;.\n"},{"id":29,"href":"/async-decision-making/","title":"Async decision making","section":"Inductive Bias","content":" Async decision making # This is the second in a series of posts on inner source/open source. Bertrand Delacretaz gave an interesting talk on how to avoid meetings by introducing an async way of making decisions.\nHe started off with a little anecdote related to Paul Graham\u0026rsquo;s maker\u0026rsquo;s vs. manager\u0026rsquo;s schedule: Bertrand\u0026rsquo;s father was a carpenter. He was working in the same house that his family was living in, so joining the family for lunch was common for him. However there was one valid excuse for him to skip lunch: \u0026ldquo;I\u0026rsquo;m glueing.\u0026rdquo; How\u0026rsquo;s that? Glueing together a chair is a process that cannot be interrupted once started without ruining the entire chair - it\u0026rsquo;s a classical maker task that can either be completed in one go, or not at all.\nSoftware development is pretty similar to that: People typically need several hours of focus to get anything meaningful done, in my personal experience at least two (for smaller bugs) or four (for larger changes). That\u0026rsquo;s the motivation to keep forced interruptions low for development teams.\nManagers tend to be on a completely different schedule: Context switching all day, communicating all day adding another one hour meeting to the schedule doesn\u0026rsquo;t make much of a difference.\nThe implication of this observation: Adding one hour of meeting time to an engineer\u0026rsquo;s schedule comes with an enourmous cost if we factor the interruption into the equation. Adding to the equation that lots of meetings actually fail for various reasons (lack of preparation, lack of participants getting prepared, bad audio or video quality, missing participants, delayed start time, bad summarisation) it seems valid to ask if there is a way to reduce the number of face to face meetings while still remaining operational.\nAs communication still remains key to a functional organisation, one approach taken by open development at Adobe (as well as at the Apache Software Foundation really) is to differentiate between things that can only be discussed in person and decisions that can be taken in an asynchronous fashion. While this doesn\u0026rsquo;t reduce the amount of time communicating (usually quite the contrary happens) it does allow for participants to participate pretty much on their own schedule thus reducing the number of forced interruptions.\nHow does that work in practice? In Bertrand\u0026rsquo;s experience decision making tends to be a four step process: Coming from an open brainstorming sessions, options need to be condensed, consensus established and finally a decision needs to be made.\nIn terms of tooling in his experience what works best is to have one and only one shared communication medium for brainstorming. At Apache those are good old mailing lists. In addition there is a need for a structured issue tracker/ case management tool to make options and choices obvious, decisions visible and archived.\nWhen looking at tooling we are missing one important ingredient though: Each meeting needs extensive preparation and thourough post processing. As an example lets take the monthly Apache board of directors\u0026rsquo; meeting: It\u0026rsquo;s scheduled to last no longer than two hours. Given each of hundreds of projects are required to report on a quarterly basis, given that executive officers need to provide reports on a monthly basis, given that each month at least one major decision item comes up and given that there is still day to day decisions about personel, budget and the like to be taken: How does reducing that to two hours work? The secret sauce is a text file in svn + a web frontend called whimsy to it.\nDirectors will read through those reports ahead of the meeting. They will add comments to them (which will be mailed automatically to projects), often those comments are used by directors to communicate with each other as well. They will pre-approve reports, they will mark them for discussion if there is something fishy. Some people will check projects\u0026rsquo; lists to match that up with what\u0026rsquo;s being discussed in the report, some will focus on community stuff, some will focus on seeing releases being mentioned. If a report gets enough pre-approvals an no mark \u0026ldquo;to be discussed\u0026rdquo; they are not being shown or touched in the real meeting.\nThat way most of the discussion happens before the actual meeting leaving time for those issues that are truely contentious. As the meeting is open for anyone in te foundation to attend questions raised beforehand that could not be resolved in writing can be answered in the voice call fairly quickly.\nSpeaking of call: How does the actual meeting proceed? All participants dial in via good old telephone. Everyone is on a telephone so the issue of \u0026ldquo;discussions among people in the same room are hard to understand for remote participants\u0026rdquo; doesn\u0026rsquo;t occur. In addition to telephone there\u0026rsquo;s an IRC backchannel for background discussion, chatter, jokes and less relevant discussion. All discussion that has to be archived and that relates to discussions is kept on the voice channel though. During the meeting the board\u0026rsquo;s chair is moderating through the agenda. In addition the secretary will make notes of who attended, which discussions were made and which arguments exchanged. Those notes are being shared after the meeting, approved at the following month\u0026rsquo;s meeting and published thereafter. If you want to dig deeper into any project\u0026rsquo;s history, there\u0026rsquo;s tooling to drill down into meeting minutes per project until the very beginning of the foundation.\nDoes the above make decision making faster? Probably not. However it enables an asynchronous work mode that fits best with a group of volunteers working together in a global, distributed community where participants do not only live in different geographies and timezones but are on different schedules and priorities as well.\n"},{"id":30,"href":"/notebook-oss-office-at-adobe/","title":"Notebook - OSS office at Adobe","section":"Inductive Bias","content":" Notebook - OSS office at Adobe # tl;dr: This post summarises what I learnt at the Adobe Open Source Summit last week about which aspects to think of when running an open source office. It\u0026rsquo;s mainly a mental note for myself, hopefully others will find it useful as well.\nLonger version:\nThis is another post in a series of articles on \u0026ldquo;random stuff I leant in Basel last week\u0026rdquo;. When I was invited to Adobe\u0026rsquo;s open source summit I had no idea what to expect from it. In retrospect I\u0026rsquo;m glad I accepted the invitation - it was a great mix of talks, a valuable insight into how others are adopting open source processes and their motivation for open sourcing code and helping out upstream.\nThe first in a series of talks gave an overview of Adobe\u0026rsquo;s open source office. Currently the expertise collected there includes one human with a software development background, one human with a marketing background and one human with program management background showing the breadth of topics faced when interacting with open source projects.\nThe self-set goal of these people is to make contributing to open source as easy as possible. That includes making people comfortable internally with the way many open source projects work. It means making (e.g. legal, branding) review processes of contributions internally as easy as possible. It means supporting teams with checking contributions for license conformance, sensitive (e.g. personal) information that shouldn\u0026rsquo;t be shared publicly. It also means supporting teams that want to run their own open source projects.\nOne idea that I found very intriguing was to run their team through a public github repository, in a sort-of eat-your-own-dogfood kind-of way.\nOne of the artifacts maintained in that repository is a launch checklist that people can follow which includes reminders for creating a blog post, publishing on the website, sharing information on social media. Most likely those are aspects that are often forgotten even in established projects when it comes to topics like releasing and promoting a new software release version.\nAnother aspect I found interesting was the creation of an OSS advisory board - a group of people intersted in open development and open source. A wider group of people tasked with figuring out how to best approach open development, how to get the organisation to move away from a private by default strategy towards an open by default way of collaborating across team boundaries.\nMany of these topics fit well within what was shared earlier this year in the Linux foundation webcast on how to go about creating an open source office in organisations.\n"},{"id":31,"href":"/agile-inner-source-open-development-open-source-development/","title":"Agile, inner source, open development, open source development","section":"Inductive Bias","content":" Agile, inner source, open development, open source development # Last week I had the honour of giving a keynote at Adobe\u0026rsquo;s Open Source Summit EU in Basel. Among many interesting talks they hosted a panel to answer questions around all things open source, strategy, inner source, open development. One of the questions I found intersting is how inner source/ open development and agile are related.\nTo get everyone on the same page, what do I mean when talking about inner source/ open development? I first encountered the concept at ApacheCon EU Amsterdam in 2008: Bertrand Delacretaz, then Day Software, now Adobe was talking about how to apply open source development principles in a corporate environment. A while ago I ran across the term http://innersourcecommons.org that essentially wants to achieve the same thing. What I find intersting about it is the idea of applying values like transparency, openess to contributions as well as concepts like asynchronous communication and decision making at a wider context.\nWhat does that have to do with Agile? Looking at the mere mechanics of Agile like being co-located, talking face to face frequently, syncing daily in in person meetings this seems to be pretty much opposite of what open source teams do.\nStarting with the values and promises of Agile though I think the two can compliment each other quite nicely:\n\u0026ldquo;Individuals and interactions over processes and tools\u0026rdquo; - there\u0026rsquo;s a saying in many successful popular projects that the most important ingredient to a successful open source project is to integrate people into a team working towards a shared vision. Apache calls this Community over code, ZeroMQ\u0026rsquo;s C4 calls it \u0026ldquo;People before code\u0026rdquo;, I\u0026rsquo;m sure there are many other incarnations of the same concept.\nMuch like in agile, this doesn\u0026rsquo;t mean that tools are neglected alltogether. Git was created because there was nothing quite like this tool before. However at the core of it\u0026rsquo;s design was the desire for ideal support for the way the Linux community works together.\n\u0026ldquo;Working software over comprehensive documentation\u0026rdquo; - While people strive for good documentation I\u0026rsquo;d be so bold as to suggest that for the open source projects I\u0026rsquo;m familiar with the most pressing motivation to provide good documentation is to lower the number of user questions as well as the amount of questions people involved with the project have to answer.\nFor projects like Apache Lucene or Apache httpd I find the documentation that comes with the project to be exceptionally good. In addition both projects are supported with dead tree documentation even. My guess would be that working on geographically distributed teams who\u0026rsquo;s members work on differing schedules is one trigger for that: Given teams that are geographically distributed over multiple time zones means that most communication will happen in writing anyway. So instead of re-typing the same answers over and over, it\u0026rsquo;s less cost intensive to type them up in a clean way and post them to a place that has a stable URL and is easy to discoveri. So yes, while working software clearly is still in focus, the lack for frequent face to face communication can have a positive influence on the availability of documentation.\n\u0026ldquo;Customer collaboration over contract negotiation\u0026rdquo; - this is something open source teams take to the extreme: Anybody is invited to participate. Answers to feature requests like \u0026ldquo;patches welcome\u0026rdquo; typically are honest requests for support and collaboration, usually if taken up resulting in productive work relationships.\n\u0026ldquo;Responding to change over following a plan\u0026rdquo; - in my opinion this is another value that\u0026rsquo;s taken to the extreme in open source projects. Without control over your participants, no ability to order people to do certain tasks and no influence over when and how much time someone else can dedicate to a certain task reaciting to changes is core to each open source project cosisting of more than one maintaining entity (be it single human being or simply one company paying for all currently active developers).\nOne could look further, digging deeper into how the mechanics of specific Agile frameworks like Scrum match against what\u0026rsquo;s being done in at least some open source projects (clearly I cannot speak for each and ever project out there given that I only know a limited set of them), but that\u0026rsquo;s a topic for a another post.\nThe idea of applying open source development practices within corporations has been hanging in the air for over 15 years, driven by different individuals over time. I think now that people are gathering to collect these ideas in one place and role them out in more corporations discussions around the topic are going to become interesting: Each open source community has their own specifics of working together, even though all seem to follow pretty much the same style when looked at from very high above. I\u0026rsquo;m curious to what common values and patterns can be derived from that which work across organisations.\n"},{"id":32,"href":"/note-to-self-backup-bottlenecks/","title":"Note to self: Backup bottlenecks","section":"Inductive Bias","content":" Note to self: Backup bottlenecks # I learnt the following relations the hard way 10 years ago when trying to backup a rather tiny amount of data, went through the computation again three years ago. Still I had to re-do the computation this morning when trying to pull a final full backup from my old MacBook. Posting here for future reference:\nNote 1: Some numbers like 10BASE-T included only for historic reference.\nNote 2: Excluded the Alice DSL uplink speed - if included the left-hand chart would no longer be particularly helpful or readable\u0026hellip;\n"},{"id":33,"href":"/how-hard-can-it-be-organising-a-conference/","title":"How hard can it be - organising a conference","section":"Inductive Bias","content":" How hard can it be - organising a conference # Setup a CfP, select a few talks, publish a schedule, book a venue, sell a few tickets - have fun: Essentially all it takes to organise a conference, isn\u0026rsquo;t it? In theory maybe - in practice - not so much. Without scaring you away from running your own here\u0026rsquo;s my experience with setting up Berlin Buzzwords (after two years of running the Berlin Hadoop Get Together, putting up a NoSQL half day meetup). Disclaimer: Though there\u0026rsquo;s a ton of long-ish posts in my blog, this one is going to be exceptionally long. It\u0026rsquo;s years since I wanted to write all this up for others to learn from and in order to point others to it - never got around to it though. Venue booking Lets start with the most risky part: Booking a venue. If you are a student - great - talk to your university to get your event hosted. As much hassle this may entail when it comes to bureaucracy this is by far the largest monetary risk factor in your calculation, so if there is any way to get rid of this factor use it. FOSDEM, FrOSCon, Linux Tage Chemnitz and several other events have handled this really well. If this is not an option for you depending on the number of targeted attendees you are faced with a smaller (some 500 Euro for 100 people per day and track) or larger (some 20.000 Euros for 300 people, two days and two tracks including chairs, tables, microphones, screens, projectors, lighting, security and technicians) sum that you will have to pay up front, potentially even before your attendees purchased any tickets (here\u0026rsquo;s one reason why there is such a thing as an early bird ticket: The earlier money flows in the less risk there is around payments).\nIn Germany there\u0026rsquo;s three ways to limit this risk: You create a so-called e.V., essentially a foundation that will cover the risk. You create a GmbH, essentially a company that assumes limited risk, namely only up to the 25.000 Euros you had to put into the company when founding it. The third way is to find a producing event agency that assumes all or part of the risk. After talking to several people (those behind Chaos Communication Congress, those setting up the Berlin Django Con back in 2010, those working on JSConf, those backing Hadoop World) for me it was logical to go for the third way: I had no idea how to handle ticket sales, taxes etc. and I was lucky enough to find an agency that would assume all risk in turn for receiving all profit made from the event.\nOne final piece of advise when selecting your venue: Having a place that is flexible when it comes to planning goes a long way to a relaxed conference experience. As much as it is desirable you won\u0026rsquo;t be able to anticipate all needs in advance. It also helps a lot to keep the event on your home turf: One of the secrets of Berlin Buzzwords is having a lot of local knowledge in the organising team and lots of contacts to local people that can help, support and provide insight:\nBy now at Berlin Buzzwords we have a tradition of having an after conference event on Monday evening. The way the tradition started relied heavily on local friends: What we did was to ask friends to take out up to 20 attendees to their favourite bar or restaurant close to the venue in turn for a drastically reduced ticket price. This approach was highly appreciated by all attendees not familiar with Berlin - several asked for a similar setup for Tuesday evening even. Starting from the second edition we were able to recruit sponsors to pay for beers and food for Monday evening. Pro-Tipp: Don\u0026rsquo;t ship your attendees to the BBQ by bus - otherwise the BBQ cook will hate you.\nSomething similar was done for our traditional Sunday evening barcamp: In the first two years it took place at the famous Newthinking Store - a location that used to be available for rent for regular events and free for community events. Essentially a subsidary of our producer. In the second year the Barcamp moved to one of the first hacker spaces world - c-base. Doing that essentially was possible only because some of the organisers had close contacts among the owners of this space.\nThe same is true for the Hackathons and meetups on Wednesday after the main conference: Knowing local (potential) meetup organisers helps recruiting meetups. Knowing local startups helps recruiting space for those people organising a meetup though themselves travelling e.g. from the UK.\nCatering Another risk factor is providing catering: If catering is included in your ticket price, the caterer will probably want to know at least one week before door open roughly how many people will attend (+/-10 people usually is fine, +/- 100 people not really). When dealing with geeks this is particularly tricky: These guys and gals tend to buy their tickets Saturday/Sunday before Berlin Buzzwords Monday morning. In the first two years that meant lots of sleepless nights and lots of effort spent advertising the conference. In the third night I decided that it's time for some geek education: I suggested to introduce a last minute ticket that is expensive enough to motivate the majority of attendees to buy their tickets at least two weeks in advance. Guess what: Ever since we the problem of \"OMG everyone is buying their ticket last minute\" went away - and at least I got my sleep back. The second special thing about catering: As much as I would like to change that Berlin Buzzwords has an extremely skewed gender distribution. When dealing with caterers what they usually only want to know is how many people will attend. If you forget to tell them that all your attendees are relatively young and male they will assume a regular gender distribution - which often leads to you running out of food before everyone is fed.\nSponsoring Except for tickets - are there any other options to acquire money apart from tickets? Sure: convince companies to support your event as sponsors. The most obvious perk is to include said company's logo on your web page. You can also sell booth space (remember to rent additional space for that at your venue). There's plenty of other creative ways to sell visibility. For package sizing I got lots of support from our producer (after all they were responsible for budgeting). Convincing companies to give us money that was mostly on Simon, Jan and myself. Designing the contracts and retrieving the money again was left to the producer. Without a decent social network on our side finding this kind of financial support would not have been possible. In retrospect I'm extremely glad the actual contract handling was not on me - guess what, there are sponsors who just simply forget to pay the sum promised though there is a signed contract... Speaking So, what makes people pay for tickets? A convincing speaker line-up of course. In our case we had decided to go for two invitation only keynote speakers and two days with two tracks of CfP based talks. Keynote speaker slots are still filled by Simon and myself. In early years where CfP ratings, schedule grid (when and how long are breaks? how many talks will fit?) and scheduling itself were on Simon, Jan and myself. As submission numbers went up we decided to share the review load with people whose judgement we trust - ensuring that each submission gets three reviews. All after that is fairly algorithmic: See an earlier blog post for details. A note on review feedback: As much as we would like to give detailed feedback to those who didn't make it: We usually get three times as many submissions as there are open slots. So far I haven't seen a single submission (except for very clear product pitches lacking technical details) that wasn't great. So in the end, most feedback would boil down to \"It was really close, but we only have a limited number of slots to fill. Sorry.\" Back in 2011 we tried an experiment to fit more talks into the schedule than usual: Submissions ranked low that were supposed to be long talks were accepted as short versions forcing speakers to focus. Unfortunately this experiment failed: Seems like people rather get a reject mail than getting accepted as a short version. Also you need really good speakers who prepare exceptionally well for the event - in our case many shortened talks would still include all introductory slides even though the speaker just one slot earlier had covered those already. Marketing Next step is to tell people that there is going to be an event. In the case of Berlin Buzzwords this meant telling people all over the world and convincing them to not only buy a conference ticket, but also a hotel room and a flight to the venue (we started with only half of all attendees coming from Germany and pretty much kept this profile until today). As a first step this meant writing a press release and convincing local tech publishers (t3n, heise, Golem, Software und Support, Open Source Press and many more) to actually publish on the event. For some of these I am an author myself, so I knew which people to talk to, for some of these newthinking as the producing event agency could help. It was only years later that I participated in a media training at Apache Con by Sally Khudairi to really learn how to do press releases. From that day on, a lot of time went into convincing potential speakers to submit talks, potential sponsors to donate money, potential attendees to make it to the event. With a lot of event organising experience on their back (they are running a multiple thousand attendees new media conference called re:publica each year - in addition to many \u0026ldquo;smaller\u0026rdquo; events) newthinking told me upfront that they would need half of my time to cover those marketing activities, essentially as I was the only one who knew the community. The offer was to re-imburse 20h per week from the conference budget. I was lucky enough to be able to convince my manager at neofonie (the company I was working for back then) though that sponsoring the event with my time in turn for a silver sponsorship would be a great opportunity. One of the reasons for doing that on their side was that they were themselves providing services in the big data and search space. Without this arrangement though Berlin Buzzwords 2010 would have been a whole lot harder to do.\nNow how do you reach people for a tech conference? You talk to the press, you create and promote a twitter account. I still have the credentials for @berlinbuzzwords - by now though it is fully managed by our social media expert, back until 2011 I was the face behind it. Ever since my twitter client is set to follow anything that contains bbuzz, berlinbuzzwords or \u0026ldquo;berlin buzzwords\u0026rdquo; - so I can still follow any conversation relating the conference. You create and maintain LinkedIn and Xing as well as Facebook groups for people to follow. You use whatever channels your target audience are reading - in our case several Apache mailing lists, a NoSQL mailing list, sourceforge hosted lists - remember to sign up for these before posting, otherwise your mail won\u0026rsquo;t get through. Also make sure that people at least roughly know your name, trust you and you provide enough context in your mail for others to not view your invitation as plain spam.\nFinally you go through your personal address book and talk to people you know would be interested personally. As a result you will wake up to 20 new mails each morning and answer another 20 new mails every evening. For me this got better after having shaped all contacts into a format that I could hand over to newthinking. However even today, every single mail you send to [target]@berlinbuzzwords.de, every comment you submit through the contact form, every talk wish you submit through our wishlist still ends up in my personal inbox - as well as the inbox of Simon and everyone involved with the conference at newthinking.\nEssentially you need this kind of visibility once for announcing the event, once for filling the CfP with proposals, once the schedule in published, once to convince sponsors to support the event and finally once to convince people to buy tickets.\nAs for the website - the most frustrating part is being a technical person but lacking the time to \u0026ldquo;just do things right\u0026rdquo;. Drupal, I\u0026rsquo;m sorry, but I have learnt to hate your blogging functionality - in particular the WYSIWYG editor. Your administrative backend could be much simpler (I gave those rights away I believe in 2012). I learnt to hate comment spam more than anything else - in particular given the fact that I pretty much would have loved to get everyone involved and able to contribute content. The only thing that helped accepting the deficiencies here was to force myself to hand of any and all content handling to the capable event managers at newthinking.\nMisc Videos: Great way to get the word out (and follow the talks yourself, though organisers may find time to go to talks they\u0026rsquo;ll never remember the actual content due to information overload). Make them free - people pay for tickets to take part in the live event. If you fear selling less tickets as a result, make them available only a little while after the event is over. Pictures: Get someone with a good camera - can be a volunteer, can be a professional or anything in between. I\u0026rsquo;ve had it many times that it took half a year for me to go over the pictures again and suddenly realise how many nice people attended Buzzwords without me knowing them when they did - except they remembered my face when we met again (sorry if you are one of those people I should have remembered once :( ) Inbox: Your\u0026rsquo;s will never be empty again. Especially if as in our case your mail address was used as primary point of contact and reference in the first few years. It took two editions to train people to use info@berlinbuzzwords.de instead of my personal mail address. Trust me - this mail address actually does get attention: Mails sent there end up in my private inbox, they end up in Simon\u0026rsquo;s private inbox and most importantly they end up in those event managers' inboxes involved with Buzzwords at newthinking. Answers typically don\u0026rsquo;t take much longer than half a day even during holidays. Today there is no reason left to contact neither Simon or myself privately - using info@ is way faster. If you still aren\u0026rsquo;t convinced: Even I\u0026rsquo;m using that same address for general inquiries and proposals. Incentives: Think early about which behaviour you would like to see. On site behave accordingly. Pre-conference set incentives: Two weeks before doors open our ticket prices go up drastically to motivate people to buy tickets before our catering deadline bites us. Speakers get travel support and hotel room paid for. However it costs us nothing to list their employer as travel sponsor should they decide to pay for the speaker - and it provides an incentive for the speaker to get their employer to pay for travel costs. Ticket prices: You will get people arguing that ticket prices are too high. Know where you stand in comparison to other conferences of the same type. Clearly you want students if the main focus of your sponsors is to recruit - provide drastically reduced student tickets to anyone with a student id, that includes PhD. students. For everyone else: Buying early should be rewarded. Also you\u0026rsquo;ll need help on site (people moderating sessions, keeping full rooms closed, people helping with video taping, networking etc.) - hand out free tickets to helpers - if those complaining aren\u0026rsquo;t willing to help their need for a cheap ticket probably isn\u0026rsquo;t large enough. The \u0026ldquo;they are stealing our attendees syndrome\u0026rdquo;: Unless there is a clear trademark infringement there\u0026rsquo;s no way to stop other people from running events that on first sight look similar to yours. First of all start by making it hard to beat your offering - not in terms of prices but in terms of content and experience. After you\u0026rsquo;ve done that follow the \u0026ldquo;keep your friends close but your enemies closer\u0026rdquo; principle by embracing those who you believe are running competing events. What we had in the past was events close in topic to ours but not quite overlapping. Where there was enough overlap but still enough distinction we would go out and ask for partnering. This usually involved cross-raffling tickets. It also meant getting better visibility for our event though different channels. Usually the end result was one of two: A) The competing event was a one-of or otherwise short-lived. B) The seemingly competing event targeted an audience that was much more different from ours than we first believed.\nOn being special What makes people coming back? I have been told Berlin Buzzwords has a certain magic to it that makes people want to come back. I\u0026rsquo;m not sure about the magical part - however involving people, providing space and time for networking, choosing a venue that is not a conference hotel, always at least trying to deliver the best experience possible goes a long way to make attendees feel comfortable.\nAs a last note: If you ever once organised a meetup or conference you will never attend other events without at least checking what others do - you will suddenly see all the little glitches that otherwise slip from your attention (overly full trash bins anyone?). On the other hand each event brings at least one story that when it happened looked horrible but turns out to be hilarious and told over and over again later ;)\n"},{"id":34,"href":"/berlin-buzzwords-associated-events/","title":"Berlin Buzzwords - Associated Events","section":"Inductive Bias","content":" Berlin Buzzwords - Associated Events # Back in 2011 I had a weird idea: Berlin Buzzwords as a core event kicks off on Sunday evening with a Barcamp but closes on Tuesday evening. There\u0026rsquo;s way too little time to meet with all the interesting people. On the other hand the organising team really was all tired on Tuesday evening, so just adding another day at the end wasn\u0026rsquo;t really an option. Now Berlin is known as one of the hottest European Startup Capitals. There\u0026rsquo;s literally at least one meetup for any topic you can imagine. There\u0026rsquo;s co-working spaces and companies happy to welcome developers for a one day meetup or Hackathon. So the idea was born to invite people to organise meetups, hackathons, workshops, unconferences or any kind of dev session of their choosing on Wednesday after Berlin Buzzwords - the goal being to keep Buzzwords speakers and attendees in town for longer. In the mean time several meetup groups actually started by having their first event on Wednesday after Berlin Buzzwords. It\u0026rsquo;s grown increasingly easy to find meetup space for these Wednesday meetups. So how does it work? Essentially it\u0026rsquo;s very easy: All that is needed is one person interested in a particular technology who is willing to organise a meetup for 20 to 120 people in Berlin on Wednesday after Buzzwords (or the weekend before, or the weekend after, it\u0026rsquo;s really up to you). It\u0026rsquo;d be greatly appreciated if you get in touch with info@berlinbuzzwords.de or even submit your meetup through the official CfP. In return it will receive marketing help and will be included in the official schedule page. (Note: Meetups are accepted as they come in - no review phase, no nothing). If you need help finding a venue for your meetup (either for free or for rent, up to your preference) talking to info@berlinbuzzwords.de will help you. We\u0026rsquo;ll get you in touch with local startups, co-working spaces and event venues. Also remember - you don\u0026rsquo;t need to be an expert in the topic you are organising the meetup for. All you need is justified interest in the topic. Don\u0026rsquo;t be shy to invite your favourite developers as speakers for your meetup. Don\u0026rsquo;t shy away from even approaching some of the Berlin Buzzwords speakers. If you are a developer yourself think about turning your Wednesday event into a hackathon - either in order to get more users familiar with your project or to gt the next release out the door together with other people working on your project also attending Buzzwords. This kind of approach really is the reason why there is a correlation between the date Berlin Buzzwords takes place and Mahout release dates in summer. (If you need inspiration on what it takes to organise a well working Hackathon - a quick search on your favourite search engine for \u0026ldquo;hackathon howto\u0026rdquo; should surface plenty of documentation.)\n"},{"id":35,"href":"/i-love-fs-2014/","title":"I love FS 2014","section":"Inductive Bias","content":" I love FS 2014 # It\u0026rsquo;s that day of the year again: Time to buy flowers and chocolate for your beloved one. However as with previous years, FSFE wants you to put the day to good use to also celebrate your favourite free software developer (you know, the people who get way more bug reports and complaints than positive feedback):\nSo here\u0026rsquo;s to the people over at Apache, Debian, Eclipse, Elasticsearch, Linux, ZeroMQ and the many other projects that make my life easier: Happy I love Free Software Day - get yourself celebrated!\n"},{"id":36,"href":"/fosdem-2014/","title":"FOSDEM 2014","section":"Inductive Bias","content":" FOSDEM 2014 # By now FOSDEM turned into some kind of tradition in our family: Since 2007 every year in February we are travelling to that one comfy B\u0026amp;B in Brussels for a weekend - not to take a closer look at the city but to attend (together with thousands of other geeks and open source hackers) one of the biggest conferences on all things open source. I love the conference concept for scaling: As it happens on a university campus they only have a limited number of huge rooms, but a fairly large number of mid-sized and small rooms. They use the venue to their advantage: Only the main tracks are organised by the official conference organisers. All other rooms are each filled with a community (usually a handful of people doing the heavy lifting of inviting speakers, putting a cfp out etc.) provided track. As a result even though they attract well over 5000 attendees it\u0026rsquo;s only the most popular topics (e.g. Elasticsearch, GPL license discussions, configuration management, JDK specifics, kdbus, discussions on the relevancy of distribution packaging) tend to fill rooms completely. When stuck with full rooms there\u0026rsquo;s always some other talk on that probably is similarly interesting. In addition there is a huge exhibition area to visit where you can talk to the core developers behind the individual open source projects (and buy stuff like T-Shirts, grab a few flyers, stickers etc.). In addition there\u0026rsquo;s so many people to talk to you usually won\u0026rsquo;t get to see too many presentations anyway. In addition as of this year the crazy people of the FOSDEM (formerly Debian-only) video team managed to get (on a best effort, \u0026ldquo;hopefully the technician doesn\u0026rsquo;t sleep in after that many beers yesterday\u0026rdquo; basis) all 445 talks video-taped with the goal of putting these online Monday after the conference. On top I had the huge advantage of being able to simply follow to the talks my husband would go to if all of the stuff I am interested in is full: That would give me insight to completely different topics but also make it extremely easy for me to identify the good speakers and interesting presentations based on his experience avoiding the \u0026ldquo;if I\u0026rsquo;m unfamiliar with the topic and area I usually bump into the boring, badly given talks\u0026rdquo; problem. I spent most of Saturday in a few talks on software patents, Daniel Naber\u0026rsquo;s Language Tool, the Jolla BoF, the FLA. The rest of the time I caught up with the FSFE (thanks again for the bringing the onesies!), Debian, Open Office, Lucene and Elasticsearch people. The day ended at the Jolla community dinner - with a waitress that was completely overwhelmed by the number of geeks wanting food, beer and soft drinks. Sunday I started with Chris Kühl\u0026rsquo;s talk on Memory Tuning Android for Low Memory Devices. After that I helped at the Elasticsearch booth - FOSDEM definitely is an incredibly busy event. But given their target audience also is an interesting mix of people to talk to: Some had no idea about how text search works but came over because they had heard about Wikipedia using the project for search and wanted to know more. Some had a very clear idea of how they could benefit from Lucene for their NLP projects and wanted to know more on how that fits with Elasticsearch. Others were switching from Solr Cloud and needed some advise on how the systems compare for their particular use case. Others again were using Elasticsearch to analyse log files in a distributed fashion and needed advise on how to implement some feature. There was this one guy from Debian I\u0026rsquo;ve known ever since helping at the FSFE booth at Chemnitzer Linuxtage back in 2008 I believe who wanted to know more about Elasticsearch because one of his fellow package maintainers had been volunteered to work on the Elasticsearch RFP (#660826 if you are interested in reading more). Overall (as every year) a really pleasant experience. What was in particular interesting this year was to meet people I knew only from completely other contexts (CCC events, system administrators, core Apache httpd people). Seems like FOSDEM is not only growing bigger but also more diverse. The only kind of feedback I would provide is to split some dev rooms by finer grained topics to parallelise and scale even better (the Java, NoSQL and configuration management ones come to my mind first but there probably are others as well - of course again this depends on room availability and actual community members volunteering to submit a dev room related to their project in particular). I\u0026rsquo;m glad I\u0026rsquo;m travelling home by train like last year - not only does that give me time to code and write the blog post you are reading just now, it\u0026rsquo;s also comfortable to get rid of the usual sleep deprivation :)\n"},{"id":37,"href":"/on-being-aggressivly-public/","title":"On being aggressivly public","section":"Inductive Bias","content":" On being aggressivly public # If it didn't happen on the mailing list it didn't happen at all. ... with all it's implications this seems to be the hardest lesson for newcomers to the Apache way of development to learn. In the minimal sense it means that any decision a project takes has to be taken publicly, preferably in some archived, searchable medium. In a wider sense it's usually interpreted as: ... don't ask people getting started questions privately by mail, rather go to the user mailing list. ... don't coordinate and take decisions in video conferences, rather take the final decision on the dev mailing list. ... don't ask people privately on IM, rather go to the user mailing list. There are even canned answers that can get sent out to those asking privately.\nThis concept seems to be very hard to grasp for people: As a student, developers tend to be afraid to ask \u0026ldquo;stupid questions\u0026rdquo; - it\u0026rsquo;s easy to say that \u0026ldquo;there are no stupid questions, only stupid answers\u0026rdquo; - truly feeling this to be true can be hard though in particular for people who are just beginning.\nHowever (assuming the question hasn\u0026rsquo;t been answered before or the answer wasn\u0026rsquo;t easy to find with a search engine of your choice) asking these questions is a valuable contribution: Chances are, there are a couple other people who would have the same question - and are too shy to ask. Chances are even bigger that those developing the project would never think of documenting the answer anywhere simply because they do not see it as a question due to their background and knowledge.\nFor researchers in particular on their way to getting a PhD. the problem usually is the habit of publishing facts only after they are checked, cleaned and well presentable. When it comes to developing software what this approach boils down to developing a huge amount of code hidden from others. The disadvantages are clear: You get no feedback on whether or not there would be simpler/faster approaches to solving parts of your problem. Developers of the upstream project won\u0026rsquo;t take the needs for this extension into account when rewriting current modules or adding new code. As a result general advise is to share and discuss your ideas before even starting to code.\nEven for seasoned software engineers this approach tends to be a problem when not used to it: In a corporate environment feedback on code often is perceived as humiliating. As much as we preach that a developer isn\u0026rsquo;t their code so feedback on coding issues is not to be taken seriously it still takes a lot of getting used to when suddenly people give that feedback - and give it in the open for everyone to see.\nAlso often decisions are taken privately before communicating publicly in corporations. As a result having heated design discussions tends to feel like a major marketing problem. Instead what is achieved is making the decision process publicly visible to others so they can follow the arguments without repeating them over and over. It also means that others can chime in and help out if they are affected (after all the second mantra is that those who do the work are the ones who ultimately take the decisions). Again another reason could be for people new to the project or new to some role in the project that asking trivial questions looks intimidating. Again - even if you\u0026rsquo;ve contributed to a project for years noone expects you to know anything. If any piece isn\u0026rsquo;t documented in sufficient depth asking the question provides a track record for others to later refer to. It also means you tend to get your answer faster because there are more people who can answer your question. Oh - and if you don\u0026rsquo;t get an answer chances are, everyone else is in the dark as well but was just not asking.\nOne final word: One fear many people seem to share when asking is to make their own knowledge gaps and flaws public - not only to peers but also to current or future superiors. There\u0026rsquo;s three parts to the advise that I tend to give: Everyone once started small. If your potential future manager or team mates don\u0026rsquo;t understand this, than learning and making mistakes obviously aren\u0026rsquo;t welcome in the environment that you are aspiring to join - is it really worth joining such an environment? Even so you start small, if you continue to contribute on a regular basis the amount of information that you produce online will grow. Guess how much time people spend on evaluating CVs - it\u0026rsquo;s very unlikely they will read and follow each discussion that you contributed to. And as a third point - do assume that by default everything you do online will be visible and searchable forever. However chances are that ten or fifteen years from now what you are writing today will no longer be accessible: Services go away, projects and their infrastructure die, discussion media change - anyone still remember using NNTP on a regular basis?\nAs an exercise to the reader: Before writing this post I spend a couple of hours trying to dig up the mails I had sent I believe in 2000 to the I think KURT and RTLinux discussion lists in order to prepare some presentation about the state of realtime Linux back at university. I search the web, search the archives I could find, searched my own backups from back then - but couldn\u0026rsquo;t find anything. If you do find some trace of that, I owe you a beer next time we meet at FOSDEM ;)\n"},{"id":38,"href":"/scientific-debugging-take-2/","title":"Scientific debugging - take 2","section":"Inductive Bias","content":" Scientific debugging - take 2 # Back in - OMG was that really back in 2010? - 2010 I wrote a post on scientific debugging. Today I was reminded of this post as I actually had the pleasure of watching this principle carried out - except this was for a medical \u0026ldquo;bug\u0026rdquo; instead of one in a piece of software. To quote the book Why programs fail the method of scientific debugging consists of 5 easy to follow steps:\nObserve a failure (i.e., as described in the problem description). Invent a hypothesis as to the failure cause that is consistent with the observations. Use the hypothesis to make predictions. Test the hypothesis by experiments and further observations: If the experiment satisfies the predictions, refine the hypothesis. If the experiment does not satisfy the predictions, create alternate hypothesis. Repeat steps 3 and 4 until the hypothesis can no longer be refined. In the past I\u0026rsquo;ve often seen developers jump immediately from a problem description in a user reported bug to implementing what looks like the obvious solution only to find out later that the actual problem underlying the problem description was something completely different. I know that making sure one has found the true problem can take time. However today I was reminded a) just how long that time can be and b) just how important each step in that search can be: So to start with the problem description (step 1 above): \u0026ldquo;On Saturday evening on my way into the bath tub some pretty bad pain hit me and forced me to lie down flat on the floor. After a few minutes the pain was gone, what remained until today\u0026rsquo;s Tuesday is a pain in my back that gets worse when I walk or lie down on my left side. As a side-note: I\u0026rsquo;m expecting a baby - not sure if that has anything to do with the above.\u0026rdquo;* husband urged me to get to our general practitioner. So off I went - told him the story above. He couldn\u0026rsquo;t do a whole lot for me but to him it seemed like the typical sciatic pain syndrome (step 2 above) - so off he sent me to the orthopaedic (he needed an expert\u0026rsquo;s opinion to check his hypothesis apparently). I called the one doctor I was recommended, was told to go there immediately, went there. Some 15 minutes later I told the doctor the story above (step 1). She followed the hypothesis - except for one tiny little problem: Given the location of the pain and my condition she knew of at least one more hypothesis that would also fit the observation (step 2). She made a manual check (step 3). Her observation was still consistent with the hypothesis (step 4). Now in order to reject said hypothesis she would have needed to do one more check - except she couldn\u0026rsquo;t do that one herself. So she sent me off to my gynecologist to get an expert\u0026rsquo;s opinion (and gave me an appointment for late this week/early next week in case really only the original hypothesis is valid). Off I went, called the third doctor. I told her the whole story (step 1), she continued where the other doctor had left off and made the missing check (step 3) - and rejected the alternative hypotheses right away (step 4). So off she went to step 5: Sometimes labour pains start in the back (step 2) - so she put me on a CTG (step 3) - after several minutes that hypothesis was rejected as well (step 4). So off she went to step 5: The last non-trivial hypothesis would have been that something\u0026rsquo;s wrong with kidneys (step 2). So she checked with her medical ultrasonics (step 3) - she wasn\u0026rsquo;t 100% sure though it seemed to look ok so she send me off to one last expert for a second opinion. So on my way home I got to tell the story above the fourth time (step 1). Again she continued with where the other doctor had left off - namely step 3: One more medical ultrasonic and a blood test later the last non trivial hypothesis was finally rejected altogether (step 4) So what was left was the initial hunch of there being some blockade in my back. Finally we can step from observing to acting, namely keeping warm (hot water bottle), relaxing and to keep moving regularly. As annoying as the day for the system to be debugged was there\u0026rsquo;s at least two valuable lessons learnt in it: Being absolutely certain about a certain problem cause can be expensive (in this case in particular time consuming for me, rest is something the doctors involved have to discuss with my public health insurance). However jumping directly from problem description to bugfix can turn out to be much more expensive if said bugfix takes a long time to implement (and show effects) but is treating the wrong problem. Proving one hypothesis right sometimes involves ruling out options that are easy to check but have bad consequences if left untreated. Translated back - before jumping from problem description to bugfix it may make sense to stop for a second and think whether the particular problem you see might actually be caused by a bug far worse than what you anticipated. Now off to search for my warm little elephant.\n* Yeah, that\u0026rsquo;s why I didn\u0026rsquo;t make it to the Berlin Open Source meetup on Sunday that I organised - at least my husband had fun together with Lennart and others trying to explain to the non German speaker what on earth is the English translation for the term Hexenschuß. "},{"id":39,"href":"/children-tinkering/","title":"Children tinkering","section":"Inductive Bias","content":" Children tinkering # Years ago I decided that in case I got the same question for at least three times I would write down the answer and put it somewhere online in a more or less public location that I can link to. The latest question I got once too often came from daddies (mostly, sorry - not even a handful of moms around me, let alone moms who are into tech) looking for ways to get there children in touch with technology. Of course every recommendation depends heavily on the age and interest of the little one in question. However most recommendations are based on using a child\u0026rsquo;s love for games - see also a story on how a father accidentally turned his daughter into a dungeons and dragons fan for a bit more background on what I mean. There are several obvious paths, including Lego Mindstorms, the programming kits by Fischertechnik, several electronics kits you get at your favourite shop, fun stuff like Makey, makey kits that can turn a banana into a controller. Also many games come with their own level designers (think Little Big Planet, though the older children might remember that even Far Cry, Doom and friends came with level designers). In addition by now there are quite a few courses and hacking events that kids are invited to go to - like the FrogLabs co-located with FrosCon, the Chaos macht Schule initiative by CCC, meetups like the ones hosted by Open Tech School, Jugend Hackt. In addition quite a few universities collaborate with schools to bring pupils in touch with research (and oftentimes tech) - like e.g. at HU Berlin. In addition there are a three more less typical recommendations: As a child I loved programming a turtle (well, a white dot really) to move across the screen forward or backwards, to turn east, south, west or north, to paint or to stop painting. The slightly more advanced (both in a graphical as well as in an interactive sense of the word) version of that would be to go for Squeak (all smalltalk, first heard about it about a decade ago at the Chemnitzer Linuxtage) or Scratch (a geek dad kindly showed that to me years ago). When it comes to hardware hacking one recommendation I can give from personal experience is to take part in one of the soldering courses by Mitch Altman - you know, the guy who invented the \"TV-B-Gone\". Really simple circuits, you solder yourself (no worries, the parts are large and robust enough that breaking them is really, really, really hard). What you end up with tends to be blinking and in some cases is programmable. As an aside: Those courses really aren't only interesting for little ones - I've seen adults attend, including people who are pretty deep into Java programming and barely ever touch circuits in their daily work. If you are more into board games: Years ago one of my friends invited me to a RoboRally session. Essentially every player programs their little robot to move across the board. When it comes to books one piece I can highly recommend (didn't know something like that existed until my colleagues came up with it) would be the book \"Geek Mom\" - there's also an edition called \"Geek Dad\". Be warned though, this is not tech only. If you know of any other events, meetups, books or games that you think should really go on that list, let me know. "},{"id":40,"href":"/dont-dream-it-be-it/","title":"Don't dream it, be it","section":"Inductive Bias","content":" Don\u0026rsquo;t dream it, be it # After two years in a row of receiving 120 submissions for Berlin Buzzwords from the usual crowd - young, white, male, caucasian - only this year we decided we needed to work towards increasing diversity.One piece in the puzzle was to get in touch with several Berlin local \u0026ldquo;tech for non-tech\u0026rdquo; people groups. In a content exchange kind of setting I was asked to do an interview as some kind of role model. In addition to a serious lack of time back then I felt the typical way these interviews go would do no good - even anyone who\u0026rsquo;s in IT already learning I co-founded Apache Mahout, am a member of the Apache Software foundation, have co-founded Berlin Buzzwords (after running quite a few successful meetups around related topics in Berlin), am married to a Linux kernel developer tends to shy away (unless the person I\u0026rsquo;m talking to happens to be into OSS development themselves thus knowing that despite quite some work this also means having lots of fun). However the invitation did get me started thinking about what kind of advise I would share with the next generation of hackers. Over time though I realised that what was most helpful for me doesn\u0026rsquo;t only apply to those who want to become successful in IT. On first sight it sounds like an extremely easy to follow advise: Once upon a time after coming back from the Kindergarden provided by my mom\u0026rsquo;s employer I spent part of an afternoon in front of a computer in an office close the hers. The game was trivial: Direct a little Snake through a maze, collect items, avoid biting yourself or the walls. Years in primary school I got to play with the computer of one of my relatives. Ever since beating their highscore I wanted a computer for myself. When I finally got a first computer on my own I used to play lots of games together with a good friend of mine - until the game supply for my Amiga 500 dried out. Back then I made a decision: To work towards simply coding my own games. Ever since I followed this tiny little dream - by now for almost twenty years. Even despite the fact I got all support I could wish for from parents, teachers and university professors seen from the outside it may have seemed like not always being easy as pie: More often than not it meant being different - instead of being part of the \u0026ldquo;I don\u0026rsquo;t know what I want to do after school\u0026rdquo; it meant being part of that small group of people who know what they are working for. Instead of being part of that large \u0026ldquo;I hate technology and I\u0026rsquo;m utterly bad at math\u0026rdquo; it meant being part of that tiny group of people who love math and who have fun dealing with any new technology. Instead of being at one of those great parties for New Year\u0026rsquo;s Eve it meant filling in the details of a project proposal a few hours before midnight. Instead of being home at 6p.m. it meant going to meetups more often than not. Instead of being home during weekends it meant flying to California for a conference on a weekend on my private budget. Despite getting 2.5 days a week from March to June to work on Berlin Buzzwords from my employer for the first two years and having lots of help and knowledge over at newthinking who did the heavy lifting of taking on the financial risk, managing registrations, booking the venue and handling speaker travel support it still meant lots of additional mornings, evenings and weekends spent on making the event fly (and an inbox that never went silent - neither at noon nor at midnight - hint: any mail you send to info@berlinbuzzwords ends up not only in an anonymous mailing list - every organiser including Simon Willnauer, Daniela Bentrup and myself will receive your mail and make sure it gets dealt with). There are a couple of reasons I kept doing this kind of stuff. But I guess the most influential reason is simply that it also is a whole lot of fun for me. There were several fellow students at school who didn\u0026rsquo;t have the courage to follow their dreams from the very start: There\u0026rsquo;s the girl who was teased into studying biology by her parents - only years later she had the courage to go for professional gardening. There\u0026rsquo;s this guy who didn\u0026rsquo;t know exactly what to do and followed many of his friends to study mechanical engineering. Months later he pivoted towards social sciences and politics, today working on a PhD. thesis on economics in German hospitals. There\u0026rsquo;s the girl who successfully passed her math degree but really also wanted to follow her musical passion - in the end she went to study music in addition to become a math and music teacher. It takes determination and courage to follow your dreams - especially if that means following a different path than what your parents had on their mind for you or following a path that doesn\u0026rsquo;t quite fit into the cliché of what society has in mind for you*. However in my statistically absolutely non-significant, completely biased and personal opinion it\u0026rsquo;s worth every effort. * Sorry for as long as it is special to love repairing cars for a girl and to love working in child day care for a boy I don\u0026rsquo;t believe in society not influencing career decisions.\n"},{"id":41,"href":"/on-geeks-growing-up/","title":"On geeks growing up","section":"Inductive Bias","content":" On geeks growing up # I\u0026rsquo;m a regular visitor of the Chemnitzer Linuxtage in March - at first going to talks learning lots of interesting stuff I didn\u0026rsquo;t know about like aspect oriented programming, strace, squeak, which open source licenses are best for different strategies. As of late I had been there mostly to help out with the FSFE booth. For context: The conference itself is hosted by the technical university in Chemnitz, it takes place on a weekend, they charge the tiny amount of 5 Euros for admission. In turn visitors get two full days of mostly well prepared, diverse talks and workshops. Speakers and exhibitors get access to the backstage catering area including free food and drinks all day and an after show dinner on Saturday evening. In general organisation is highly professional - WiFi just works, no super-long queues for meals (that for attendees are available for purchase during the breaks), equipment in the rooms usually just works. One thing I found particular about the Linuxtage in Chemnitz was always how family friendly they are: Standing at the FSFE booth I\u0026rsquo;ve had it more often than not that parents who are not into IT at all would take their young kids who are \u0026ldquo;into computers\u0026rdquo; to the event. However also quite a few geeks tend to bring their off-spring: It all started with a toy corner years ago. By now the offer has been extended to be a separate quite room stuffed with lots of toys, visited not only by parents and kids but also engaged clowns and magicians for entertainment. Ever since it seems like other conferences have followed the example: Froscon isn\u0026rsquo;t only offering a nursing room and play area - there\u0026rsquo;s a jumping castle in the backyard for smaller children. For little hackers there is a special track stuffed with coding topics suitable for children - often even taught by younger ones. EuRuCamp went another step further: Not only do they sell children tickets that are a lot cheaper than those offered for adults. For the very young ones the ticket includes babysitting services - organised in collaboration with a local Berlin babysitting service. I been there for a while but last time I visited also Chaos Communication Congress and Camp drew several small hackers - in general there were tinkering workshops well suitable for slightly older little people. Even FOSDEM that to my knowledge doesn\u0026rsquo;t yet offer any special tracks or separate rooms for smaller ones was still able to draw a few families - most likely due in part to the \u0026ldquo;we are one big family\u0026rdquo; nature of the event (despite attendee numbers as high as 5k each year). At least for Berlin it seems this trend has been acknowledged - as tech conferences you can get makey-makey packages for free from a local IT foundation. On a more personal note: In contrast to all of the above the conference I\u0026rsquo;m involved in personally - Berlin Buzzwords - is pretty much business driven and profit oriented. However for good reason it has the reputation of still being very community oriented. For several editions I have tried finding ways to turn the event into something that is slightly more family friendly: There once was an offer to bring your non-tech spouse or relatives with us organising a city tour for them. In an initial trial run this was tried on speakers - there was some response, but overall too few people made use of the offer to run it again. There usually were play areas featuring foosball tables, table tennis and the like - but those mostly catered the geeks themselves really. We ran at least one blog post asking for people in need for child care to get in touch with us - though there is the occasional request on twitter, nothing substantial came out of these initiatives. I asked parents who I knew were visiting the conference themselves what would make them bring their children - the ones I asked mostly came back with a need for child care for very little people or a conference date during school holidays to bring older kids. This year the approach we try is slightly different: We again host the event in Kulturbrauerei - a venue that is itself very well suited to experimenting with different formats: Several rooms from large to small, a nice back yard, a cinema and a few shops, well located in Prenzlauer Berg which itself is known for being almost too family friendly. We got in touch with the organisers of EuRuCamp to learn how they got baby sitting services sponsored - Dajana, thanks a ton for your input. In addition we put the invitation to bring kids and the baby sitting offering up online where every attendee inevitably will see it: There is a special ticket for kids (with limited availability though as this is the first trial run) that includes catering and day care you can book. In addition there\u0026rsquo;s also a catering only ticket that is way cheaper than the full conference access pass - so in case the conference pass is too expensive for you to pay privately however you\u0026rsquo;d still like to be at the event during your lunch break or in the evening this is the ideal option for you. I have to admit I\u0026rsquo;m highly curious how this will play out. For me Berlin Buzzwords always was a great excuse to hand to friends in order to get them to visit the city at the best time of the year. As a result it meant that I could go to the conference by bicycle and have everyone else I would love to meet in town. It would be great if these two changes enable more people to be with us. It would be even better if these two changes did actually support the community flavour that I have been told Buzzwords has. Looking forward to seeing you in June!\n"},{"id":42,"href":"/hello-elasticsearch/","title":"Hello elasticsearch","section":"Inductive Bias","content":" Hello elasticsearch # First of all a disclaimer: I had a little bit of time left during the last few weeks. As a result my blog migrated from dynamic wordpress content to statically hosted pages. If anything looks odd, in case you find any encoding issues, if you miss specific functionality - please do let me know. I\u0026rsquo;ll switch from this beta url back to the old sub-domain in a week or so unless there are major complaints. Today was my first day in a new office. Some of you may have heard it already: As of today I\u0026rsquo;m working for Elasticsearch. Apparently the majority of devs here are using Apple hardware so with a little help from Random Tutor and my husband I got my new machine equipped to boot Linux in parallel yesterday. As a result what was left for today was reading lots of documentation, getting accustomed to the internal tools, attending my first daily standup, forking the repository, starting to take a closer look at the source code, issue tracker and design docs. Now looking forward to both - the elasticsearch training in December and meeting other elasticsearch people over at FOSDEM next year: Find me at their booth there.\nThanks for the warm welcome everyone!\n"},{"id":43,"href":"/building-online-communities-from-the-0mq-trenches130/","title":"Building online communities - from the 0MQ trenches","section":"Inductive Bias","content":" Building online communities - from the 0MQ trenches # After seeing several talks on how open source communitites are organised at FOSDEM, on how to license open source software strategically at Chemnitzer Linuxtage and on how to nurture open source communities at Berlin Buzzwords over the past couple of years during the past year or so I\u0026rsquo;ve come to read quite a few articles and books on the art of building online communities. It all started with the now famous video on poisonous people of a talk given by Brian Fitzpatrick and Ben Collins-Sussman. Starting from there I went on to read their book \u0026ldquo;Team Geek\u0026rdquo; - a great read not only if you are working in the open source space but also if you have to deal with tech geeks and managers on a daily basis as part of your job.\nI continued the journey with reading \u0026ldquo;Producing Open Source Software\u0026rdquo; - a book commonly recommended to read for those trying to understand how to run open source projects. Even though I started Apache Mahout back in 2008, first got in touch with the nutch/Lucene community in 2004 and wrote my first mails to realtime Linux mailing lists to ask for help for some university assignment as far back as I guess 2001 the book still contained many ideas that were new and valuable to me. Most important of all it presented most of the important aspects of running an open source project in a very concise nicely presented format.\nAfter going to a talk on engineering a collaborative culture in the midst of flame wars (including a side note on how to even turn trolls into valuable community members that help new comers substantially) given by Kristian Koehntopp earlier at Froscon this year I started reading a book that he recommended: Building Successful online communities by MIT press.\nMany of these texts come from people that either have an Apache background one way or another - or are of more general nature. Yesterday I was happy to take the ZeroMQ guide (also available on dead trees) and as github project you can contribute to) that Pieter Hintjens had kindly given to my husband earlier this year during FOSDEM and find a whole chapter on how he manages ZeroMQ.\nThe text is unique in that iMatix got into a very influential position in the project very early on. However based on decades of open source experience Pieter managed to avoid many of the mistakes beginners make from the very outset. Also having built several online communities before (ranging from open source projects to the NGO FFII) he deliberately designed the ZeroMQ development in a\nway that would encourage a healthy community.\nThere are several essential aspects that I find interesting: The ZeroMQ development model is explicitly codified - they call this C4: After the painful experience of discussing what seemed obvious but unspoken rules before codification the development team came up with a protocol for developing ZeroMQ - the protocol definition formulation being based on the rules IETF RFC are written. Many rules at Apache are not written down - especially when explaining how the Apache Way works to new projects in the incubator this becomes obvious again and again. Granted - apart from a handful of core values - Apache projects are essentially free to define their own way of working. However even within one project your mileage may very depending on who you ask how things are done. This makes it hard for newcomers to understand what\u0026rsquo;s going on - but also can become an issue when problems arise.\nA concept that I find interesting about the way ZeroMQ works is the separation between maintainers and contributors: Maintainers are people who pull code into mainline - contributors are those doing the actual coding. Essentially this means that in order to get a patch in it needs at least two people to look at it. This isn\u0026rsquo;t too much different from a review-than-commit policy - just enforced and written down as good practice. It helps avoid panic errors of people committing code in a hurry. But it also makes sure that those writing code actually get the positive feedback they deserve - which in turn might help\navoiding fast contributor burn out.\nAlso this kind of split in roles makes sure that there are no people with special privileges - just because someone has commit access to the main repository doesn\u0026rsquo;t mean he can take any shortcuts process wise: They still have to come up with a decent problem description, file a ticket, create a patch, submit the patch through a pull request and have it reviewed like anyone else. I found it interesting that though ZeroMQ is backed and initiated by iMatix Pieter considers it to be very important to keep a balance in power and delegate to non-iMatix contributors both, coding and design decisions.\nWith iMatix being a small company the stance on making ZeroMQ an LGPL license project is a very clear decision. It\u0026rsquo;s the only way to ensure that downstream users cannot just take the project, make modifications to it, re-package and ship it to users without the accompanying source code under the same license. In turn this tends to make it much more likely that even capable users tend to contribute to upstream. Of course taking the idea itself and turning it into some proprietary project would still be very possible. However the one thing that sets ZeroMQ apart from other efforts is not the source code or the architecture alone - it\u0026rsquo;s the way the community works and blossoms.\nOne part where this choice of license is particularly handy is the deliberate decision to not go through any copyright assignment process. Instead each patch gets licensed to the project under the regular LGPL terms. This means that even should iMatix one day be sold or change their minds re-licensing the whole project is utterly hard. The impact on the community is clear: It makes sure that contributors\u0026rsquo; patches remain their own - including all merit and praise that comes with it. This approach prevents re-licensing but encourages a sense of shared ownership. Essentially this model of copyright handling is not unlike the way the Linux kernel works.\nThe last point that I found important is the way the project itself is structured: Instead of having everyone work on one single project ZeroMQ makes it easy to write extensions to the core library. There is a whole guide on how to write language bindings. Those writing these bindings aren\u0026rsquo;t regulated at all - they are hosted in their own repositories with their own governance if they want - in the end it\u0026rsquo;s up to the user to decide which ones are good and which ones will never become popular. In turn this lead to many people contributing indirectly to the value of ZeroMQ in significant ways. This is not unlike other projects: Apache HTTPd provides APIs to write modules against. ElasticSearch provides a clean REST API that encourages people speaking other languages to develop plugins that will translate the REST API into whatever their preferred language is. Open/Libre Office deliberately encourages writing extensions and plugins - even providing hosting facilities where users can search and download extensions from third parties.\nI leave it as an exercise to the reader to check out the whole book. Even in the community chapter there are several other interesting concepts as well: The experience ZeroMQ went through with actively encouraging even developers with commit access to the main repository to work with forks instead of feature branches for experimental development, the trouble they went through with making backwards in-compatible changes to user facing APIs way too often, the exact definition of the C4 development process.\nOverall a really interesting perspective on open source development from the trenches with lots of experience to back the advise given. If you are interested in learning more on how open source projects work - and if you are using any you definitely should be, otherwise you are betting part of your business on something you do not understand which generally isn\u0026rsquo;t the best idea of all.\n"},{"id":44,"href":"/wonder-if-you-should-switch-from-your-rdbms-to-apache-hadoop-dont436/","title":"Wonder if you should switch from your RDBMS to Apache Hadoop: Don't!","section":"Inductive Bias","content":" Wonder if you should switch from your RDBMS to Apache Hadoop: Don\u0026rsquo;t! # Last weekend I spend a lot of fun time at FrOSCon* in Sankt Augustin - always great to catch up with friends in the open source space. As always there were quite a few talks on NoSQL, Hadoop, but also really solid advise on tuning your system for stuff like MySQL (including a side note on PostgreSQL and Oracle) from Kristian Köhntopp. When following some of the discussions in the audience before and after the talk I could not help but shake my head on some of the advise given about HDFS and friends.\nThis is to give a really short rule of thumb on what project to use for which occasion. Maybe it helps clear some false assumptions. Note: All of the below are most likely gross oversimplifications. Don\u0026rsquo;t use it as hard and fast advise but as a first step towards finding more information with your preferred search engine.\nUse Case 1 - relational dataTechnology\nI have usual relational dataUse a relational database - think MySQL and friends\nI have relational data but my database doesn\u0026rsquo;t perform.Tune your system, go to step 1.\nI have relational data but way more reads than one machine can accomodate.Have master-slave replication turned on, configure enough slaves to accommodate your traffic.\nI have relational data, but way too much data for a single machine.Start sharding your database.\nI have a lot of reads (or writes) and too much data for a single machine.If the sharding+replication pain gets unbearable but you still need strong consistency guarantees start playing with HBase. You might loose the option of SQL but win being able to scale beyond traditional solutions. Hint: Unless your online product is hugely successful switching to HBase usually means you\u0026rsquo;ve missed some tuning option.\nUse Case 2 - CrawlingTechnology\nI want to store and process a crawl of the internet.Store it as flat files, if you like encode metadata together with the data in protocol buffers, thrift or Avro.\nMy internet crawl no longer fits on a single disk.Put multiple disks in your machine, RAID them if you like.\nProcessing my crawl takes too long.Optimise your pipeline. Make sure you utilise all processors in your machine.\nProcessing the crawl still takes too long.If your data doesn\u0026rsquo;t fit on a single machine, takes way too long to process but there is no bigger machine that you can reasonably pay for you are probably willing to take some pain. Get yourself more than one machine, hook them together, install Hadoop and use either plain map reduce , Pig, Hive or Cascading to process the data. Distribution-wise Apache, Cloudera, MapR, Hortonworks are all good choices.\nUse Case 3 - BITechnology\nI have structured data and want my business analysts to find great new insights.Use a data warehouse your analysts are most familiar with.\nI want to draw conclusions from one year worth of traffic on a busy web site (hint: resulting log files no longer fit on the hard disk of my biggest machine).Stream your logs into HDFS. From here it depends: If it\u0026rsquo;s your developers that want to get their hands dirty, Cascading and depending packages might be a decent idea. There\u0026rsquo;s plenty of UDFs in Pig that will help you as well. If the work is to be done by data analysts that only speak SQL use Hive.\nI want to correlate user transactions with social media activity around my brand.See above.\nA really short three bullet point summary\nUse HBase to scale the backend your end users interact with. If you want to trade strong consistency for being able to span multiple datacenters on multiple continents take a look at Cassandra.\nUse plain HDFS with Hive/Pig/Cascading for batch analysis. This could be business intelligence queries against user transactions, log file analysis for statistics, data extraction steps for internet crawls, social media data or other sensor data.\nUse Drill or Impala for low latency business intelligence queries.\nGood advise from ApacheConEU 2008/9\nBack at one of the first ApacheCons I ever attended there was an Apache Hadoop BoF. One of the attendees asked for good reasons to switch from his current working infrastructure to Hadoop. In my opinion the advise he got from Christophe Bisciglia is still valid today. Paraphrased version: For as long as you wonder why you should be switching to Hadoop, don\u0026rsquo;t. A parting note: I\u0026rsquo;ve left CouchDB, MongoDB, JackRabbit and friends out of the equation. The reason for this is my own lack of first-hand experience with those projects. Maybe someone else can add to the list here.\n* A note to the organisers: Thilo and myself married last year in September. So when seeing the term \u0026ldquo;Fromm\u0026rdquo; in a speaker\u0026rsquo;s surname doesn\u0026rsquo;t automatically mean that the speaker hotel room should be booked on the name \u0026ldquo;Thilo Fromm\u0026rdquo; - the speaker on your list could as well be called \u0026ldquo;Isabel Drost-Fromm\u0026rdquo;. It was hilarious to have the speaker reimbursement package signed by my husband though this year around I was the one giving a talk at your conference ;)\n"},{"id":45,"href":"/jax-projec-nashorn245/","title":"JAX: Project Nashorn","section":"Inductive Bias","content":" JAX: Project Nashorn # The last talk I went to was on project Nashorn - demonstrating the capability\nto run dynamic languages on the JVM by writing a JavaScript implementation as a\nproof of concept that is fully ECMA compliant and still performs better than\nMozilla\u0026rsquo;s project Rhino.\nIt was nice to see Lisp, created in 1962, referenced as being the first\nlanguage that featured a JIT compiler as well as garbage collection. It was\nalso good to see Smalltalk referenced as pioneering class libraries, visual GUI\ndriven IDEs and bytecode.\nAs such Java essentially stands on the shoulders of giants. Now dynamic\nlanguage writers can themselves use the JVM to boost their productivity by\nprofiting from the VM\u0026rsquo;s memory management, JIT optimisations, native threading.\nThe result could be a smaller code base and more time to concentrate on\ninteresting language features (of course another result would be that the JVM\nbecomes interesting not only for Java developers but also to people who want to\nuse dynamic languages instead).\nThe projects invoke dynamic as well as the DaVinci machine are both interesting\nareas for people to follow who are interested in running dynamic languages on\nthe JVM.\n"},{"id":46,"href":"/jax-tales-from-production246/","title":"JAX: Tales from production","section":"Inductive Bias","content":" JAX: Tales from production # In a second presentation Peter RoÃbach together with Andreas Schmidt provided\nsome more detail on what the topic logging entails in real world projects.\nDevelopment messages turn into valuable information needed to uncover issues\nand downtime of systems, capacity planning, measuring the effect of software\nchanges, analysing resource usage under real world usage. In addition to these\ntechnical use cases there is a need to provide business metrics.\nWhen dealing with multiple systems you deal with correlating values across\nmachines and systems, providing meaningful visualisations to draw the correct\ndecisions.\nWhen thinking of your log architecture you might want to consider storing not\nonly log messages. In addition facts like release numbers should be tracked\nsomewhere - ready to join in when needed to correlate behaviour with release\nversion. To do that also track events like rolling out a release to production.\nLaunching in a new market, switching traffic to a new system could be other\nevents. Introduce not only pure log messages but also provide aggregated\nmetrics and counters. All of these pieces should be stored and tracked\nautomatically to free operations for more important work.\nHave you ever thought about documenting not only your software, it\u0026rsquo;s interfaces\nand input/output format? What about documenting the logged information as well?\nWhat about the fields contained in each log message? Are they documented or do\npeople have to infer their meaning from the content? What about valid ranges\nfor values\nare they noted down somewhere? Did you store whether a specific\nfield can only contain integers or whether some day it also could contain\nletters? What about the number format - is it decimal, hexadecimal?\nFor a nice architecture documentation of the BBC checkout\nWinning the metrics battle by the BBC dev blog.\nThere\u0026rsquo;s an abundance of tools out there to help you with all sorts of logging\nrelated topics:\nFor visualisation and transport: Datadog, kibana, logstash, statsd,\ngraphite, syslog-ng\nFor providing the values: JMX, metrics, Jolokia\nFor collection: collecd, statsd, graphite, newrelic, datadog\nFor storage: typical RRD tools including RRD4j, MongoDB, OpenTSDB based\non HBase, Hadoop\nFor charting: Munin, Cacti, Nagios, Graphit, Ganglia, New Relic, Datadog\nFor Profiling: Dynatrace, New Relic, Boundary\nFor events: Zabbix, Icinga, OMD, OpenNMS, HypericHQ, Nagios,JbossRHQ\nFor logging: splunk, Graylog2, Kibana, logstash\nMake sure to provide metrics consistently and be able to add them with minimal\neffort. Self adaption and automation are useful for this. Make sure developers,\noperations and product owners are able to use the same system so there is no\ninformation gap on either side. Your logging pipeline should be tailored to\nprovide easy and fast feedback on the implementation and features of the\nproduct.\nTo reach a decent level of automation a set of tools is needed for:\nConfiguration management (where to store passwords, urls or ips, log\nlevels etc.). Typical names here include Zookeeper,but also CFEngine, Puppet\nand Chef.\nDeployment management. Typical names here are UC4, udeploy, glu, etsy\ndeployment.\nServer orchestration (e.g. what is started when during boot). Typical\nnames include UC4, Nolio, Marionette Collective, rundeck.\nAutomated provisioning (think ``how long does it take from server failure\nto bringing that service back up online?\u0026rsquo;\u0026rsquo;). Typical names include kickstart,\nvagrant, or typical cloud environments.\nTest driven/ behaviour driven environments (think about adjusting not\nonly your application but also firewall configurations). Typical tools that\ncome to mind here include Server spec, rspec, cucumber, c-puppet, chef.\nWhen it comes to defining the points of communication for the whole\npipeline there is no tool you can use that is better than traditional pen and\npaper, socially getting both development and operations into one room.\nThe tooling to support this process goes from simple self-written bash scripts\nin the startup model to frameworks that support the flow partially, up to\nprocess based suites that help you. No matter which path you choose the goal\nshould always be to end up with a well documented, reproducable step into\nproduction. When introducing such systems problems in your organisation may\nbecome apparent. Sometimes it helps to just create facts: It\u0026rsquo;s easier to ask for\nforgiveness than permission.\n"},{"id":47,"href":"/jax-logging-best-practices243/","title":"JAX: Logging best practices","section":"Inductive Bias","content":" JAX: Logging best practices # The ideal outcome of Peter RoÃbach\u0026rsquo;s talk on logging best practices was to have\nattendees leave the room thinking ``we know all this already and are applying\nit successfully\u0026rsquo;\u0026rsquo; - most likely though the majority left thinking about how to\nimplement even the most basic advise discussed.\nFrom his consultancy and fire fighter background he has a good overview of what\nlogging in the average corporate environment looks like: No logging plan, no\nrules, dozens of logging frameworks in active use, output in many different\nlanguages, no structured log events but a myriad of different quoting,\nformatting and bracketing standards instead.\nSo what should the ideal log line contain? First of all it should really be a\nlog line instead of a multi line something that cannot be reconstructed when\ninterleaved with other messages. The line should not only contain the class\nname that logged the information (actually that is the least important piece of\ninformation), it should contain the thread id, server name, a (standardised and\nalways consistently formatted) timestamp in a decent resolution (hint: one new\ntimestamp per second is not helpful when facing several hundred requests per\nsecond). Make sure to have timing aligned across machines if timestamps are\nneeded for correlating logs. Ideally there should be context in the form of\nrequest id, flow id, session id.\nWhen thinking about logs, do not think too much about human readability - think\nmore in terms of machine readability and parsability. Treat your logging system\nas the db in your data center that has to deal with most traffic. It is what\nholds user interactions and system metrics that can be used as business\nmetrics, for debugging performance problems, for digging up functional issues.\nMost likely you will want to turn free text that provides lots of flexibility\nfor screwing up into a more structured format like json, or even some binary\nformat that is storage efficient (think protocol buffers, thrift, avro).\nIn terms of log levels, make sure to log development traces on trace, provide\ndetailed problem analysis stuff on debug, put normal behaviour onto info. In\ncase of degraded functionality, log to warn. In case of things you cannot\neasily recovered from put them on error. When it comes to logging hierarchies -\ndo not only think in class hierarchies but also in terms of use cases: Just\nbecause your http connector is used in two modules doesn\u0026rsquo;t mean that there\nshould be no way to turn logging on just for one of the modules alone.\nWhen designing your logging make sure to talk to all stakeholders to get clear\nrequirements. Make sure you can find out how the system is being used in the\nwild, be able to quantify the number of exceptions; max, min and average\nduration of a request and similar metrics.\nTools you could look at for help include but are not limited to splunk, jmx,\njconsole, syslog, logstash, statd, redis for log collection and queuing.\nAs a parting exercise: Look at all of your own logfiles and count the different\nformats used for storing time.\n"},{"id":48,"href":"/jax-java-performance-myths242/","title":"JAX: Java performance myths","section":"Inductive Bias","content":" JAX: Java performance myths # This talk was one of the famous talks on Java performance myths by Arno Haase.\nHis main point - supported with dozens of illustrative examples was for\nsoftware developers to stop trusting in word of mouth, cargo cult like myths\nthat are abundant among engineers. Again the goal should be to write readable\ncode above all - for one the Java compiler and JIT are great at optimising. In\naddition many of the myths being spread in the Java community that are claimed\nto lead to better performance are simply not true.\nIt was interesting to learn how many different aspects of both software and\nhardware contribute to code performance. Micro benchmarks are considered\ndangerous for a reason - creating a well controlled environment that matches\nwhat the code will encounter in production is influenced by things like just in\ntime compilation, cpu throttling, etc.\nSome myths that Arno proved wrong include final making code faster (in case of\nmethod parameters it doesn\u0026rsquo;t make a difference up to bytecode being identical\nwith and without), inheritance being always expensive (even with an abstract\nclass between the interface and the implementation Java 6 and 7 can still\ninline the method in question). Another one was on often wrongly scoped Java\nvs. C comparisons. One myth resolved around the creation of temporary objects -\nsince Java 6 and 7 in simple cases even these can be optimised away.\nWhen it comes to (un-)boxing and reflection there is a performance penalty. For\nthe latter mostly for method lookup, not so much for calling the method. What we\nare talking about however are penalties in the range of about 1000 compute\ncycles. Compared to doing any remote calls this is still dwarfed. Reflection on\nfields is even cheaper.\nOne of the more wide spread myths resolved around string concatenation being\nexpensive - doing a A'' + B\u0026rsquo;\u0026rsquo; in code will be turned into ``AB\u0026rsquo;\u0026rsquo; in\nbytecode. Even doing the same with a variable will be turned into the use of\nStringBuilder ever since -XX:OptimizeStringConcat was turned on by default.\nThe main message here is to stop trusting your intuition when reasoning about a\nsystem\u0026rsquo;s performance and performance bottlenecks. Instead the goal should be to\ngo and measure what is really going on. Those are simple examples where your\naverage Java intuition goes wrong. Make sure to stay on top with what the JVM\nturns your code into and how that is than executed on the hardware you have\nrolled out if you really want to get the last bit of speed out of your\napplication.\n"},{"id":49,"href":"/jax-does-parallel-equal-performant239/","title":"JAX: Does parallel equal performant?","section":"Inductive Bias","content":" JAX: Does parallel equal performant? # In general there is a tendency to set parallel implementations to being equal\nto performant implementations. Except in the really naive case there is always\ngoing to be some overhead due to scheduling work, managing memory sharing and\nnetwork communication overhead. Essentially that knowledge is reflected in\nAmdahl\u0026rsquo;s law (the amount of serial work limits the benefit from running parts\nof your implementation in parallel, http://en.wikipedia.org/wiki/Amdahl's_law),\nand Little\u0026rsquo;s law ( http://en.wikipedia.org/wiki/Little's_law) in case of queuing\nproblems.\nWhen looking at current Java optimisations there is quite a bit going on to\nsupport better parallelisation: Work is being done to provide for improving\nlock contention situations, the GC adaptive sizing policy has been improved to\na usable state, there is added support for parallel arrays and lampbda\u0026rsquo;s\nsplitable interface.\nWhen it comes to better locking optimisations what is most notable is work\ntowards coarsening locks at compile and JIT time (essentially moving locks from\nthe inside of a loop to the outside); eliminating locks if objects are being\nused in a local, non-threaded context anyway; and support for biased locking\n(that is forcing locks only when a second thread is trying to access an\nobject). All three taken together can lead to performance improvements that\nwill almost render StringBuffer and StringBuilder to exhibit equal performance\nin a single threaded context.\nFor pieces of code that suffer from false sharing (two variables used in\nseparate threads independently that end up in the same CPU cacheline and as a\nresult are both flushed on update) there is a new annotation: Adding the\n\u0026quot;@contended\u0026quot; annotation can help the compiler for which pieces of code to add\ncacheline padding (or re-arrange entirely) to avoid that false sharing from\nhappening. One other way to avoid false sharing seems to be to look for class\ncohesion - coherent classes where methods and variables are closely related\ntend to suffer less from false sharing. If you would like to view the resulting\nlayout use the \u0026ldquo;-XX:PrintFieldLayout\u0026rdquo; option.\nJava 8 will bring a few more notable improvements including changes to the\nadaptive sizing GC policy, the introduction of parallel arrays that allow for\nparallel execution of predicates on array entries, changes to the concurrency\nlibraries, internalised iterators.\n"},{"id":50,"href":"/jax-pigs-snakes-and-deaths-by-1k-cuts244/","title":"JAX: Pigs, snakes and deaths by 1k cuts","section":"Inductive Bias","content":" JAX: Pigs, snakes and deaths by 1k cuts # In his talk on performance problems Rainer Schuppe gave a great introduction to\nwhich kinds of performance problems can be observed in production and how to\nbest root-cause them.\nSimply put performance issues usually arise due to a difference in either data\nvolumn, concurrency levels or resource usage between the dev, qa and production\nenvironments. The tooling to uncover and explain them is pretty well known:\nStaring with looking at logfiles, ARM tools, using aspects, bytecode\ninstrumentalisation, sampling, watching JMX statistics, and PMI tools.\nAll of theses tools have their own unique advantages and disadvantages. With\nlogs you get the most freedom, however you have to know what to log at\ndevelopment time. In addition logging is i/o heavy, so doing too much can slow\nthe application down itself. In a common distributed system logs need to be\naggregated somehow. As a simple example of what can go wrong are cascading\nexceptions spilled to disk that cause machines to run out of disk space one\nafter the other. When relying on logging make sure to keep transaction\ncontexts, in particular transaction ids across machines and services to\ncorrelate outages. In terms of tool support, look at scribe, splunk and flume.\nA tool often used for tracking down performance issues in development is the\nwell known profiler. Usually it creates lots of very detailed data. However it\nis most valuable in development - in production profiling a complete server\nstack produces way too much load and data to be feasable. In addition there\u0026rsquo;s\nusually no transaction context available for correlation again.\nA third way of watching applications do their work is to watch via JMX. This\ncapability is built in for any Java application, in particular for servlet\ncontainers. Again there is not transaction context. Unless you take care of it\nthere won\u0026rsquo;t be any historic data.\nWhen it comes to diagnosing problems, you are essentially left with fixing\neither the \u0026ldquo;it does not work\u0026rdquo; case or the \u0026ldquo;it is slow case\u0026rdquo;.\nFor the \u0026ldquo;it is slow case\u0026rdquo; there are a few incarnations:\nIt was always slow, we got used to it.\nIt gets slow over time.\nIt gets slower exponentially.\nIt suddenly gets slow.\nThere is a spontanous crash.\nIn the case of \u0026ldquo;it does not work\u0026rdquo; you are left with the following observations:\nSudden outages.\nAlways flaky.\nSporadic error messages.\nSilent death.\nIncreasing error rates.\nMisleading error messages.\nIn the end you will always be spinning in a Look at symptoms, Elimnate\nnon-causes, Identifiy suspects, Confirm and Eliminate comparing to normal. If\nnot done with that, leather, rinse, repeat. When it comes to causes for errors\nand slowness you will usually will run into one of the following causes: In\nmany cases bad coding practices are a problem, too much load, missing backends,\nresource conflicts, memory and resource leakage as well as hardware/networking\nissues are causes.\nSome symptoms you may observe include foreseeable lock ups (it\u0026rsquo;s always slow\nafter four hours, so we just reboot automatically before that), consistent\nslowness, sporadic errors (it always happens after a certain request came in),\ngetting slow and slower (most likely leaking resources), sudden chaos (e.g.\nsomeone pulling the plug or someone removing a hard disk), and high utilisation\nof resources.\nLinear memory leak\nIn case of a linear memory leak, the application usually runs into an OOM\neventually, getting ever slower before that due to GC pressure. Reasons could\nbe linear structures being filled but never emptied. What you observe are\ngrowing heap utilisation and growing GC times. In order to find such leakage\nmake sure to turn on verbose GC logging, do heapdumps to find leaks. One\nchallenge though: It may be hard to find the leakage if the problem is not one\nlarge object, but many, many small ones that lead to a death by 1000 cuts\nbleeding the application to death.\nIn development and testing you will do heap comparisons. Keep in mind that\ntaking a heap dump causes the JVM to stop. You can use common profilers to look\nat the heap dump. There are variants that help with automatic leak detection.\nA variant is the pig in a python issue where sudden unusually large objects\ncause the application to be overloaded.\nResource leaks and conflicts\nAnother common problem is leaking resources other than memory - not closing\nfile handles can be one incarnation. Those problems cause a slowness over time,\nthey may lead to having the heap grow over time - usually that is not the most\nvisible problem though. If instance tracking does not help here, your last\nresort should be doing code audits.\nIn case of conflicting resource usage you usually face code that was developed\nwith overly cautious locking and data integrity constraints. The way to go are\nthreaddumps to uncover threads in block and wait states.\nBad coding practices\nWhen it comes to bad coding practices what is usually seen is code in endless\nloops (easy to see in thread dumps), cpu bound computations where no result\ncaching is done. Also layeritis with too much (de-)serialisation can be a\nproblem. In addition there is a general \u0026ldquo;the ORM will save us all\u0026rdquo; problem that\nmay lead to massive SQL statements, or to using the wrong data fetch strategy.\nWhen it comes to caching - if caches are too large, access times of course grow\nas well. There could be never ending retry loops, ever blocking networking\ncalls. Also people tend to catch exceptions but not do anything about them\nother than adding a little #fixme annotation to the code.\nWhen it comes to locking you might run into dead-/live-lock problems. There\ncould be chokepoints (resources that all threads need for each processing\nchain). In a thread dump you will typically see lots of wait instead of block\ntime.\nIn addition there could be internal and external bottlenecks. In particular\nkeep those in mind when dealing with databases.\nThe goal should be to find an optimum for your application between too many too\nsmall requests that waste resources getting dispatched, and one huge request\nthat everyone else is waiting for.\n"},{"id":51,"href":"/jax-java-hpc-by-norman-maurer241/","title":"JAX: Java HPC by Norman Maurer","section":"Inductive Bias","content":" JAX: Java HPC by Norman Maurer # For slides see also: Speakerdeck: High performance networking on the JVM\nNorman started his talk clarifying what he means by high scale: Anything above\n1000 concurrent connections in his talk are considered high scale, anything\nbelow 100 concurrent connections is fine to be handled with threads and blocking\nIO. Before tuning anything, make sure to measure if you have any problem at\nall: Readability should always go before optimisation.\nHe gave a few pointers as to where to look for optimisations: Get started by\nstudying the socket options - TCP-NO-DELAY as well as the send and receive\nbuffer sizes are most interesting. When under GC pressure (check the GC locks\nto figure out if you are) make sure to minimise allocation and deallocation of\nobjects. In order to do that consider making objects static and final where\npossible. Make sure to use CMS or G1 for garbage collection in order to\nmaximise throughput. Size areas in the JVM heap according to your access\npatterns. The goal should always be to minimise the chance of running into a\nstop the world garbage collection.\nWhen it comes to using buffers you have the choice of using direct or heap\nbuffers. While the former are expensive to create, the latter come with the\ncost of being zero\u0026rsquo;ed out. Often people start buffer pooling, potentially\ninitialising the pool in a lazy manner. In order to avoid memory fragmentation\nin the Java heap, it can be a good idea to create the buffer at startup time\nand re-use it later on.\nIn particular when parsing structured messages like they are common in\nprotocols it usually makes sense to use gathering writes and scattering reads\nto minimise the number of system calls for reading and writing. Also try to\nbuffer more if you want to minimise system calls. Use slice and duplicate to\ncreate views on your buffers to avoid mem copies. Use a file channel when\ncopying files without modifications.\nMake sure you do not block - think of DNS servers being unavailable or slow as\nan example.\nAs a parting note, make sure to define and document your threading model. It\nmay ease development to know that some objects will always only be used in a\nsingle threaded context. It usually helps to reduce context switches as well as\nmay ease development to know that some objects will always only be used in a\nsingle threaded context. It usually helps to reduce context switches as well as\nkeeping data in the same thread to avoid having to use synchronisation and the\nuse of volatile.\nAlso make a conscious decision about which protocol you would like to use for\ntransport - in addition to tcp there\u0026rsquo;s also udp, udt, sctp. Use pipelining in\norder to parallelise.\n"},{"id":52,"href":"/jax-hadoop-overview-by-bernd-fondermann240/","title":"JAX: Hadoop overview by Bernd Fondermann","section":"Inductive Bias","content":" JAX: Hadoop overview by Bernd Fondermann # After breakfast was over the first day started with a talk by Bernd on the\nHadoop ecosystem. He did a good job selecting the most important and\ninteresting projects related to storing data in HDFS and processing it with Map\nReduce. After the usual \"what is Hadoop\", \"what does the general architecture\nlook like\", \"what will change with YARN\" Bernd gave a nice overview of which\npublications each of the relevant projects rely on:\nHDFS is mainly based on the paper on GFS.\nMap Reduce comes with it's own publication.\nThe big table paper mainly inspired Cassandra (to some extend), HBase,\nAccumulo and Hypertable.\nProtocol Buffers inspired Avro and Thrift, and is available as free\nsoftware itself.\nDremel (the storage side of things) inspired Parquet.\nThe query language side of Dremel inspired Drill and Impala.\nPower Drill might inspire Drill.\nPregel (a graph database) inspired Giraph.\nPercolator provided some inspiration to HBase.\nDynamo by Amazon kicked of Cassandra and others.\nChubby inspired Zookeeper, both are based on Paxos.\nOn top of Map Reduce today there are tons of higher level languages,\nstarting with Sawzall inside of Google, continuing with Pig and Hive at Apache\nwe are now left with added languages like Cascading, Cascalog, Scalding and\nmany more.\nThere are many other interesting publications (Megastore, Spanner, F1 to\nname just a few) for which there is no free implementation yet. In addition\nwith Storm, Hana and Haystack there are implementations lacking canonical\npublications.\nAfter this really broad clarification of names and terms used, Bernd went into\nsome more detail on how Zookeeper is being used for defining the namenode in\nHadoop 2, how high availablility and federation works for namenodes. In\naddition he gave a clear explanation of how block reports work on cluster\nbootup. The remainder of the talk was reserved for giving an intro to HBase,\nGiraph and Drill.\n"},{"id":53,"href":"/bigdatacon125/","title":"BigDataCon","section":"Inductive Bias","content":" BigDataCon # Together with Uwe Schindler I had published a series of articles on Apache\nLucene at Software and Support Media's Java Mag several years ago. Earlier this\nyear S\u0026amp;S kindly invited my to their BigDataCon - co-located with JAX to give a\ntalk of my choosing that at least touches upon Lucene.\nThinking back and forth about what topic to cover what came to my mind was to\ngive a talk on how easy it is to do text classification with Mahout when\nrelying on Apache Lucene for text analysis, tokenisation and token filtering.\nAll classes essentially are in place to integrate Lucene Analyzers with Mahout\nvector generation - needed e.g. as a pre-processing step for classification or\ntext clustering.\nFeel free to check out some of my sandbox code over at \u0026lt;a\nhref=``http://github.org/MaineC/sofia''\u003egithub\u0026lt;/a\u0026gt;.\nAfter attending the conference I can only recommend everyone interested in Java\nprogramming and able to understand German to buy a ticket for the conference.\nIt's really well executed, great selection of talks (though the sponsored\nkeynotes usually aren't particularly interesting), tasty meals, interesting\npeople to chat with.\n"},{"id":54,"href":"/hadoop-summit-amsterdam223/","title":"Hadoop Summit Amsterdam","section":"Inductive Bias","content":" Hadoop Summit Amsterdam # About a month ago I attended the first European Hadoop Summit, organised by\nHortonworks in Amsterdam. The two day conference brought together both vendors\nand users of Apache Hadoop for talks, exhibition and after conference beer\ndrinking.\nRussel Jurney kindly asked me to chair the Hadoop applied track during\nApache Con EU. As a result I had a good excuse to attend the event. Overall\nthere were at least three times as many submissions than could reasonably be\naccepted. Accordingly accepting proposals was pretty hard.\nThough some of the Apache community aspect was missing at Hadoop summit it was\ninteresting nevertheless to see who is active in this space both as users as\nwell as vendors.\nIf you check out the talks on Youtube make sure to not miss the two sessions by\nTed Dunning as well as the talk on handling logging data by Twitter.\n"},{"id":55,"href":"/apacheconna-misc101/","title":"ApacheConNA: Misc","section":"Inductive Bias","content":" ApacheConNA: Misc # In his talk on Spdy Mathew Steele explained how he implemented the spdy protocol\nas an Apache httpd module - working around most of the safety measures and\ndesign decisions in the current httpd version. Essentially to get httpd to\nsupport the protocol all you need now is mod_spdy plus a modified version of\nmod_ssl.\nThe keynote on the last day was given by the Puppet founder. Some interesting\npoints to take away from that:\nThough hard in the beginning (and half way through, and after years) it\nis important to learn giving up control: It usually is much more productive and\nleads to better results to encourage people to do something than to be\nrestrictive about it. A single developer only has so much bandwidth - by\nfarming tasks out to others - and giving them full control - you substantially\nincrease your throughput without having to put in more energy.\nBe transparent - it's ok to have commercial goals with your project. Just\nmake sure that the community knows about it and is not surprised to learn about\nit.\nBe nice - not many succeed at this, not many are truely able to ignore\nreligion (vi vs. emacs). This also means to be welcoming to newbies, to hustle\nat conferences, to engage the community as opposed to announcing changes.\nOverall good advise for those starting or working on an OSS project and seeking\nto increase visibility and reach.\nIf you want to learn more on what other talks were given at ApacheCon NA or want to follow up in more detail on some of the talks described here check out the slides archive online. "},{"id":56,"href":"/apacheconna-hadoop-metrics99/","title":"ApacheConNA: Hadoop metrics","section":"Inductive Bias","content":" ApacheConNA: Hadoop metrics # Have you ever measured the general behaviour of your Hadoop jobs? Have you\nsized your cluster accordingly? Do you know whether your work load really is IO\nbound or CPU bound? Legend has it noone expecpt Allen Wittenauer over at\nLinked.In, formerly Y! ever did this analysis for his clusters.\nSteve Watt gave a pitch for actually going out into your datacenter measuring\nwhat is going on there and adjusting the deployment accordingly: In small\nclusters it may make sense to rely on raided disks instead of additional\nstorage nodes to guarantee ``replication levels''. When going out to vendors to\nbuy hardware don't rely on paper calculations only: Standard servers in Hadoop\nclusters are 1 or 2u. This is quite unlike beefy boxes being sold otherwise.\nFigure out what reference architecture is being used by partners, run your\nstandard workloads, adjust the configuration. If you want to run the 10TB\nTerrasort to benchmark your hardware and system configuration. Make sure to\ncapture data during all your runs - have Ganglia or SAR, watch out for\nintersting behaviour in io rates, cpu utilisation, network traffic. The goal is\nto get the cpu busy, not wait for network or disk.\nAfter the instrumentation and trial run look for over- and underprovisionings,\nadjust, leather, rinse, repeat.\nAlso make sure to talk to the datacenter people: There are floor space, power\nand cooling constraints to keep in mind. Don't let the whole datacenter go down\nbecause your cpu intensive job is drawing more power than the DC was designed\nfor. Ther are also power constraints per floor tile due to cooling issues -\nthose should dictate the design.\nTake a close look at the disks you deploy: SATA vs. SAS can make a 40%\nperformance difference at a 20% cost difference. Also the number of cores per\nmachines dictates the number of disks to spread the likelyhood of random read\naccess. As a rule of thumb - in a 2U machine today there should be at least\ntwelve large form factor disks.\nWhen it comes to controllers he goal should be to get a dedicated lane to disc,\nsafe one controller if price is an issue. Trade off compute power against power\nconsumption.\nDesigning your network keep in mind that one switch going down means that one\nrack will be gone. This may be a non-issue in a Y! size cluster, in your\nsmaller scale world it might be worth the money investing in a second switch\nthough: Having 20 nodes go black isn't a lot of fun if you cannot farm out the\nwork and re-replication to other nodes and racks. Also make sure to have enough\nports in rack switches for the machines you are planning to provision.\nAvoid playing the ops whake-a-mole game by having one large cluster in the\norganisation than many different ones where possible. Multi-tenancy in Hadoop is\nstill pre-mature though.\nIf you want to play with future deployments - watch out for HP currently\npacking 270 servers where today are just two via system on a chip designs.\n"},{"id":57,"href":"/apacheconna-monitoring-httpd-and-tomcat102/","title":"ApacheConNA: Monitoring httpd and Tomcat","section":"Inductive Bias","content":" ApacheConNA: Monitoring httpd and Tomcat # Monitoring - a task generally neglected - or over done - during development.\nBut still vital enough to wake up people from well earned sleep at night when\ndone wrong. Rainer Jung provided some valuable insights on how to monitor Apache httpd and Tomcat.\nOf course failure detection, alarms and notifications are all part of good\nmonitoring. However so is avoidance of false positives and metric collection,\nvisualisation, and collection in advance to help with capacity planning and\nuncover irregular behaviour.\nIn general the standard pieces being monitored are load, cache utilisation,\nmemory, garbage collection and response times. What we do not see from all that\nare times spent waiting for the backend, looping in code, blocked threads.\nWhen it comes to monitoring Java - JMX is pretty much the standard choice. Data\nis grouped in management beans (MBeans). Each Java process has default beans,\non top there are beans provided by Tomcat, on top there may be application\nspecific ones.\nFor remote access, there are Java clients that know the protocol - the server\nmust be configured though to accept their connection. Keep in mind to open the\nfirewall in between as well if there is any. Well known clients include\nJVisualVM (nice for interactive inspection), jmxterm as a command line client.\nThe only issue: Most MBeans encode source code structure, where what you really\nneed is change rates. In general those are easy to infer though.\nOn the server side for Tomcat there is the JMXProxy in Tomcat manager that\nexposes MBeans. In addition there is Jolohia (including JSon serialisation) or\nthe option to roll your own.\nSo what kind of information is in MBeans:\nOS - load, process cpu time, physical memory, global OS level\nstats. As an example: Here deviding cpu time by time geves you the average cpu\nconcurrency.\nRuntime MBean gives uptime.\nThreading MBean gives information on count, max available threads etc\nClass Loading MBean should get stable unless you are using dynamic\nlanguaes or have enabled class unloading for jsps in Tomcat.\nCompliation contains HotSpot compiler information.\nMemory contains information on all regions thrown in one pot. If you need\nmore fine grained information look out for the Memory Pool and GC MBeans.\nAs for Tomcat specific things:\nThreadpool (for each connector) has information on size, number of busy\nthreads.\nGlobalRequestProc has request counts, processing times, max time bytes\nreceived/sent, error count (those that Tomcat notices that is).\nRequestProcessor exists once per thread, it shows if a request is\ncurrently running and for how long. Nice to see if there are long running\nrequests.\nDataSource provides information on Tomcat provided database connections.\nPer Webapp there are a couple of more MBeans:\nManagerMBean has information on session management - e.g. session\ncounter since start, login rate, active sessions, expired sessions, max active\nsinse restart sessions (here a restart is possible), number of rejected\nsessions, average alive time, processing time it took to clean up sessions,\ncreate and required rate for last 100 sessions\nServletMBean contains request count, accumulated processing time.\nJspMBean (together with activated loading/unloading policy) has\ninformation on unload and reload stats and provides the max number of loaded\njsps.\nFor httpd the goals with monitoring are pretty similar. The only difference is\nthe protocol used - in this case provided by the status module. As an\nalternative use the scoreboard connections.\nYou will find information on\nrestart time, uptime\nserverload\ntotal number of accesses and traffic\nidle workers and number of requests currently processed\ncpu usage - though that is only accurate when all children are stopped\nwhich in production isn't particularly likely.\nLines that indicate what threads do contain waitinng, request read, send reply\n- more information is documented online.\nWhen monitoring make sure to monitor not only production but also your stress\ntests to make meaningful comparisons.\n"},{"id":58,"href":"/apacheconna-on-security105/","title":"ApacheConNA: On Security","section":"Inductive Bias","content":" ApacheConNA: On Security # During the security talk at Apache Con a topic commonly glossed over by\ndevelopers was covered in quite some detail: With software being developed that\nis being deployed rather widely online (over 50% of all websites are powered\nby the Apache webserver) natually security issues are of large concern.\nCurrently there are eight trustworthy people on the foundation-wide security\nresponse team, subscribed to security@apache.org. The team was started by\nWilliam A. Rowe when he found a volnarability in httpd. The general work mode -\nas opposed to the otherwise ``all things open'' way of doing things at Apache -\nis to keep the issues found private until fixed and publicise widely\nafterwards.\nSo when running Apache software on your servers - how do you learn about\nsecurity issues? There is no such thing as a priority list for specific\nvendors. The only way to get an inside scoop is to join the respective\nproject's PMC list - that is to get active yourself.\nSo what is being found? 90% of all security issues are found be security\nresearches. The remaining 10% are usually published accidentially - e.g. by\nusers submitting the issue through the regular public bug tracker of the\nrespective project.\nIn Tomcat currently no issues was disclosed w/o letting the project know. httpd\nstill is the prime target - even of security researchers who are in favour of\na full disclosure policy - the PMC cannot do a lot here other than fix issues\nquickly (usually within 24 hours).\nAs a general rule of thumb: Keep your average release cycle time in mind - how\nlong will it take to get fixes into people's hands? Communicate transparently\nwhich version will get security fixes - and which won't.\nAs for static analysis tools - many of those are written for web apps and as\nsuch not very helpful for a container. What is highly dangerous in a web app\nmay just be the thing the container has to do to provide features to web apps.\nAs for Tomcat, they have made good experiences with Findbugs - most others have\ntoo many false positives.\nWhen dealing with a volnarability yourself, try to get guidance from the\nsecurity team on what is actually a security volnarability - though the final\ndecision is with the project.\nDealing with the tradeoff of working in private vs. exposing users affected by\nthe volnarability to attacks is up to the PMC. Some work in public but call the\nactual fix a refactoring or cleanup. Given enough coding skills on the attacker\nside this of course will not help too much as sort of reverse engineering what\nis being fixed by the patches is still possible. On the other hand doing\neverything in private on a separate branch isn't public development anymore.\nAfter this general introduction Mark gave a good overview of the good, the bad\nand the ugly way of handling security issues in Tomcat. For his slides\n(including an anecdote of what according to the timing and topic looks like it\nwas highly related to the 2011 Java Hash Collision talk at Chaos Communication\nCongress).\n"},{"id":59,"href":"/apacheconna-on-documentation104/","title":"ApacheConNA: On documentation","section":"Inductive Bias","content":" ApacheConNA: On documentation # In her talk on documentation on OSS Noirin gave a great wrap up of the topic of\nwhat documentation to create for a project and how to go about that task.\nOne way to think about documentation is to keep in mind that it fulfills\ndifferent tasks: There is conceptual, procedural and task-reference\ndocumentation. When starting to analyse your docs you may first want to debug\nthe way it fails to help its users: ``I can't read my mail'' really could mean\n``My computer is under water''.\nA good way to find awesome documentation can be to check out Stackoverflow\nquestions on your project, blog posts and training articles. Users today really\nare searching instead of browsing docs. So where to find documentation actually\nis starting to matter less. What does matter though is that those pages with\nrelevant information are written in a way that makes it easy to find them\nthrough search engines: Provide a decent title, stable URLs, reasonable tags\nand descriptions. By the way, both infra and docs people are happy to talk to\n*good* SEO guys.\nIn terms of where to keep documentation:\nFor conceptual docs that need regular review it's probably best to keep them in\nversion control. For task documentation steps should be easy to upgrade once\nthey fail for users. Make sure to accept bug reports in any form - be it on\nFacebook, Twitter or in your issue tracker.\nWhen writing good documentation always keep your audience in mind: If you don't\nhave a specific one, pitch one. Don't try to cater for everyone - if your docs\nare too simplistic or too complex for others, link out to further material.\nUnderstand their level of understanding. Understand what they will do after\nreading the docs.\nOn a general level always include an about section, a system overview, a\ndescription of when to read the doc, how to achieve the goal, provide\nexamples, provide a trouble shooting section and provide further information\nlinks. Write breadth first - details are hard to fill in without a bigger\npicture. Complete the overview section last. Call out context and\npre-requesites explicitly, don't make your audience do more than they really\nneed to do. Reserve the details for a later document.\nIn general the most important and most general stuff as well as the basics\nshould come first. Mention the number of steps to be taken early. When it comes\nto providing details: The more you provide, the more important the reader will\ndeem that part.\n"},{"id":60,"href":"/on-delegation103/","title":"ApacheConNA: On delegation","section":"Inductive Bias","content":" ApacheConNA: On delegation # In her talk on delegation Deb Nicholson touched upon a really important topic in\nOSS: Your project may live longer than you are willing to support it yourself.\nThe first important point about delegation is to delegate - and to not wait\nuntil you have to do it. Soon you will realise that mentoring and delegation\nactually is a way to multiply your resources.\nIn order to delegate people to delegate to are needed. To find those it can be\nhelpful to understand what motivates people to work in general as well as on\nopen source in particular: Sure, fixing a given problem and working on great\nsoftware projects may be part of it. As important though is recognition\nindividually and in groups of people.\nKeeping that in mind, ``Thanking'' is actually a license to print free money in\nthe open source world. Do it in a verbose manner to be believable, do it in\npublic and in a way that makes your contributors feel a little bit of glory.\nAnother way to lead people in is to help out socially: Facilitate connections,\nsuggest connections, introduce people. Based on the diversity of the project\nyou are working on you may be in a way larger network and have access to much\nmore corporations and communities than any peer who is not active. Use that\npotential.\nAlso when leading OSS projects keep in eye on people being rude: Your project\nshould be accessible to facilitate participation.\nIn case of questions treat them as a welcome opportunity to pull a new\ncommunity member in: Answer quickly, answer on your list, delegate to middle\nseniors to pull them in. Have training missions for people who want to get\nstarted and don't know your tooling yet. Have prepared documents to provide\nlinks to in case questions occur.\nIn Apache we tend to argue people should not fall victim of volunteeritis.\nAnother way to put that is to make sure to avoid the licked cookie syndrom:\nWhen people volunteer to do a task and never re-appear that task is tainted\nuntil explicitly marked as ``not taken'' later on. One way to automate that is\nto have a fixed deadline after which tasks are automatically marked as free to\ntake and tackle by anyone.\nWhen it comes to the question of When to write documentation: There really is\nno point in time that should stop you from contributing docs - all the way from\njust above getting started level (writing the getting started docs for those\nfollowing you) up to the ``I'm an awesome super-hacker'' mode for those trying\nto hack on similar areas.\nEspecially when delegating to newbies make sure to set the right expectations:\nHow long is it going to take to fix an issue, what is the task complexity, tell\nthem who is going to be involved, who is there to help out in case of road\nblocks.\nIn general make sure to be a role model for the behaviour you want in your\nproject: Ask questions yourself, step back when your have taken on too much,\nappreciate people stepping back.\nUnderstand the motivation of your new comers - try to talk to them one on one\nto understand their motivation and help to align work on the project with their\nlife goal. When starting to delegate, start with tasks that seem to small to\ndelegate at all to get new people familiar with the process - and to get\nyourself familiar with the feeling of giving up control. Usually you will need\nto pull tasks apart that before were done by one person. Don't look for a\nperson replacement - instead look for separate tasks and how people can best\nperform these.\nMake visible and clear what you need: Is it code or reviews? Documentation or\ntranslations, UX helpers? Incentivise what you really need - have code sprints,\ngamify the process of creating better docs, put the logo creation under a\nchallenge.\nAll of this is great if you have only people who all contribute in a very\npositive way. What if there is someone who's contributions are actually\ndetrimental to the project? How to deal with bad people? They may not even do\nso intentionally... One option is to find a task that better suits their\nskills. Another might be to find another project for them that better fits\ntheir way of communicating. Talk to the person in question, address head on\nwhat is going on. Talking around or avoiding that conversation usually only\ndelays and enlarges your problem. One simple but effective strategy can be to\ntell people what you would like them to do in order to help them find out that\nthis is not what they want to do - that they are not the right people for you\nand should find a better place.\nMore on this can be found in material like ``How assholes are killing your\nproject'' as well as the ``Poisonous people talk'' and the book ``Producing\nopen source software''.\nOn the how of dealing with bad people make sure to criticise privately first,\nchack in a backchannel of other committers for their opinion - otherwise you\nmight be lonely very quickly. Keep to criticising the bahaviour instead of the\nperson itself. Most people really do not want to be a jerk.\n"},{"id":61,"href":"/apacheconna-first-keynote98/","title":"ApacheConNA: First keynote","section":"Inductive Bias","content":" ApacheConNA: First keynote # All three ApacheCon keynotes were focussed around the general theme of open\nsource communities. The first on given by Theo had very good advise to the\nengineer not only striving to work on open source software but become an\nexcellent software developer:\nBe loyal to the problem instead of to the code: You shouldn't be\naddicted to any particular programming language or framework and refuse to work\nand get familiar with others. Avoid high specialisation and seek cross\nfertilisation. Instead of addiction to your tooling you should seek to\ndiversify your toolset to use the best for your current problem.\nWork towards becoming a generalist: Understand your stack top to bottom -\nstarting with your code, potentially passing the VM it runs in up down to the\nhardware layer. Do the same to requirements you are exposed to: Being 1s old\nmay be just good enough to be ``always current'' when thinking of a news\nserving web site. Try to understand the real problem that underpins a certain\ntechnical requirement that is being brought up to you. This deep understanding\nof how your system works can make the difference in fixing a production issue\nin three days instead of twelve weeks.\nThe last point is particularly interesting for those aiming to write scalable\ncode: Software and frameworks today are intended to make development easier -\nwith high probability they will break when running at the edge.\nWhat is unique about the ASF is the great opportunity to meet people with\nexperience in many different technologies. In addition there is an unparalleled\nlevel of trust in a community as diverse as the ASF. One open question that\nremains is how to leverage this potential successfully within the foundation.\n"},{"id":62,"href":"/apache-hadoop-get-together-berlin-547/","title":"Apache Hadoop Get Together Berlin","section":"Inductive Bias","content":" Apache Hadoop Get Together Berlin # This evening I joined the group over at Immobilienscout 24 for today\u0026rsquo;s Hadoop Get Together. David Obermann had invited Dr. Falk-Florian Henrich from CeleraOne to talk about their real-time analytics on live data streams.\nTheir system is being used by the New York Times Springer\u0026rsquo;s Die Welt for traffic analysis. The goal is to identify recurring users that might be willing to pay for the content they want to read. The trade-off here is to keep readers interested long enough to make them pay in the end, instead of scaring them away with a restrictive pay wall which would immediately lead to way less ad revenues.\nCurrently CeleraOne\u0026rsquo;s system is based on a combination of MongoDB for persistent storage, ZeroMQ for communicating with the revenue engine and http/json for connecting to the controlling web frontend. The live traffic analysis is all done in RAM, while long term storage ends up in MongoDB.\nThe second speaker was Michael Hausenblas from MapR. He spends most of his time contributing to Apache Drill - an open source implementation of Google\u0026rsquo;s Dremel.\nBeing an Apache project Drill is developed in an open, meritocratic way - contributors come from various different backgrounds. Currently Drill is in its early stages of development: They have a logical plan, a reference interpreter, a basic SQL parser. There is a demo application. As data backends they support HBase.\nFor most of the implementation they are trying to re-use existing libraries, e.g. for the columnar storage Drill is looking into either using Twitter\u0026rsquo;s Parquet or Hive ORC file format.\nIn terms of contributing to the project: There is no need to be a rockstar programmer to make valuable contributions to Apache Drill: Use cases, documentation, test data are all valuable and appreciated by the project.\nFor more information check out the slide deck (this is an older version - this nights edition most likely soon to be published):\nIf you missed today\u0026rsquo;s event make sure to get enlisted in the Hadoop Get Together Xing Group so next time you get a notification. One thing to note though: When registering for the event - please make sure to free your ticket if you cannot make it. I had a few requests from people who would have loved to attend today who didn\u0026rsquo;t get a ticket but would most likely have fit into the room.\n"},{"id":63,"href":"/meet-the-indian-tribe100/","title":"ApacheConNA: Meet the indian tribe","section":"Inductive Bias","content":" ApacheConNA: Meet the indian tribe # ApacheCon is the ``User Conference of the Apache Software Foundation\u0026rsquo;\u0026rsquo;. What\nshould that mean? If you are going to Apache Con you have the chance of meeting\ncommitters of your favourite projects as well as members of the foundation\nitself. Though there are a lot of talks that are interesting from a technical\npoint of view the goal really is to turn you into an active member of the\nfoundation yourself. This is true for the North American version even more than\nfor the European edition.\nThough why should you as a general user of Apache software be interested in\nattending then? Pieter Hintjens put it quite nicely in an interview on his\nlatest ZeroMQ book with O\u0026rsquo;Reilly:\nIf you are using free software in particular in commercial setups you really do\nwant to know how the project is governed and what it takes to get active and\ninvolved yourself. What would it take to move the project into a direction that\nfits your business needs? How do you make sure features you need are actually\nbeing added to the project instead of useless stuff?\nApacheCon is the conference to find out how Apache projects work internally,\nthe place to be to meet active people in person and put faces to names. Lots of\ncommunity building events focus on getting newbies in touch with long term\ncontributors.\n"},{"id":64,"href":"/how-to-get-your-submission-accepted-at-berlin-buzzwords236/","title":"How to get your submission accepted at Berlin Buzzwords","section":"Inductive Bias","content":" How to get your submission accepted at Berlin Buzzwords # Disclaimer: Intentionally posting on my private blog - these are my own criteria, not general advice from the review committee.\nBerlin Buzzwords is in it\u0026rsquo;s fourth year. Probably the most tedious task of all is having to select talks to make it into the final schedule. With roughly 120 submissions and roughly 30 slots to fill the result is that three quarters of all submissions have to be rejected. Last year I shared some details on how we do talk ranking given reviewers have provided their input.\nNow the mechanics of ranking are clear, people have asked me what goes into the reviews themselves. Here I can only speak for myself: After doing reviews ourselves during the first two years, Simon, Jan and myself decided to spread the work of reviewing submissions among a larger team of people. As nearly all of them had attended Berlin Buzzwords in the past already (or had at least followed the conference remotely) we could assume they were roughly familiar with what kind of content would be a good fit. As a result review guidelines that we send out tend to be rather light:\nBerlin Buzzwords is a conference from geeks for geeks: The goal is to get the people actively working in the field together to meet and exchange ideas. Content should have some technical depth - in particular pure marketing talks and obvious product placements without further technical value are not welcome. We usually invite both, interesting case studies as well as talks highlighting the technical details a project is built upon.\nIn the end judgement is up to the individual reviewer - so I can speak only for myself when listing what you should do to get your talk accepted.\nBe on topic. There\u0026rsquo;s always a handful of submissions that look and sound like pure marketing, product placement or simply aren\u0026rsquo;t related to software engineering at all. Those tend to be easy to spot and weed out.\nTell us what you are talking about. An abstract is there to provide some detail on your presentation - don\u0026rsquo;t be just funny, promising overly generic content. In order to decide whether or not your talk is relevant please provide some details on which direction you\u0026rsquo;ll be heading.\nDon\u0026rsquo;t be too detailed in the abstract neither - there\u0026rsquo;s no need to list the content of every slide. Make sure the abstract correctly summarizes your talk, making it catchy and nice to read usually helps if the content is solid.\nWe try to find those speakers that have not only an interesting topic to talk about but are also a pleasure to listen to, who can successfully get their point across. We cannot know every potential speaker in person though. As a result it helps if you list which conferences you\u0026rsquo;ve spoken at in the past, any videos of previous talks is helpful as well. As a general piece of advice: Choosing Berlin Buzzwords as your first conference to speak at ever usually is a great way to disaster. Get some practice at local meetups like the Berlin Hadoop Get Together, the data science day, the Java User Group Berlin Brandenburg, the RecSys Stammtisch Berlin or the MongoDB User Group Berlin to name just a few.\nMake sure your talk is novel - submitting the same topic in 2012 and 2013 is a great way to ensure getting rejected. Also it is fine to submit a talk you have given at another conference earlier. However if everyone in the Buzzwords audience is very likely to have watched the exact same version of your presentation earlier already, we are less likely to accept your talk.\nFinally: When drafting your bio make sure to include details that explain why you are the perfect expert to talk about the topic at hand. As much as I\u0026rsquo;d like to I don\u0026rsquo;t know every project\u0026rsquo;s committer by name. Provide some help by pointing out explicitly what your contributions have been or in what context you have used the technology you are presenting. Don\u0026rsquo;t be shy to list that you are a co-founder of a successful project. Not only does this information help with selecting talks, it also provides some background for the audience to judge the claims you make.\nTwo words on the role of free software at Buzzwords: There is no explicit requirement to only talk about software that is publicly available under a free software license however if some project or framework is presented it helps to be open source to raise the applicability for the audience. Most projects discussed at Berlin Buzzwords are developed openly. In order to get the maximum out of these projects it pays to know how they work internally, how to get active yourself, how to contribute. As a result discussions and talks on project governance are generally welcome.\nA parting note: With way more than half of all submissions to reject making a final decision will always be hard. Being rejected doesn\u0026rsquo;t necessarily mean that your proposal was bad. Following the above advise may raise chances of being accepted - however it is no guarantee. We could raise the number of accepted talks by extending the conference by another track or even another day - at the cost of raising the ticket price substantially. However we want not only \u0026ldquo;big corp representatives\u0026rdquo; but a diverse audience, attendees that get active themselves, that help shape the conference: Get active on stage at our Sunday evening Buzzwords Barcamp\nProvide introductory content to those entering the field at our weekend Open Tech School trainings\nPresent your favorite topic on our Open Stage during the main conference days.\nHost workshops, hacking lessons or a bug squashing party at our interactive program during the main conference days.\nOrganise or give a talk at one of our After-conference hackathons, workshops, meetups\nThere\u0026rsquo;s plenty of space and time to get active in addition to the main conference program. Use the time and space to shape the conference.\n"},{"id":65,"href":"/keepers-of-secrets-fosdem-08256/","title":"Keepers of secrets - FOSDEM 09","section":"Inductive Bias","content":" Keepers of secrets - FOSDEM 09 # The closing keynote was given by Leslie Hawthorn whom I had the pleasure of meeting last year during Berlin Buzzwords. In her talk she shared insights into a topic commonly encountered in open source leadership that is way less often talked about than should be the case: Being in the role of a community leader people will talk to you about all sorts of confidential information and ask you to not share that information with other no matter how beneficial that might be for both parties.\nEssentially if you\u0026rsquo;ve never been a community leader – it is much less about technical skills and way more about strategy, marketing, development events and unpaid therapy really.\nLeslie first introduced the types of secrets:\nThere are lots of one-on-one communications. There are several small group conversations. After all this is what makes humans human. However no matter how much a community trusts the people meeting in small groups, ultimately someone will feel betrayed, someone will suspect evil things being drafted in those discussions – even though the conversation really may just involve the quality of the beer they had yesterday.\nBeing social entities we ultimately need input from our peers. This may mean that we require input on things we perfectly well know that we are not supposed to discuss these topics with anyone.\nThere are secrets that are only secrets when told to the wrong person. There is information that is shared publicly – as in “on a website that requires no authentication whatsoever” - but that due to the nature of how information is discovered by certain people will never make it to the right person anyway.\nSome things are innociuous.\nSome things are blindingly apparent, but aren\u0026rsquo;t told anyway.\nAll of this becomes all the more interesting once you become a community leader. Your ultimate goal is to foster empathy and inclusion. You have to understand not only what you communicate, but also how to say certain things.\nOne example: Assume there is a contributor in a critical code path that is having a hard time privately and appears less and less often online. He told you the reason why, but asked you to not talk about it for whatever reason. On the other hand the community – being uninformed as they are – is loosing trust in the community member, blaming him for stopping progress. How should you react? Well, the three solution paths are extremely obvious but that doesn\u0026rsquo;t make them any easier:\nEncourage disclosure.\nAsk for permission to disclose parts yourself.\nEncourage the community to talk to the individual directly.\nThe worst you can do is to ignore the issue. Still that is what many people do, simply because it is the most comfortable solution. Go out of your comfort zone – your goal should be to make your project thrive.\nWhat about that one person that just doesn\u0026rsquo;t get they are hurting the project. The good-hearted person whose actions slow down the project? People on your project will get cranky, waste cycles on herding volunteer work if you avoid dealing with this person. There is no manual on dealing with frustrations and feelings in open source projects - though Poisonous people is a great intro to the topic by Brian Fitspatrick and Ben Collins-Sussman, so is their book on “Team Geek” published at O\u0026rsquo;Reilly:\nThough these issues are messy and make you feel uncomfortable – do deal with them as quickly as you can, otherwise they will kill your project. Either correct the educational issues of the person in question, suggest other ways to be effective and ultimately be willing to kindly but sincerely ask the person to move on.\nWe have negotiations each day – most of them we do not notice as they happen in the comfort zone of “I like the person and our interests are very well aligned.” The more uncomfortable ones that we actually remember are the ones involving either constellations where we do not like the people involved but are well aligned, where we like the people involved but aren\u0026rsquo;t well aligned or in the extreme case neither like the people involved nor are we well aligned. Especially in the uncomfortable situations it makes sense to remember that negotiations really come in up to six stages:\nBeing willing to openly ask for what you need\nAsking for what you need.\nFinding common ground and reaching agreement\nIf impossible, finding the best alternative for boot\nIf still impossible, agreeing to not having reached agreement.\nValue honesty above all, but really do not be a tactless jerk. Diplomacy in order to reach your goals is ok: Ultimately you have to decide whether you want to be right or whether you want to win.\nTo summarise make sure you care about your project – the people in it will need most love when you have most reason to hate them.\nOne final recommendation after a question for leader burnout from the audience: Noticing burn out is as easy as observing that each morning you wake up with that “oh no, I don\u0026rsquo;t want to do this, I want to walk away” kind of feeling wrt. to stuff that formerly used to be a lot of fun to do. First counter measure: RUN AWAY! Take vacation, turn of your electronics, hug a tree – get away from what is turning you down. After returning make sure you involve your peers in your work. If you cannot get on with your former pet project, find a successor. Nothing will kill your project faster than a burnt out leader dragging the project down. The reason for your burn out really can be as simple as having seen the same negative things over and over again so you do not want to deal with them yet again and having seen the same positive things over and over again so they do no longer give you any reward for your work. It may just be time to move on and do something else.\n"},{"id":66,"href":"/on-making-libre-office-suck-less-a-major-refactoring-effort-fosdem-08307/","title":"On making Libre Office suck less – a major refactoring effort - FOSDEM 08","section":"Inductive Bias","content":" On making Libre Office suck less – a major refactoring effort - FOSDEM 08 # Libre Office is currently in a phase of code cleanup and refactoring that turns the whole code base upside down. What that means is that people need tooling to avoid quality from going down and allow for new features going in without too much risk. The project made good experiences with using gerrit for code review of patches, tinderbox for fast integration testing, strict whitespace checks to avoid unintended mistakes, use clang compiler plugins. They have less process that allows for change anywhere in any part of the code base.\nThere is an easy hacks page for people to get started quickly. I know that kind of thing from the Hadoop issue tracker and really appreciate having this to get new developers comfortable with the code base and all the tooling around. They apply reply-header mangling to allow for responses to go back to posters on their mailing list w/o prior subscription. They moved from their own dmake that wasn\u0026rsquo;t industry standard to standard make tooling to build the project. They are in the process of translating all the German comments – shout-out to all German speaking readers of this blog: Help the Libre Office developers understand the code better by providing your German speaking skills for translation.\nSome anecdotes: They found 4+ String classes in the code base and managed to get rid of one of them only recently. They are busy killing dead code, kicking out string macros, fixing cpplint warnings, refactoring code that was writing pre-STL into clean STL code, getting rid of obsolete libraries, fixing the windows installer, killing proprietary translation services and replacing that with an open one, getting rid of cargo-cult componentisation, getting rid of code duplication e.g. in the import filter implementation. They are reducing structure sizes for calc, switching to a layout based frontend, optimising the red lining for writer. The goal is to really have no no-go areas.\nIn order to retain quality in this fluid setup they opted for a drastic increase in unit- and integration test coverage, using bug documents as source for tests. Though the Bugzilla assistant they made it way easier even for non-experts and end-users to submit bug reports.\nThey are going for time-based, 6-monthly releases. Due to a long build time they are keeping track of all past binary builds for bi-section purposes – currently in git which most likely isn\u0026rsquo;t the most ideal choice.\nWhat works is putting graphs of bugs created vs. fixed over time in front of developers to keep the number of bugs low.\nWithin version 4.0 Libre Office is shipping:\nbetter interop features for word documents with comments\nRTF drawing imports\nRTF improved formulae import\nDocx annotation support\nCMIS support for better interaction with Sharepoint, Alfresco and Nuxeo\nMore import filters e.g. for Microsoft publisher\nVisio is now completely supported\nSupport for arbitrary XML to spreadsheet mappings\nConditional formulae\nStock option pricing support\nAndroid remote control support for slides\nLibre Logo integration for schools\nImage rendering, smoothing, re-sizing and scaling was improved\nBetter support for right-to-left writing arabic languages\nStyle previews for fonts\nBetter unity integration\nThere even is an Android port in the works!\n"},{"id":67,"href":"/e17-fosdem-07165/","title":"E17 - FOSDEM 07","section":"Inductive Bias","content":" E17 - FOSDEM 07 # I\u0026rsquo;m really glad the NoSQL room was all packed on afternoon – otherwise I\u0026rsquo;d have missed an amazing talk by people behind Enlightenment – a window manager that is older than Gnome, nearly older than KDE and has been my favourite choice for years and years (simply because they have sensible default configuration options: focus follows mouse, virtual desktops that allow for desktop switching when moving the mouse close to the screen edges, menu opening when clicking anywhere on the desktop background, options for remembering window placement and configuration on re-boot etc).\nFinally in December last year they actually did realease E17 after more than a decade of work. They now feature a tiling module, split desktops per screen, launchers, taskbars, systrays (brrr), screenshotting and multiple sharing options, custom layout modules for desktop and mobile.\nThere\u0026rsquo;s a full fledged file manager that is also used as file selector. There is a compositor with wayland client support, that works decently even on old or slow hardware (think raspberry pi).\nTheir main goal is not to build a window manager that even your grandma can use. Rather they focus on stuff for the geeks that just works, is efficient, has lots of eye candy and when run on a nexus7 instead of unity saves 200MB of RAM.\nTheir main goal is to be a base for touch and mobile development. The number of desktops is shrinking giving way to more and more mobile devices. Fortunately the project is now sponsored (as in paid developers) by Samsung as part of their Tizen efforts. E17 does work as part of Tizen for years now, the only part missing is a product running the software available for purchase.\nThe goals for E18 (to be released end of 2013 – hear, hear) include going beyond the desktop, to polish things up, provide more default profiles for diverse devices, optimise battery and memory consumption, run without swap space, avoid going to memory instead of the cache to avoid draining the battery of mobile devices. There will be image and font sharing across processes, faster software rendering, async rendering with more threads. There\u0026rsquo;s even thoughts to deal with different finger size issues on touch devices.\nOn using the composite manager as default: It made the code and optimisation a whole lot easier, though there are still issues with multiple screens that all switch compositing off in case of full screen games that cannot run with it turned on.\nThere will be work to integrate better with wayland, support for physics and sounds in themes, more compositing signals, improved gadget infrastructures, easier content sharing options – and all the cool stuff users can think of.\n"},{"id":68,"href":"/systemd-fosdem-06387/","title":"Systemd - FOSDEM 06","section":"Inductive Bias","content":" Systemd - FOSDEM 06 # As sort of a “go out of your comfort zone and discover new stuff” exercise I went to the systemd – two years later talk next. It\u0026rsquo;s just plain amazing to see a machine boot in roughly one second (that is not counting the 7s that the BIOS needs for initialization). The whole project started as a init-only project but has since grown to a much larger purpose: An init platform ranging from mobile, embedded, desktop devices to servers many features were just over-due across the board.\nEssentially the event-based system brings together what was split and duplicated before in things like console-kit, sysVinit, initscripts, inetd, pm-utils, acpid, syslog, watchdog services, cgrulesd, cron and atd. It brings support for event based container spawning, suspending and shutdown which brings whole new opportunities for optimisations. In addition for the first time in the history of Linux there is the possibility of grouped resource management: Instead of having nice levels bound to processes you now can group services to cgroups and give them guaranteed resources (which makes resource management of e.g multiple Apache processes plus some MySQL instances all running on the same machine so much easier).\n(Post kindly proof-read and corrected by Thilo Fromm)\n"},{"id":69,"href":"/notes-on-storage-options-fosdem-05300/","title":"Notes on storage options - FOSDEM 05","section":"Inductive Bias","content":" Notes on storage options - FOSDEM 05 # On MySQL\nSecond day at FOSDEM for me started with the MySQL dev room. One thing that made me smile was in the MySQL new features talk: The speaker announced support for “NoSQL interfaces” to MySQL. That is kind of fun in two dimensions: A) What he really means is support for the memcached interface. Given the vast number of different interfaces to databases today, announcing anything as “supports NoSQL interfaces” sounds kind of silly. B) Given the fact that many databases refrain from supporting SQL not because they think their interface is inferior to SQL but because they sacrifice SQL compliance for better performance, Hadoop integration, scaling properties or others this seems really kind of turning the world upside-down.\nAs for new features – the new MySQL release improved the query optimiser, subquery support. When it comes to replication there were improvements along the lines of performance (multi threaded slaves etc.), data integrity (replication check sums being computed, propagated and checked), agility (support for time delayed replication), failover and recovery.\nThere were improvements along the lines of performance schemata, security, workbench features. The goal is to be the go-to-database for small and growing businesses on the web.\nAfter that I joined the systemd in Debian talk. Looking forward to systemd support in my next Debian version.\nHBase optimisation notes\nLars George\u0026rsquo;s talk on HBase performance notes was pretty much packed – like any other of the NoSQL (and really also the community/marketing and legal dev room) talks.\nHBase Sizing Notes from larsgeorge Lars started by explaining that by default HBase is configured to reserve 40% of the JVM heap for in memory stores to speed up reading, 20% for the blockcache used for writing and leaves the rest as breath area.\nOn read HBase will first locate the correct region server and route the request accordingly – this information is cached on the client side for faster access. Prefetching on boot-up is possible to save a few milliseconds on first requests. In order to touch as little files as possible when fetching bloomfilters and time ranges are used. In addition the block cache is queried to avoid going to disk entirely. A hint: Leave as much space as possible for the OS file cache for faster access. When monitoring reads make sure to check the metrics exported by HBase e.g. by tracking them over time in Ganglia.\nThe cluster size will determine your write performance: HBase files are so-called log structured merge trees. Writes are first stored in memory and in the so-called Write-Ahead-Log (WAL, stored and as a result replicated on HDFS). This information is flushed to disk periodically either when there are too many log files around or the system gets under memory pressure. WAL without pending edits are being discarded.\nHBase files are written in an append-only fashion. Regular compactions make sure that deleted records are being deleted.\nIn general the WAL file size is configured to be 64 to 128 MB. In addition only 32 log files are permitted before a flush is forced. This can be too small a file size or number of log files in periods of high write request numbers and is detrimental in particular as writes sync across all stores, so large cells in one family will cause a lot of writes.\nBypassing the WAL is possible though not recommended as it is the only source for durability there is. It may make sense on derived columns that can easily be re-created in a co-processor on crash.\nToo small WAL sizes can lead to compaction storms happening on your cluster: Many small files than have to be merged sequentially into one large file. Keep in mind that flushes happen across column families even if just one family triggers.\nSome handy numbers to have when computing write performance of your cluster and sizing HBase configuration for your use case: HDFS has an expected 35 to 50 MB/s throughput. Given different cell size this is how that number translates to HBase write performance:\nCell sizeOPS\n0.5MB70-100\n100kB250-500\n\u0026lt; td\u0026gt;10kBwith 800 less than expected as this HBase is not optimised for these sizes\n1kB6000, see above\nAs a general rule of thumb: Have your memstore be driven by size number of regions and flush size. Have the number of allowed WAL logs before flush be driven by fill and flush rates.. The capacity of your cluster is driven by the JVM heap, region count and size, key distribution (check the talks on HBase schema design). There might be ways to get rid of the Java heap restriction through off-heap memory, however that is not yet implemented.\nKeep enough and large enough WAL logs, do not oversubscribe the memstore space, keep the flush size in the right boundaries, check WAL usage on your cluster. Use Ganglia for cluster monitoring. Enable compression, tweak the compaction algorithm to peg background I/O, keep uneven families in separate tables, watch the metrics for blockcache and memstore.\n"},{"id":70,"href":"/afero-gpl-panel-discussion-fosdem-047/","title":"AFERO GPL Panel discussion - FOSDEM 04","section":"Inductive Bias","content":" AFERO GPL Panel discussion - FOSDEM 04 # The panel started with a bit of history of the AGPL: Born in the age of growing ASP (application service provider) businesses AGPL tried to fix the hosting loop whole in GPL in the early 2000s. More than ten years later it turns out the license hasn\u0026rsquo;t quite caught traction: On the one hand the license does have a few wording issues. In addition it is still rather young and used by few so there is less trust compared to GPL or ASL to last when put on trial. However there\u0026rsquo;s another reason for low adoption:\nThose that are being targeted with the license – people developing web services – tend to prefer permissive licenses over copyleft ones (see Django, Rails for example). People are still in the postion of trying to gain strong positions when opening up their infrastructure. As a result there is a general preference for permissive licenses. Also there are many more people working on open source not as their hobby project but as their general day job. As a result the number of people backing projects that are infrastructure only, company driven and trying to establish de-facto standards through the availability of free software is growing.\nDepressing for the founders of AGPL are businesses using the AGPL to try and trick corporations into using their software as open source and later go after them with additional clauses in their terms and conditions to enforce subscription based services.\n"},{"id":71,"href":"/mozilla-legal-issues288/","title":"Mozilla legal issues - FOSDEM 03","section":"Inductive Bias","content":" Mozilla legal issues - FOSDEM 03 # In the next talk Gervase Markham talked about his experience working for Mozilla on legal and license questions. First the speaker summarized what kind of requests he gets most:\nThere are lots of technical support requests.\nNext on the top list is the question for whether or not shipping Mozilla with a set of modifications is ok.\nNext is an internal question, namely: Can I use this code?\nRelated to that is the “We have a release in two weeks, can we ship with this code?”\nAnother task is finding code that was used but is not ok.\nYet another one is getting code licensed or re-licensed.\nMaintaining the about:license page is another task.\nDealing with ECCV/CCATS requests is another issue that comes up often.\nHowever there are also bigger tasks: There was the goal of tri-licensing Mozilla. The only issue was the fact that they had accumulated enough individually copyrighted contributions to make that that task really tricky. In the end they wrote a tool to pull out all contributor names, send them mails asking for permission to tri-license. After little over three years they had responses from all but 30 contributors. As a result the “find this hacker” campaign was launched on /. and other news sites. In the end most could be found.\nAs another step towards easier licensing the MPL 2 replacing 1.1 was introduced – it fixes GPL/ASL license incompatibilities, notification and distribution requirements, the difference for initial developers and the use of conditional/Jacobson language.\nThere are still a few issues with source files lacking license headers (general advise that never has been tested in court is the concept of license bleeding: If there are files with and without license headers in one folder, most likely those w/o have the same license as those with. “aehem” ;)\nThere are lots of questions on license interpretation. This includes questions from people wanting to use Mozilla licensed software that wasn\u0026rsquo;t even developed within the Mozilla foundation. Also there are lots of people who do not understand the concept of “free does not mean non-commercial use only”.\nSometimes there a license archeology task where people ask “hey, is that old code yours and is it under the Mozilla license?”\nAnother interesting case was a big, completely unknown blue company asking whether the hunspell module, having changed licenses so often (from BSD forked to GPL, changed to LGPL, to CC-Attr, to the tri license of Mozilla, including changed GPL stuff with the author\u0026rsquo;s permission) really can be distributed by Mozilla under the MPL. After lots of digging through commit logs and change logs they could indeed verify that the code is completely clean.\nThen there was the case of Firefox OS which was a fast development effort, involving copying lots of stuff from all over the internet just to get things running. A custom license scanner written to verify all bits and pieces was finally implemented and used to give clearance on release. It found dozens of distinct versions of the Mozilla and BSD licenses (mainly due to the fact that people are invited to add their own name to it when releaseing code). As a result there now is a discussion on OSI to discourage that behaviour to keep the number of individual license files to ship with the software down to a minimal number.\nThe speaker\u0026rsquo;s general recommendation on releasing small software projects under a non-copyleft license was to use the CC-0 license, for larger stuff his recommendation was to go for the ASL due to its patent grant clauses. Even at Mozilla quite a few projects have switched over to Apache.\nThere also were a few license puzzlers:\nOpenJDK asked for permission to use their root-store certificates. Unfortunately at the time of receiving them they had not been given any sort of contract under which they may use them. ahem\nThe case with search engine icons … really isn\u0026rsquo;t … so much different.\nThere also tend to be some questions on the Firefox/Mozilla trademarks ranging from\n“can I use your logo for purpose such\u0026rsquo;n\u0026rsquo;such”?\n”Do you have \u0026lsquo;best viewed in\u0026hellip;\u0026rsquo; button”? - Nope, as we generally appreciate developers writing web sites that comply with web standards instead of optimizing for one single browser only.\nThey did run into the subscription on download scam trap and could stop those sites due to trademark infringement.\nMost of this falls under fair use – especially cases like Pearson asking for permission (with a two-page mail + pdf letter) to link to the mozilla web site\u0026hellip;\nIn general when people ask for permission if they do not need to ask: Tell them so but give them permission anyway. This is in order to avoid an “always-ask-for-permission” culture, and really to keep the number of requests down to those that are really necessary. One thing that does need prior permission though is shipping Firefox with a bunch of plugins pre-installed as a complete package.\nOn Patents – Mozilla does not really have any and spends time (e.g. on OPUS) avoiding them. On a related note there sometimes even are IPO requests.\n"},{"id":72,"href":"/trademarks-and-oss412/","title":"Trademarks and OSS - FOSDEM 02","section":"Inductive Bias","content":" Trademarks and OSS - FOSDEM 02 # So the first talk I went to ended up being in the legal dev room on trademarks in open source projects. The speaker had a background mainly in US American trademark law and quite some background when it comes to open source licenses.\nTo start Pamela first showed a graphic detailing the various types of trademarks: In the pool of generic names there is a large group of trademarks that are in use but not registered. The amount of registered trademarks actually is rather small. The main goal of trademarks is to avoid confusing costumers. This is best seen when thinking about scammers trying to trick users into downloading users pre-build and packaged software from third party servers demanding a credit card number that is later charged based on a subscription service the user signed by clicking away the fine print on the download page. Canonical example seems to be e.g. the Firefox/Mozilla project that was effected by this kind of scam. But also other end-user software (think Libre/Open Office, Gimp) could well be targets. This kind of deceiving web pages usually can be taken down way faster with a cease and desist letter due to trademark infringement rather than due to the fraud they do.\nSo when selecting trademarks – what should a project look out for? One is the name should not be too generic as that would lead to a name that is not enforceable. It should not be too theme-y as the names that are themed usually are already taken. The time to research should be contrasted with the pain it will cost to rename the project in case of any difficulties.\nThere are few actual court decisions that relate to trademarks and OSS: In Germany it was decided that forking the ENIGMA project and putting it on set-op boxes but keeping the name was ok for as long as the core function would be kept and third party plugins would still work.\nIn the US there was a decision that keeping the name of re-furbished SparkPlugs is ok for as long as it is clearly marked what to expect when buying them (in this case re-furbished instead of newly made).\nAnother thing to keep in mind are trademarks are naked trademarks – those that were not enforced and have become too ubiquitous. In the US that would be the naked license trademarks, in Greece the recycling mark “Der Grüne Punkt” has become too ubiquitous to be treated as a trademark any more.\nTrademark law already fails in multinational corporation setups with world wide subsidies. It gets even worse with world wide distributed open source projects. The question of who owns the mark, who is allowed to enforce it, who exercises control gets worse the more development is distributed. When new people take over trademarks there should be some clear paper transferral document to avoid confusion.\nTrademarks only deal with avoiding usage confusion: Using the mark when talking about it is completely fine. Phrases like “I\u0026rsquo;m $mark compatible”, “I\u0026rsquo;m running on top of $mark” care completely ok. However make sure to use as little as possible – there is no right to also just use the logos, icons or design forms of the project you are talking about – unless you are talking about said logo of course.\nSo to conclude: respect referential use, you can\u0026rsquo;t exercise full control but should avoid exercising too little control.\nThere is a missing consistent understanding of and behaviour towards trademarks in the open source community. Now is the time to shape the law according to what open source developers think they need.\n"},{"id":73,"href":"/fosdem-2013-01188/","title":"FOSDEM 2013 - 01","section":"Inductive Bias","content":" FOSDEM 2013 - 01 # On Friday morning our train left for this year\u0026rsquo;s FOSDEM. Though a bit longish I have a strong preference for going by train as this gives more time and opportunity for hacking (in my case trying out Elastic Search), reading (in my case the book “Team Geek”) and chatting with other FOSDEM visitors.\nMonday morning was mostly busy with meeting people - at the FSFE, Debian, Apache Open Office booths, generally in the hallways. And with getting some coffee to the Beaglebone booth where my husband helped out . For really fun videos on the hardware they had there see:\nif you want to get the hardware underneath talk to circuitco.\nUnfortunately I didn\u0026rsquo;t make it to the community and marketing room – too full during the talks that I wanted to see (as a general shout-out to people attending conferences: If you do not find a seat, move into the room instead of standing right next to the door, if you do have a seat and a free one just next to you, move to the seat next to you).\nIf you missed some of the talks you might want to try your luck with the FOSDEM video archive - it\u0026rsquo;s really extensive featuring videos taken at previous editions as well and is a great resource to find talks of the most important tracks.\n"},{"id":74,"href":"/elastic-search-meetup-berlin-january-2013168/","title":"Elastic Search meetup Berlin – January 2013","section":"Inductive Bias","content":" Elastic Search meetup Berlin – January 2013 # The first meetup this year I went to started with a large bag of good news for Elastic Search users. In the offices of Sys Eleven (thanks for hosting) the meetup started at 7p.m. last Tuesday. Simon Willnauer gave an overview of what to expect of the upcoming major release of Elastic Search:\nFor all 0.20.x version ES features a shard allocator version that is ignorant of which index shards belong to, machine properties, usage patterns. Especially ignoring index information can be detrimental and lead to having all shards of one index on one machine in the end leading to hot spots in your cluster. Today this is solved by lots of manual intervention or even using custom shard allocator implementations.\nWith the new release there will be an EvenShardCountAllocator that allows for balancing shards of indexes on machines – by default it will behave like the old allocator but can be configured to take weighted factors into account. The implementation will start with basic properties like “which index does this shard belong to” but the goal is to also make variables like remaining disk space available. To avoid constant re-allocation there is a threshold on the delta that has to be passed for re-allocation to kick in.\n0.21 will be released when Lucene 4.1 is integrated. That will bring new codecs, concurrent flushing (to avoid the stop-the-world flush during indexing that is used in anything below Lucene 4 – hint: Give less memory to your JVM in order to cause more frequent flushes), there will be compressed sort fields, spellchecking and suggest built into the search request (though unigram only). There will be one similarity configurable per field – that means you can switch from TF-IDF to alternative built-in scoring models or even build your own. Speaking of rolling your own: There is a new interface for FieldData (used for faceting, scoring and sorting) to allow for specialised data structures and implementations per field. Also the default implementation will be much more memory efficient for most scenarios be using UTF-8 instead of UTF-16 characters).\nAs for GeoSpatial: The code came to Lucene as a code dump that the contributor wasn\u0026rsquo;t willing to support or maintain. It was replaced by an implementation that wasn\u0026rsquo;t that much better. However the community is about to take up the mess and turn it into something better.\nAfter the talk the session essentially changed to an “interactive mailing list” setup where people would ask questions live and get answers both from other users as well as the developers. Some example was the question for recommendability of pyes as a library. Most people had used it, many ran into issues when trying to run an upgrade with features being taken away or behaviour being changed without much notice. There are plans to release Perl, Ruby and Python clients. However also using JRuby, Groovy, Scala or Clojure to communicate with ES works well.\nOn the benefit of joining the cluster for requests: That safes one hop for routing, result merging, is an option to have a master w/o data and helps with indexing as the data doesn\u0026rsquo;t go through an additional node. As for plugins the next thing needed is an upgrade and versioning schema. Concerning plugin reloading without restarting the cluster there was not much ambition to get that into the project from the ES side of things – there is just too much hazzle when it comes to loading and unloading classes with references still hanging around to make that worthwhile.\nSpeaking of clients: When writing your own don\u0026rsquo;t rely on the binary protocol. This is a private interface that can be subject to change at any time.\nWhen dealing with AWS: The S3 gateway is not recommended to be used as it is way too slow (and as a result very expensive). Rather backup with replicas, keep the data around for backup or use rsync. When trying to backup across regions this is nothing that ES will help you with directly – rather send your data to both sites and index locally. One recommendation that came from the audience was to not try and use EBS as the IO optimised versions are just too expensive – it\u0026rsquo;s much more cost effective to rely on ephermeral storage. Another thing to checkout is the support for ES being zone aware to avoid having all shards in one availability zone. Also the node discovery timeout should be increased to at least one minute to work in AWS. When it comes to hosted solutions like heroko you usually are too limited in what you can do with these offers compared to the low maintenance overhead of running your own cluster. Oh, and don\u0026rsquo;t even think about index encryption if you want to have a fast index without spending hours and hours of development time on speeding your solution up with custom codecs and the like :)\nLooking forward to the Elastic Search next meetup end of February – location still to be announced. It\u0026rsquo;s always interesting to see such meetup groups grow (this time from roughly 15 in November to over 30 in January).\nPS: A final shout-out to Hossman - that psychological trick you played on my at your boosting and biasing talk at Apache Con EU is slightly annoying: Everytime someone mentions TF-IDF in a talk (and that isn\u0026rsquo;t too unlikely in any Lucene, Solr, Elastic Search talks) I panicingly double check whether there are funny pictures on the slide shown! ;) "},{"id":75,"href":"/linux-vs-hadoop-some-inspiration265/","title":"Linux vs. Hadoop - some inspiration?","section":"Inductive Bias","content":" Linux vs. Hadoop - some inspiration? # This (even for my blog’s standards) long-ish blog post was inspired by a talk given late last year at Apache Con EU as well as from discussions around what constitutes “Apache Hadoop compatibility” and how to make extending Hadoop easier. The post is based on conversations with at least one guy close to the Linux kernel community and another developer working on Hadoop. Both were extremly helpful in answering my questions and sanity checking the post below. After all I’m neither an expert on Linux kernel development and design, nor am I an expert on the detailed design and implementation of features coming up in the next few Hadoop releases. Thanks for your input.\nPosting this here as I thought the result of my trials to understand the exact design commonalities and differences better might be interesting for others as well. Disclaimer: This is by no means an attempt to influence current development, it just summarizes some recent thoughts and analysis. As a result I’m happy about comments pointing out additions or corrections - preferably as trackback or maybe on Google Plus as I had to turn of comments on this very blog for spamming reasons.\nIn his slides on “Insides Hadoop dev” during Apache Con EU:\nInside hadoop-dev from Steve Loughran Steve Loughran included a comparison that popped up rather often already in recent past but still made me think:\n“Apache Hadoop is an OS for the datacenter”\nIt does make a very good point, even though being slightly misleading in my opinion:\nThere are lots of applications that need to run in a datacenter that do not imply having to use Hadoop at all - think mobile application backends, content management systems of publishers, encyclopedia hosting. Growing you may still run into the need for central log processing, scheduling and storing data.\nEven if your application benefits from a Hadoop cluster you will need a zoo of other projects not necessarily related to the project to successfully run your cluster - think configuration management, monitoring, alerting. Actually many of these topics are on the radar of Hadoop developers - with an intend to avoid the NIH principle and rather integrate better with existing proven standard tools.\nHowever if you do want to do large scale data analysis on largely unstructured data today you will most likely end up using Apache Hadoop.\nWhen talking about operating systems in the free software world inevitably the topic will drift towards the Linux kernel. Being one the most successful free software projects out there from time to time it’s interesting and valuable to look at its history and present in terms of development process, architecture, stakeholders in the development cycle and the way conflicting interests are being dealt with.\nAlthough interesting in many dimensions this blog post focuses just on two related aspects: How to balance innovation for stability in critical parts of the system.\nHow to deal with modularity and API stability from an architectural point of view taking project-external (read: non-mainline) module contributions into account.\nThe post is not going to deal with just “everything map/reduce” but focus solely on software written specifically to work with Apache Hadoop. In particular Map/Reduce layers plugged on top of existing distributed file systems that ignore data locality guarantees as well as layers on top of existing relational database management systems that ignore easy distribution and fail over are intentionally being ignored.\nBalancing innovation with stability\nOne pain point mentioned during Steve’s talk was the perceived need for a very stable and reliable HDFS that prevents changes and improvements from making it into Hadoop. The rational is very simple: Many customers have entrusted lots (as in not easy to re-create in any reasonable time frame) of critical (as in the service offered degrades substantially when no longer based on that data) data to Hadoop. Even when in a backup Hadoop going down for a file system failure would still be catastrophic as it would take ages to get all systems back to a working state - time that means loosing lots of customer interaction with the service provided.\nWhen glancing over to Linux-land (or Windows, or MacOS really) the situation isn’t much different: Though both backup and recovery are much cheaper there, having to restore a user’s hard-disk just due to some weird programming mistake still is not acceptable. Where does innovation happen there? Well, if you want durability and stability all you do is to use one of the well proven file system implementations - everyone knows names like ext2, xfs and friends. A simple “man mount” will reveal many more. If on the contrary you need some more cutting edge features or want to implement a whole new idea of how a file system should work, you are free to implement your own module or contribute to those marked as EXPERIMENTAL.\nIf Hadoop really is the OS of the datacenter than maybe it’s time to think about ways that enable users to swap in their prefered file system implementation, maybe it’s time for developers to focus implementation of new features that could break existing deployed systems to separate modules. Maybe it’s time to announce an end-of-support-date for older implementations (unless there are users that not only need support but are willing to put time and implementation effort into maintaining these old versions that is.)\nDealing with modularity and API stability\nWith the vision of being able to completely replace whole sub-systems comes the question of how to guarantee some sort of interoperability. The market for Hadoop and surrounding projects is already split, it’s hard to grasp for outsiders and newcomers which components work with wich version of Hadoop. Is there a better way to do things?\nLooking at the Linux kernel I see some parallels here: There’s components built on top of kernel system calls (tools like ls, mkdir etc. all rely on a fixed set of system calls being available). On the other hand there’s a wide variety of vendors offering kernel drivers for their hardware. Those come in three versions:\nSome are distributed as part of the mainline kernel (e.g. those for Intel graphics cards).\nSome are distributed separately but including all source code (e.g. ….)\nSome are distributed as binary blog with some generic GPLed glue logic (e.g. those provided by NVIDIA for their graphics cards).\nEssentially there are two kinds of programming interfaces: ABIs (Application Binary Interfaces) that are being developed against from user space applications like “ls” and friends. APIs (Application Programming Interfaces) that are being developed against by kernel modules like the one by NVIDIA.\nComing back to Hadoop I see some parallelism here: There are ABIs that are being used by user space applications like “hadoop fs -ls” or your average map/reduce application. There are also some sort of APIs that strictly only allow for communication between HDFS, Map/Reduce and applications on top.\nThe Java ecosystem has a history of having APIs defined and standardised through the JCP and implemented by multiple vendors afterwards. With Apache projects people coming from a plain Java world often wonder why there is no standard that defines the APIs of valuable projects like Lucene or even Hadoop. Even log4j, commons logging and build tooling follow the “defacto standardisation” approach where development defines the API as opposed to a standardisation committee.\nGoing one step back the natrual question to ask is why there is demand for standardisation. What are the benefits of having APIs standardised? Going through a lengthy standardisation process obviously can’t be the benefit.\nAdvantages that come to my mind:\nWhen having multiple vendors involved that do not want to or cannot communicate otherwise a standardisation committee can provide a neutral ground for communication in particular for the engineers involved.\nFor users there is some higher level document they can refer to in order to compare solutions and see how painful it might be to migrate.\nHaving been to a DIN/ISO SQL meetup lately there’s also a few pitfalls that I can think of:\nYou really have to make sure that your standard isn’t going to be polluted with things that never get implemented just because someone thought a particular feature could be interesting.\nStandardisation usually takes a long time (read: mutliple years) until something valuable that than can be adopted and implemented in the industry is created.\nMore concerns include but are not limited to the problem of testing the standard - when putting the standard into main focus instead of the implementation there is a risk of including features in the standard that are hard or even impossible to implement. There is the risk of running into competing organisations gaming the system, making deals with each other - all leading to compromises that are everything but technologically sensible. There clearly is a barrier to entry when standardisation happens in a professional standards body. (On a related note: At least the German group working on the DIN/ISO standard defining the standard query language in particular in big data environments. Let me know if you would like to get involved.)\nConcerning the first advantage (having some neutral ground for vendors to meet): Looking at your average standardisation effort those committees may be neutral ground. However communication isn’t necessarily available to the public for whatever reasons. Compared to the situation little over a decade ago there’s also one major shift in how development is done on successful projects: Software is no longer developed in-house only. Many successful components that enable productivity are developed in the open in a collaborative way that is open to any participant. Httpd, Linux, PHP, Lucene, Hadoop, Perl, Python, Django, Debian and others are all developed by teams spanning continents, cultures and most importantly corporations. Those projects provide a neutral ground for developers to meet and discuss their idea of what an implementation should look like.\nPondering a bit more on where successful projects I know of came from reveals something particularly interesting: ODF first was implemented as part of Open Office and then turned into a standardised format. XMPP was first implemented and than turned into an IETF standardised protocol. Lucene never went for any storage format or even search API standardisation but defined very rigid backwards compatibility guidelines that users learnt to trust. Linux itself never went for ABI standardisation - instead they opted for very strict ABI backwards compat guidelines that developers of user space tools could rely on.\nLooking at the Linux kernel in particular the rule is that user facing ABIs are supposed to be backwards compatible: You will always be able to run yesterday’s ls against a newer kernel. One advantage for me as a user is that this way I can easily upgrade the kernel in my system without having to worry about any of the installed user space software.\nThe picture looks rather different with Linux’ APIs: Those are intentionally not considered holy and subject to change if need be. As a result vendors providing proprietary kernel driver like NVIDIA have the burden of providing updated versions in case they want to support more than one kernel version.\nI could imaging a world similar to that for Hadoop: A world in which clients run older versions of Hadoop but are still able to talk to their upgraded clusters. A world in which older MapReduce programs still run when deployed on newer clusters. The only people who would need to worry about API upgrades would be those providing plugins to Hadoop itself or replace components of the system. According to Steve this is what YARN promises: Turn MR into user layer code, have the lower level resource manager for requesting machines near the data. "},{"id":76,"href":"/abc-die-katze-lief-im-schnee5/","title":"ABC - die Katze lief im Schnee","section":"Inductive Bias","content":" ABC - die Katze lief im Schnee # Seen this morning in Berlin:\nA little impression from what the city looked like the weeks before it turned green on Christmas:\nFor winter images of other years see also previous posts. Title taken from a children\u0026rsquo;s song:\n"},{"id":77,"href":"/on-taming-text309/","title":"On Taming Text","section":"Inductive Bias","content":" On Taming Text # This time of the year I would usually post pictures of my bicycle standing in the snow somewhere in Tierpark. This year however I was tricked into using public transport instead: a) After my husband found a new job, we now share some of the route to work - and he isn\u0026rsquo;t crazy going by bike when it\u0026rsquo;s snowing. b) I got myself a Nexus7 earlier this month which obsoleted having to take paper books with me when using public transport. c) Early in December Grant Ingersoll asked me for feedback on the by now nearly finished \u0026ldquo;Taming Text (currently available as MEAP at Manning). So I even had a really interesting book to read on my way home.\nUp to mid-December \u0026ldquo;Taming Text\u0026rdquo; was one of those books that always were very high on my to-read list: At least from the TOC it looked like the book to read if ever you wanted to write a search application. So I was really curious which topics it would cover and how deep explanations would go when I got the offer to read and review the book.\ntl\u0026amp;dr\nShort version: If you are building search applications - that is anything that makes a search box available on a web site, be it an online store or a new article archive - this is the book to read. It covers all the gory details of how to implement features we have come to take for granted when using search: Type ahead, spelling correction, facetting, automatic tagging and more. The book motivates what the value of these features is from the user side, explains how to implement these features with proven technologies like Apache Lucene, OpenNLP, and Mahout and how those projects work internally to provide you with the functionality you need.\nLonger summary\nSearch can be as easy as providing one box in some corner on your web site that users can type into to find relevant pages. However when thinking about the topic just a little more some more handy features that users have come to expect come to mind: Type ahead to avoid superfluous typing - it also comes in handy to avoid spelling errors and to know exactly which query actually will return a decent number of documents.\nSpelling correction is pretty much standard - and avoids user frustration with hard to spell query terms.\nFacetting is a great way to discover and explore more content in particular when there are a few structured attributes attached to your items (prices to books, colors to cars etc).\nNamed Entity Recognition is well known among publishers who use automatic tagging services to support their staff.\nThe authors of Taming Text decided to structure the book around the task of building an automatic Question Answering system. Throughout the book they present technologies that need to be orchestrated to build such an application but are each valuable in it\u0026rsquo;s own right.\nIn contrast to Search Patterns (which is focused mainly on the product manager perspective and contains much less technical detail) Taming Text is the book to read for any engineer working on search applications. In contrast to books like Programming Collective Ingelligence Taming Text takes you one level further by not only showing the tools to use but also explaining their inner workings so that you can adapt them exactly to your use case. To me, Taming Text is the ideal complimentary book to Mahout in Action (for the machine learning part) and Lucene in Action for the search part.\nBack in 1998 it was estimated that 80% of all information is unstructured data. In order to make sense of that wealth of data we need technologies that can deal with unstructured data. Search is one of the most basic but also most powerful ways to analyse texts. With a good mixture of theoretical background and hands-on-examples Taming Text guides you through the process of building a successful search application, no matter if you are dealing with a vast product database that you want to make more accessible to your users, with an ever growing news archive or with several blog posts and twitter messages that you want to extract data from.\n"},{"id":78,"href":"/thanks-for-all-the-help406/","title":"Thanks for all the help","section":"Inductive Bias","content":" Thanks for all the help # This year was a blast: It started with the ever great FOSDEM in Brussels (see you there in 2013?), an invitation to GeeCon in Poznan (if you ever get an invitation to speak there - do accept, the organisers do an amazing job at that event). In summer we had Berlin Buzzwords in Berlin for the third time with 700 attendees (to retain the community feel to the conference we decided to limit tickets in 2013, so make sure you get your\u0026rsquo;s early). In autumn I changed my name and afterwards spent two amazing weeks in Sydney, only to attend Strata EU afterwards. Finally in December I was invited to go through the most amazing submissions for Hadoop Summit in Amsterdam 2013 (it was incredibly hard to pick and choose - thanks to Sean and Torsten for assisting me with that choice for the Hadoop Applied track.)\nI think I would have gone mad if it hadn\u0026rsquo;t been for all the help from friends and family: A big hug to my husband for keeping me sane when ever times got a bit rough in terms of stuff in my calendar. Thanks for all your support throughout the year. Another huge hug to my family - in particular to my mom who early 2012 volunteered to take care of most of the local organisation of our wedding (we got married close to where I grew up) and put in several surprises that she \u0026ldquo;kept mum\u0026rdquo; about up to the very last second. Also everyone who helped fill your wedding magazine with content (and train my ma in dealing with all sorts of document formats containing that content in her mail box - me personally I was forbidden to even just touch her machine during the first nine months of 2012 ;) ).\nAnother thanks to David Obermann for a series of interesting talks at this year\u0026rsquo;s Apache Hadoop Get Together Berlin. It\u0026rsquo;s amazing to see the event continue to grow even after essentially stepping down from being the main organiser.\nSpeaking of events: Another Thank You to Julia Gemählich, Claudia Brückner and the whole Berlin Buzzwords team. This was the first year I reduced the time I put into the event considerably - it was the first year I could attend the conference and not be all too tired to enjoy at least some of the presentations. You did a great job! Also thanks to my colleagues over at Nokia who provided me with a day a week to get things done for Buzzwords. In the same context: A very big thank you to every one who helped turn Berlin Buzzwords into a welcoming event for everyone: Thanks to all speakers, sponsors and attendees. Looking forward to seeing you again next year.\nFinally a really big Thanks to all the people who helped turn our wedding day and vacation afterwards into the great time it was: Thanks to our families, our friends (including but not limited to the best photographer and friend I\u0026rsquo;ve met so far, those who hosted us in Sydney, and the many people who provided us with information on where to go and what to do.)\nThere\u0026rsquo;s one thing though that bugged me by the end of this year: So I decided that my New Year\u0026rsquo;s resolution for 2013 would be to ramp up the time I spend on Apache one way or another: At least as committer for Apache Mahout, Mentor for Apache Drill and as a Member of the foundation.\nWishing all of you a Happy New Year - and looking forward to another successful Berlin Buzzwords in 2013.\n"},{"id":79,"href":"/recsys-stammtisch-berlin-december-2012331/","title":"RecSys Stammtisch Berlin - December 2012","section":"Inductive Bias","content":" RecSys Stammtisch Berlin - December 2012 # Earlier this month I attended the fourth Recommender Stammtisch in Berlin. The event was kindly hosted by Soundcloud - who on top of organising the speakers provided a really yummy buffet by Kochzeichen D.\nWith Paul Lamere the evening started with a very entertaining but also very packed talk on why music recommendation is special - or put more generally why all recommender systems are special:\nTraditionally recommender systems found their way into the wild to drive sales. In music however the main goal is to help users discover new content. Listeners are very different: Ranging from those indifferent to what is being played (imagine someone sitting in a coffee bar enjoying their espresso - it\u0026rsquo;s unlikely that those would want to influence the playlist of the shop\u0026rsquo;s entertainment system unless they are really annoyed with it\u0026rsquo;s content). There are casual listeners who from time to time skip a piece. There are more engaged people who train their own recommender through services like last.fm. Finally there are fanatics that are really into certain kinds of music. Building just one system to fit them all won\u0026rsquo;t do. Also relying on just one signal won\u0026rsquo;t do - instead you will have to deal with both, content signals like loudness plots as well as community signals.\nMusic applications tend to be highly interactive. So even if there is little to no reliable explicit feedback people tell you how much you like your music when skipping, turning pieces louder, interacting with the content behind the song being played.\nIn contrast to many other domains music deals with a vast item space and a huge long tail of songs that almost never get interacted with.\nIn contrast to shopping recommenders however in music making mistakes is comparably cheap: In most situations music isn\u0026rsquo;t purchased on a song by song basis but based on some subscription model. That way the actual cost of playing the wrong song is low. Also songs tend to be not much longer than 5min so also users are less annoyed when confronted with a slightly wrong piece of music.\nWhen implementing recommenders for a shopping site it is to be avoided to re-recommend stuff a user has purchased already. This is not the case in music recommendation. Quite the contrary: Re-recommending known music is one indicator for playlists people will like.\nWhen it comes to building playlists care must be taken to organise songs in a coherent way, mixing new and familiar songs in a pleasing order - essentially the goal should be to take the listener on a journey.\nFast updates are crucial: Music business itself is fast paced with new releases coming out regularly and being taken up very quickly by major broadcasting stations.\nMusic is highly contextual: It pays to know if the user is in the mood for calm or for faster music..\nThere are highly passionate users that are very easy to scare away - those tend to be the loudest ones influencing your user community most.\nThough meta data is key in music as well, never expect it to be correct. There are all sorts of weird band and song names that you never thought would be possible - an observation that also Ticketmaster made when building their ticket search engine.\nMusic is highly social and irrational - so just knowing your users friends and their tastes won\u0026rsquo;t get you to being perfect.\nOverall I guess the conclusion is that no matter which domain you deal with you will always need to know the exact properties of that domain to build a successful system.\nIn the second talk Brian McFee explained one way of modeling playlists with context. With that he concentrated on passive music discovery - that is based on one query return a list of music to listen to sequentially as opposed to active retrieval where users issue a query to search for a specific piece of music.\nHistorically it turned out to be difficult to come up with any playlist generator that is better than randomly selecting songs to play. His model is based on a random walk notian where the vertices are songs and edges represent learnt group similarities. Groups were represented by features like familiarity, social tags, specific audio features, metadata, release dates etc. Depending on the playlist category in most cases he was able to show that his model actually does perform better than random.\nIn the third talk Oscar Celma showed some techniques to also benefit from some of the more complicated signals for music recommendation. Essentially his take was that by relying on usage signals only you will be stuck with the head of the usage distribution only. What you want though is to be able to provide recommendations for the long tail as well.\nSome signals he mentioned included content based features (rythm, BPM, timbre, harmony), usage signals, social signals (beware of people trying to game the system or make fun of it though) and a mix of all those. His recommendation was to put content signals at the end of the processing pipeline for re-ranking and refining playlists.\nWhen providing recommendations it is essential to be able to answer why something was recommended. Even just in the space of novelty vs. relevancy to the user there are four possible strategies: a) recommend only old stuff that is marginally relevant to the specific user: This will end up pulling up mostly popular songs. b) recommend what is new but not relevant to the user: This will end up pulling out songs that turn your user away. c) recommend what is relevant to the user but old, this will mostly surface stuff the user knows already but is a safe bet to play. d) recommend what is both relevant and new to the user - here the real interesting work starts as this deals with recommending genuinely new songs to users.\nTo balance discovery with safe fallback go for skips, bans, likes and dislikes. Take into account the user context and attention.\nThe final point the speaker made was the need to take into account the whole picture: Your shiny new recommendation algorithm will just be a tiny piece in the puzzle. Much more work will need to go into data collection and ingestion, into API design.\nThe last talk finally went into some detail of the history of playlist creation - back from music creators\u0026rsquo; choices, via radio station mixes, mix tapes and finally ending up at spotify and fully automatic playlist creation.\nThere is a vast body of knowledge on how to create successful playlists e.g. among DJs that speak about warm-up phases, chillout times, alternating types of music in order to take the audience on a journey. Even just shuffling music the user already knows can be very powerful given the pool of songs the shuffle is based on neither too large (containing too broad types of music) nor too small (leading to frequent repetitions). According to Ben Fields the science and art of playlist generation and in particular evaluation is still pretty much in it\u0026rsquo;s infancy with much to come.\n"},{"id":80,"href":"/elastic-search-meetup-berlin167/","title":"Elastic Search meetup Berlin","section":"Inductive Bias","content":" Elastic Search meetup Berlin # Today Retresco hosted the (to my knowledge fourth) Elastic Search User Group Berlin - a group dedicated to using Lucene as part of Elastic Search. With roughly fifteen attendees the meetup attracted a decent crowd - most interestingly many of the people there were already using the software either in production or for closed beta projects.\nThe fist talk given was by people from ferret-go - a company doing media monitoring for brands focused on the German market. They are pretty new to the search topic, on top they aren\u0026rsquo;t fluent Java developers but do most stuff in Python. Essentially their whole application is built on top of Elastic Search - most features are implemented as more or less complex search queries. In recent weeks they had to deal with typical problems related to growing data set sizes, nodes getting hot in particular when load balancing isn\u0026rsquo;t configured quite right and balancing shards on a per index level instead of doing it globally for all shards (particularly bad in their case as they added an index with smaller shards and one with significantly larger shards into ES).\nThe second talk gave a really nice overview on things to keep in mind before putting ES to production - as is usually the case, the default configuration makes it easy to get started but most likely is not what you want in your production environment.\nReally nice, technically focused event. Thanks to Retresco for hosting the meetup including beer and Club Mate and to ElasticSearch.com for paying for the pizza. Check their meetup page for the next event - most likely to be scheduled in January.\nAlso if you\u0026rsquo;d like to learn more on Lucene 4 (talk by Simon Willnauer) - make sure to attend the Apache Hadoop Get Together December 2012 (if you need another ticket, it might make sense to politely ask the organiser, make sure you are registered, otherwise most likely you won\u0026rsquo;t get through security). Other two talks scheduled: Pere Urbon Bayes on NoSQL and Graph, a love story! as well as myself on How to Fail Your Big Data Project Quick and Rapidly.\n"},{"id":81,"href":"/apacheconeu-part-11-last-part97/","title":"ApacheConEU - part 11 (last part)","section":"Inductive Bias","content":" ApacheConEU - part 11 (last part) # One of the last sessions covered logging frameworks for Java. Christian Grobmeier started by detailing the common requirements for all logging frameworks:\nSpeed - developers do not want to pay a disproportional penalty for using a logging framework.\nFail-safety and reliability - under no circumstances should your logging framework kill your application. In addition it would be most annoying to find that one log message that would help you de-cypher the problem your application ran into missing. As obvious as those requirements sound - there are counter examples to both: There is a memory leak in log4j1, when reconfiguring logback on the fly it may well lose messages.\nLog frameworks should be backwards compatible: Both, changing the API as well as incompatible configuration file formats aren’t particularly great when wanting to upgrade the logging framework version you use.\nOn top of all the way you do logging ultimately is a matter of taste - by now there are several implementations even just for Java online catering needs ranging from pure simplicity to huge flexibility: log4j, logback, java.util.logging, AVSL, tinyLog. On top of that there are aggregators like SLF4j and commons logging - even though the speaker himself does contribute to the latter framework, at the current time he still recommends using SLF4J as it is actively being developed, more modern and better supported.\nBiggest news shared in the talk was the release of log4j2 which comes with commons-logging and slf4j integration. This version finally gets rid of the if (debug.enabled()) { log.debug(\u0026hellip;)} idiom by introducing place holders and variable length argument lists essentially enabling format strings for logging. Markers can be added to the logging code to allow for later filtering. Writing plugins has been made a whole lot simpler. There is support for an easier to read xml configuration format as well as json configurations. In additions configurations can be set to be reloaded on change in a pre-defined interval. It is slightly slower than logback and log4j on average, though we are still talking about a few milliseconds for a large amount of log messages. Unfortunately those averages did not come with error bars which would have made interpreting them in comparison a bit easier.\nHowever log4j is not just about logging. It does have sub projects for viewing logs of httpd, log4j and others (called chainsaw), logging for php and .net.\nLog4j has a rather complex history: As soon as the leader of the project left, activity died away. By now activity has taken up again quite a bit with 4 contributors. They created 6 releases in the last year alone, on top of 600 mailing list messages and a huge amount of commits. Still also log4j is hiring - if you want to work for free on a fun project that affects nearly every Java developer world wide, work together with awesome coders this project is seeking new contributors.\nIf you consider logging boring just be reminded that logs are a valuable source of user activity leading to features like being able to recommend new products to customers, localise content offerings or even just adjusting the default settings of your web page to increase click through. Related to log4j there’s Apache Flume dealing with distributed logging, there’s challenges in the cloud and mobile space, Apache Mayhem for Logger ingestions.\nThe last technical session dealt with Apache Buildr - a build system for Java, though not only Java, written in Ruby. The advantage being that it delivers the artifact resolution and download from maven archives through ivy plugins, provides greater flexibility through ruby integration and can fall back to ant tasks if needed.\nThe final session was the closing plenary given by Nick Burch. Most noteably he invited attendees for ApacheCon2013 Europe. Looking forward to meeting all of you there. The community edition of ApacheCon was an awesome setup for people to meet and not only pitch their projects but to also provide deep technical detail and show off more of what the Apache community is all about. Looking forward to the audio recordings as well as to the videos taken during the conference. CU all next year!\n"},{"id":82,"href":"/apacheconeu-part-1096/","title":"ApacheConEU - part 10","section":"Inductive Bias","content":" ApacheConEU - part 10 # In the next session Jukka introduced Tika - a toolkit for parsing content from files including a heuristics based component for guessing the file type: Based on file extension, magic and certain patterns in the file the file type can be guessed rather reliably. Some anecdotes:\nnot all mime types are registered with IANA, there are of course conflicting file extensions,\nMicrosoft Word not only localises their interface but also the magic in the file,\nhtml detection is particularly hard as there is quite some overlap with other file formats (e.g. there are such things as html mails\u0026hellip;)\nxhtml parsing is quite reliable by using an actual xml parser for the first few bytes to identify the namespace of the document\nidentifying odf documents is easy - though zipped the magic is preserved uncompressed at a pre-defined location in the file\nfor ooxml the file has to be unpacked to identify it\nplain text content is hardest - there are a few heuristics based on UTF BOMs, ASCII stats, line ending stats, byte histograms, still it\u0026rsquo;s not fool proof.\nIn addition Tika can extract metadata: For text that can be as easy as encoding, length, content type and comments. For images that is extended by image size and potentially EXIF data. For pdf data it gets even more comprehensive including information on the file creator, save date and more (same applies for MS office documents). Most document metadata is based on the doublin core standard, for images there’s EXIF and IPCT - soon there’ll also be xmb related data that can be discovered.\n"},{"id":83,"href":"/apachecon-eu-part-0987/","title":"ApacheCon EU - part 09","section":"Inductive Bias","content":" ApacheCon EU - part 09 # In the Solr track Elastic Search and Solr Cloud went into competition. The comparison itself was slightly apples-and-oranges like as the speaker compared the current ES version based on Lucene 3.x and Solr Cloud based on Lucene 4.0. During the comparison it still turned out that both solutions are more or less comparable - so choice again depends on your application. However I did like the conclusion: The speaker did not pick a clear winner in terms of projects. However he did have another clear winner: The user community will benefit from there being two projects as this kind of felt competition did speed up development already considerably.\nThe day finished with hoss\u0026rsquo; Stump the Chump session: The audience was asked to submit questions before the session, the jury was than asked to pick the winning question that stumped Hoss the most.\nSome interesting bits from that question: One guy had the problem of having to provide somewhat diverse results in terms e.g. manufacturers in his online shop. There are a few tricks to deal with this problem: a) clean your data - don\u0026rsquo;t have items that use keyword spamming side by side with regular entries. Assuming this is done you could b) use grouping to collapse items from the same manufacturer and let the user drill deeper from there. Also using c) a secondary sorting value can help - one hint: Solr ships with a random value out of the box for such cases. For me the last day started with hossman\u0026rsquo;s session on boosting and scoring tricks with Solr - including a cute reference for explaining TF-IDF ranking to people (see also a message tweeted earlier for an explanation of what a picture taken during my wedding has to do with ranking documents):\nThough TF-IDF is standard IR scoring it probably is not enough for your application. There\u0026rsquo;s a lot of domain knowledge that you can encode in your ranking:\nnovelty factors - number of ratings/ standard deviation of ratings - ranks controversial items on top that might be more interesting than just having the stuff that everyone loves\nscarcity - people like buying what is nearly sold out\nprofit margin\ncreate your score manually by an external factor - e.g. popularity by association like categories that are more popular than others or items that are more popular depending on the time of day or year\nThere are a few sledge hammers people usually think of that can turn against you really badly: Say you rank by novelty only - that is you sort by date. The counter example given was the case of the AOL-Time Warner merger - being a big story news papers would post essays on it, do evaluations etc. However also articles only remotely related to it would mention the case. So be the end of the week when searching for it you would find all those little only remotely relevant articles and have to search through all of them to find the really big and important essay.\nThere are cases where it seems like only recency is all that matters: Filter only for the most recent items and re-try only in case of no results. The counter example is the case where products are released just on a yearly basis but you set the filter to say a month. This way up until May 31 your users will run into the retry branch and get a whole lot of results. However when a new product comes out on June first from that day onward the old results won\u0026rsquo;t be reachable anymore - leading to a very weird experience for those of your users who saw yesterday\u0026rsquo;s results.\nThere were times when scoring was influenced by keyword stuffing to emulate higher scores - don\u0026rsquo;t do that anymore, Solr does support sophisticated per field and document boosting that make such hacks superfluous.\nInstead rather use edismax for weighting fields. Some hints on that one: configure omitNorms in order to avoid having keyword stuffing influence your ranking. Configure omitTermFrequencyAndPosition if the term frequency in any document does not really tell you much e.g. in case of small documents only.\nWith current versions of Solr you can use your custom scoring per field. In addition a few ones are shipped that come with options for tweaking - like for instance the sweetSpotSimilarity wher you can tell the scorer that up to a certain length no length penalisation should happen.\nCreate your own boost functions that in addition to TF-IDF rely on rating values, click rates, prices or category influences. There\u0026rsquo;s even an external file field option to allow you to load your scoring value per document or category from an external file that can be updated on a much more frequent basis than you would otherwise want to re-index all documents in your solr. For those suffering from the \u0026ldquo;For business reasons this document must come first no matter what the algorithm says\u0026rdquo; syndrom - there\u0026rsquo;s a query elevation component for very fine grained tuning of rankings per query. Keep in mind so that this can easily turn into a maintanance nightmare. However it can be handy when quickly fixing a highly valuable business based use case: With that component it is possible to explicitly exclude documents from matching and precisely setting where to rank individual documents.\nWhen it comes to user analytics and personalisation many people think of highly sophisticated algorithms that need lots of data to be trained. Yes Mahout can help you with personalisation and recommendation - but there are a few low hanging fruits to grab before:\nUse the history of registered users or those you can identify through cookies - track the keywords they are looking for, the sort and filter functions commonly used.\nBucket people by explicit or implicit demographics.\nEven just grouping people by the os and browser they use can help to identify preferences.\nAll of this information is relatively cheap to get by and can be used in many creative ways:\nProvide default sort and filter functions for returning users.\nFilter on the current query but when scoring take the older query into account.\nBased on category facet used before do boosting in the next search assuming that the two queries are related.\nEssentially the goal is to identify three factors for your users: What is their preference, what is the differentiator and what is your confidence in your estimation.\nAnother option could be to use the SweetSpotPlateau: If someone clicked on a price range facet on the next related query do not hide other prices but boost those that are in the previous facet.\nOne side effect to keep in mind: Your cache hit rate will go down now you are tailoring your results to individual users or user groups.\nBiggest news for Lucene and Solr was the release of Lucene 4 - find more details online in an article published recently.\n"},{"id":84,"href":"/apacheconeu-part-0895/","title":"ApacheConEU - part 08","section":"Inductive Bias","content":" ApacheConEU - part 08 # Jan Lehnardt\u0026rsquo;s talk covered the history of CouchDB - including lessons learnt along the way. The first issue he went into: Shipping 1.0 is hard! They spent a lot of effort and time in order to have a stable database that won\u0026rsquo;t loose your data - only to have a poorly patch slip in for 1.0 that resulted in data loss. The fury of action happening afterwards was truely amazing - people working on rolling shifts all over the planet to not only fix the issue but also provide recovery tooling for those affected by the bug. The lessons learnt form that are as obvious as they are often neglected: Both test coverage as well as code review are crucial for any software project.\nThe second topic Jan went into was the disctraction and tension that comes from having a company built around your favourite open source project. When going down this road keep in mind that the whole VC setup usually is very time consuming - the world starts revolving around the need to either gather more VC funding or make up a successful business case to support your company. All of this results in less time spent coding, friction around the fact that the corporate interests may not always be what is best for your open source project. In CouchDB the result was the explosion of the project founder who eventually left the project. This hit CouchDB particularly badly as the project essentially was built around the idea of the one brilliant coder, relied on his information channels for marketing. The lesson learnt was that having communications centralised that way can easily turn against you - don\u0026rsquo;t trust your benevolent dictator.\nUsually it is quite ok for users to move on - in particular if the project does no longer fit their needs. However having multiple key people leave at the same time can be detrimental, in particular if they are the vocal ones. In terms of lessons learnt: Embrace the fact that people will fail your software. Use the resulting knowledge about your application boundaries - or fix what failed them.\nIn terms of general advise: The world moved on after any of these cases. What does help is to ship what users need instead of running after the next big hype. Also good ideas will stick - using json as format and js for query formulation did make it into many other applications with the former also making it into the next SQL standard to be released in 2015. The goal should be to build stuff that is easy (and fun) to use.\nIn the mean time CouchDB grew up. Not only does it have another release and a new web site. It has turned into a project that is no longer a thing pushed forward by a single person but that moves on its own. The secret behind that development is to acknowledge that having just few people in the leading position will burn them out - make sure to enable others and that your strong leaders to get to lead. Oh and as any Apache project also CouchDB is happy about any new contributor joining the project.\nWhen it comes to communication the Apache incubation process made sure to burn the \u0026ldquo;everything happens on the mailing list\u0026rdquo; mantra into their mind. Still IRC was a valuable way of communication for non-decision stuff like user support and community building. IRC is fun - in particular when you can train irc bots based on earlier communication to automatically answer incoming user questions.\nAnother option CouchDB used to fix the community issues was to meet with people face-2-face - for three days in Boston, later in Dublin, later in Vienna. In addition they added a roadmap for the next 2 to 3 years including points like:\nfaster releases - they switched to time based instead of feature based releases except for security patches\nthey are the first to use git@apache to make branching and merging easier\nthey are github lovers with pull requests ending up on their dev list\nthey enabled a Erlang beginners question list in order to be able to recruit new contributors in a world of lacking Erlang developers. A very specific result of that was that people are much more comfortable even asking simple question - and on a more practical note one question for the birds eye view of couchdb resulted in Jan spending an hour and a half drawing up that particular picture: Spending an hour on docs to get to really new people is time well spend.\nIn terms of PMC chair lessons learnt: The goal should be to get the right people to care about the right thing. Having people finish stuff helps - and is infectious.\nIn the end as an open source project your biggest asset is your community. Motivating more people to join is key. If for your target audience JIRA is one step too much talk to infra to figure out how to make things better (and help them with the solutions).\nWhat is fascinating about CouchDB is the whole ecosystem around the project. CouchDB is not just a database project hosted at Apache. It comes with a really well working replication API. There are implementations in js running in Browsers, there\u0026rsquo;s BigCouch (dynamo in Erlang on top of CouchDB), there is an iOS app, there is PouchDB (the couch for your pocket), TouchDB (iOS and android implementations on top of sqlLight). The fun part to watch is that the idea is bigger than the project at Apache. The bigger the ecosystem the better for the community - there\u0026rsquo;s no need to fold everything into the original project.\nAnd of course also CouchDB is hiring.\n"},{"id":85,"href":"/apacheconeu-part-0794/","title":"ApacheConEU - part 07","section":"Inductive Bias","content":" ApacheConEU - part 07 # Julien Nioche shared some details on the nutch crawler. Being the mother of all Hadoop projects (as in Hadoop was born out of developments inside of nutch) the project has become rather quite with a steady stream of development in the recent past. Julien himself uses the nutch for gathering crawled data for several customer projects - feeding this data into an NLP pipeline based on Behemoth that glues Mahout, UIMA and Gate together.\nThe basic crawling steps including building the web graph, computing a link based ranking method and indexing are still the same since last I looked at the project - just that for indexing the project now uses solr instead of their own lucene based solution.\nThe main advantage of nutch is its pluggability: the protocol parser, html filter, url filter, url normaliser all can be exchanged against your own implementations.\nIn their 2.0 version they moved away from using their own plain hdfs storage to a table schema - mapped to the real database through Gora, an abstraction layer to connect to e.g. Cassandra or HBase. The schema itself is based on Avro but can be adopted to your needs. The advantages are obvious: Though still distributed this approach is much easier and simpler in terms of logic kept in nutch itself. Also it is easier to connect to the data for third parties - all you need is the schema as well as Gora. The current disadvantage lies in it\u0026rsquo;s configuration overhead and instability compared to the old solution. Most likely at least the latter one will go away as version 2.0 stabelises.\nIn terms of future work the project focuses on stabilisation, synchronising features of version 1.x and 2.x (link ranking is only available in version 1.x while support for elastic search is only available in version 2.x). In terms of functionality the goal is to move to Solr Cloud, support sitemaps (as implemented by commons crawler), more (pluggable?) indexers.\nThe goal is to delegate implementations - it was already done for Tika and Solr. Most likely it will also happen for the fetcher, protocol handling, robots.txt handling, url normalisation and filtering, graph processing code and others.\nThe next talk in the Solr/Lucene talk dealt with scaling Solr to big data. The goal of the speaker was to index 100 million documents - the number of documents was expected to grow in the future. Being under heavy time pressure and having a bash wizard on the project they started building lots of their glue code and components in bash scripts: There were scripts for starting/stopping services, remote indexing, performance monitoring, content extraction, ingestion and deployment. Short term this was a very good idea - it allowed for fast iterations and learning. On the long run they slowly replaced their custom software with standard components (tika for content extraction, puppet for deployment etc.).\nThey quickly learnt to value property files in order to easily reconfigure the system even in production (relying heavily on bash xml was of course not an option). One problem this came in handy with was adjusting the sharding configuration - going from a simple random sharding to old vs new to monthly they could optimise the configuration to their load. What worked well for them was to separate JVM startup from Solr core startup - they would start with empty solrs and only point them to the data directories aafter verifying that the JVMs booted up correctly.\nIn terms of performance they learnt to go wide quickly - instead of spending hours on optimising their one huge box they ended up distributing their solrs to multiple separate machines.\nIn terms of ingestion pipelines: Theirs is currently based on an indexing directory convention, moving the indexing as soon as all data is ingested. The advantage here is the atomicity of mv that can be used - disadvantage is having to move around lots of data. Their goal is to go for hdfs for indexing soon and zookeeper for configuration handling.\nIn terms of testing: In contrast to having a dev-test-production environment their goal is to have an elastic compute cloud that can be adjusted accordingly. Though EC2 itstelf is very cost intensive, poses additional problems with firewalling and moving data their cloud computing could still be a solution - in particular given projects like cloud stack or open cloud. The goal would be to do cycle scavaging in their own datacenter, do heavy computations when there is not a lot of load on the system and turn those analysis of in case of incoming traffic.\nWhen it comes to testing and monitoring changes they made good experiences with using JConsole (connecting it to several solrs at once through a simple ip discovery script) and solr meter as a performance debugging tool.\nSome implementation details: They used Solr as some sort of NoSQL cache as well (for thousands of queries/s), push the schema definition to solr from the app, have common fields and the option for developers to add custom fields in the app. Their experience is to not do expensive stuff in solr but to move that outside - this applies in particular to content extraction. For storage they used an avro based binary file format (mainly in order to save storage space, have a versioned schema and fast compression and de-compression). They are starting to use tika as their pipeline and for auto content detection, scaling up with behemoth. They learnt to upgrade indexes without re-indexing using the lucene upgrade tooling. In addition they use Grimreaper to kill servers if anything goes wrong and restart it later.\n"},{"id":86,"href":"/apacheconeu-part0693/","title":"ApacheConEU - part 06","section":"Inductive Bias","content":" ApacheConEU - part 06 # For the next session I joined the Tomcat crowd in Marc Thomas\u0026rsquo; to learn more on Tomcat reverse proxy configurations. One rather common setup is to have Tomcat connected to an httpd instance. One common issue encountered with this setup in particular when running httpd with the event mpm is the problem of thread exhaustion on tomcat\u0026rsquo;s side. Fixes include always having more active tomcat threads than there can be httpd threads at any one time and to disable persistent connections. Keep in mind that tomcat performance does not degrade gracefully here - in case of thread exhaustion it just goes downhill very quickly. One famous example here was an issue with the ASF jira: After weeks of debugging bad performance, after blaming the hardware, the os, the JVM, the java implementation in generally finally the number of threads was increased resulting in a smoothly running system\u0026hellip;\nAnother common configuration problem is to rename to deployed web application war - for instance in order to keep the version number in the war name itself - and change the path on httpd\u0026rsquo;s side. This is bad for at least for reasons:\nredirects will fail - you can configure ProxyReversePath which will fix some issues but does not affect all http headers\ncookie paths break - you can configure CookiePathReverse here\nlinks that are generated in the web app will fail - you can use mod_sed/ _substitute/ _proxy_html to fix that - however those configurations tend to become messy and are error prone\ncustom headers usually also break\nIf the only reason for doing such a thing is to keep the version number in the file name it might be an option to use \u0026ldquo;foo##bar.1.2.3\u0026rdquo; as filename - tomcat will ignore anything after the hashtags.\nWhen dealing with proxying traffic make sure to inform tomcat about https termination events in order to correctly handle secure cookies and sessions. This is done automatically with mode_jk and mod_ajp, mod_proxy needs some more manual work. When dealing with virtual hosting make sure to use ProxyPreserveHeader in order to be able to switch hosts on Tomcat\u0026rsquo;s side.\nJulien Nioche shared some details on the nutch crawler. Being the mother of all Hadoop projects (as in Hadoop was born out of developments inside of nutch) the project has become rather quite with a steady stream of development in the recent past. Julien himself uses the nutch for gathering crawled data for several customer projects - feeding this data into an NLP pipeline based on Behemoth that glues Mahout, UIMA and Gate together.\nThe basic crawling steps including building the web graph, computing a link based ranking method and indexing are still the same since last I looked at the project - just that for indexing the project now uses solr instead of their own lucene based solution.\nThe main advantage of nutch is its pluggability: the protocol parser, html filter, url filter, url normaliser all can be exchanged against your own implementations.\nIn their 2.0 version they moved away from using their own plain hdfs storage to a table schema\nmapped to the real database through Gora, an abstraction layer to connect to e.g. Cassandra or HBase. The schema itself is based on Avro but can be adopted to your needs. The advantages are obvious: Though still distributed this approach is much easier and simpler in terms of logic kept in nutch itself. Also it is easier to connect to the data for third parties - all you need is the schema as well as Gora. The current disadvantage lies in it\u0026rsquo;s configuration overhead and instability compared to the old solution. Most likely at least the latter one will go away as version 2.0 stableises.\nIn terms of future work the project focuses on stabilisation, synchronising features of version 1.x and 2.x (link ranking is only available in version 1.x while support for elastic search is only available in version 2.x). In terms of functionality the goal is to move to Solr Cloud, support sitemaps (as implemented by commons crawler), more (pluggable?) indexers.\nThe goal is to delegate implementations - it was already done for Tika and Solr. Most likely it will also happen for the fetcher, protocol handling, robots.txt handling, url normalisation and filtering, graph processing code and others.\nThe next talk in the Solr/Lucene talk dealt with scaling Solr to big data. The goal of the speaker was to index 100 million documents - the number of documents was expected to grow in the future. Being under heavy time pressure and having a bash wizard on the project they started building lots of their glue code and components in bash scripts: There were scripts for starting/stopping services, remote indexing, performance monitoring, content extraction, ingestion and deployment. Short term this was a very good idea - it allowed for fast iterations and learning. On the long run they slowly replaced their custom software with standard components (tika for content extraction, puppet for deployment etc.).\nThey quickly learnt to value property files in order to easily reconfigure the system even in production (relying heavily on bash xml was of course not an option). One problem this came in handy with was adjusting the sharding configuration - going from a simple random sharding to old vs new to monthly they could optimise the configuration to their load. What worked well for them was to separate JVM startup from Solr core startup - they would start with empty solrs "},{"id":87,"href":"/apacheconeu-part-592/","title":"ApacheConEU - part 05","section":"Inductive Bias","content":" ApacheConEU - part 05 # The afternoon featured several talks on HBase - both it\u0026rsquo;s implementation as well as schema optimisation. One major issue in schema design in the choice of key. Simplest recommendation is to make sure that keys are designed such that on reading data load will be evenly distributed accross all nodes to prevent region-server hot-spotting. General advise here are hashing or reversing urls.\nWhen it comes to running your own HBase cluster make sure you know what is going on in the cluster at any point in time:\nHbase comes with tools for checking and fixing tables,\ntools for inspecting hfiles that HBase stores data in,\ncommands for inspecting the binary write ahead log,\nweb interfaces for master and region servers,\noffline meta data repair tooling.\nWhen it comes to system monitoring make sure to track cluster behaviour over time e.g. by using Ganglia or OpenTSDB and configure your alerts accordingly.\nOne tip for high traffic sites - it might make sense to disable automatic splitting to avoid splits during peaks and rather postpone them to low traffic times. One rather new project presented to monitor region sizes was Hannibal.\nAt the end of his talk the speaker went into some more detail on problems encountered when rolling out HBase and lessons learnt:\nthe topic itself was new so both engineering and ops were learning.\nat scale nothing that was tested on small scale works as advertised.\nhardware issues will occur, tuning the configuration to your workload is crucial.\nthey used early versions - inevitably leading to stability issues.\nit\u0026rsquo;s crucial to know that something bad is building up before all systems start catching fire - monitoring and alerting the right thing is important. With Hadoop there are multiple levels to monitor: the hardware, os, jvm, Hadoop itself, HBase on top. It\u0026rsquo;s important to correlate the metrics.\nkey- and schema design are key.\nmonitoring and knowledgable operations are important.\nthere are no emergency actions - in case of an emergency it just will take time to recover: Even if there is a backup, even just transferring the data back can take hours and days.\nHBase (and Hadoop) is DevOps technology.\nthere is a huge learning curve to get to a state that makes using these systems easy.\nIn his talk on HBase schema design Lars George started out with an introduction to the HBase architecture. On the logical level it\u0026rsquo;s best to think of HBase as a large distributed hash table - all data except for names are stored as byte arrays (with helpers to transform that data back into well known data types provided). The tables themselves are stored in a sparse format which means that null values essentially come for free.\nOn a more technical level the project uses zookeeper for master election, split tracking and state tracking. The architecture itself is based on log structured merge trees. All data initially ends up in the write ahead log - with data always being appended to the end this log can be written and ready very efficiently without any seek penalty. The data is inserted at the respected region server in memory (mem store, size of 64 MB typically) and synched to disk in regular intervals. In HBase all files are immutable - modifications are done only by writing new data and merging it in later. Deletes also happen by marking data as deleted instead of really deleting it. On a minor compaction the recently few files are being merged. On a major compaction all files are merged and deletes are being handled. Handling your own major compaction is possible as well. In terms of performance lookup by key is the best you can do. If you do lookup by value this will result in a full-table scan. There is an option to give HBase a hint as to where to find the key when it is updated only infrequently - there is an option to provide a timestamp of roughly where to look for it. Also there are options to use Bloomfilters for better read performance. Another option is to move more data into the row key itself if that is the data you will be searching for very often. Make sure to de-normalize your data as HBase does not do any sophisticated joins, there will be duplication in your data as all should be pre-joined to get better performance. Have intelligent keys that make match your read/write patterns. Also make sure to keep your keys reasonably short as they are being repeated for each entry - so moving the whole data into the key isn\u0026rsquo;t going to get you anything.\nSpeaking of read write patterns - as a general rule of thumb: to optimise for better write performance tune the memstore size. For better read performance tune the block cache size. One final hint: Anything below 8 servers really is just a test setup as it doesn\u0026rsquo;t get you any real advantages.\n"},{"id":88,"href":"/apacheconeu-part-0491/","title":"ApacheConEU - part 04","section":"Inductive Bias","content":" ApacheConEU - part 04 # The second talk I went to was the one on the dev@hadoop.a.o insights given by Steve Loughran. According to Steve Hadoop has turned into what he calls an operating system for the data center - similar to Linux in that it\u0026rsquo;s development is not driven by a vendor but by its users: Even though Hortenworks, Cloudera and MapR each have full time people working on Hadoop (and related projects), this work usually is driven by customer requirements which ultimately means that someone is running a Hadoop cluster that he has trouble with and wants to have fixed. In that sense the community at large has benefitted a lot from the financial crisis that Y! has slipped into: Most of the Hadoop knowledge that is now spread across companies like Linked.In, Facebook and others comes from engineers leaving Y! and joining one of those companies. With that also the development cycle of Hadoop has changed: While it was mostly driven by Y! schedule in the beginning - crunching out new releases nearly on a monthly basis with a dip around Christmas time it\u0026rsquo;s got more irregular later - to pick up a more regular schedule just recently.\nImage taken a few talks earlier in the same session.\nIn terms of version numbers: 1.x is to be considered stable, production ready with fixes and low risk patches applied only. For 2.x the picture is a bit different - currently that is in alpha stage, features and fixes go there first, new code is developed there first. However Hadoop is not just http://hadoop.apache.org - the ecosystem is much larger including projects like Mahout, Hama, HBase and Accumulo built on top of Hadoop and Hive, Pig, Sqoop and Flume for integraion and analysis, as well as oozie for coordination an zookeeper for distributed configuration. There\u0026rsquo;s even more in incubation (or just recently graduated): Kafka for logging, whirr for cloud deployment, giraph for graph processing, ambari for cluster management, s4 for distributed stream processing, hcatalog and templeton for schema management, chuckwa for loggin purposes. All of the latter ones love helping hands. If you want to help out and want to \u0026ldquo;play Apache\u0026rdquo; those are the place to go to.\nWith such a large ecosystem one of the major pain points when rolling out your own Hadoop cluster is integrating all components you need: All projects release on separate release schedules, usually documenting against which version of Hadoop they were built. However finding a working combination is not always trivial. The first place to go to that springs to mind are the standard Linux distributions - however for their release and support cycles (e.g. Debian\u0026rsquo;s guarantees for stable releases) the pace inside of Hadoop and the speed with which old versions were declared \u0026ldquo;no longer supported\u0026rdquo; still is too fast. So what alternatives are there? You can go with Cloudera who ship Apache Hadoop extended with their patches and additional proprietary software. You can opt for Hortenworks that ships the ASF Hadoop only. Or you can opt for Apache itself and either tame the zoo yourself or rely on BigTop that aims to integrate the latest versions.\nEven though there are people working fulltime on the project there\u0026rsquo;s still way more work to do than hands to help out. Some issues do fall through the cracks, in particular if you are the only person affected by the issue. Ultimately this may mean that if the bug only affects you you will have to be the one to fix the issue. Before going out and hacking the source itself it may make sense to go out and search for the stack trace you are looking at, the error message in front of you - look in JIRA and on the mailing list archives to see if there\u0026rsquo;s someone else who has solved the problem already - and in an ideal case even provided a patch that just didn\u0026rsquo;t make it into the distribution so far.\nAlso contributing to Hadoop is a great way to get to work on CS hard problems: There\u0026rsquo;s distributed computing involved (clearly), there\u0026rsquo;s consensus implementations like Paxos, there\u0026rsquo;s work to optimise Hadoop for specific CPU architectures, there\u0026rsquo;s scheduling and data placement problems to solve, machine learning and graph algorithm implementations and more. If you are into these topics - come and join, those are highly needed skills. People on the list tend to be friendly - they are \u0026ldquo;just\u0026rdquo; overloaded.\nOf course there are barriers to entry - just like the Linux kernel Hadoop has become business critical for many of its users. Steve\u0026rsquo;s recommendation for dealing with this circumstance was to not compete with existing work that is being done - instead either concentrate on areas that aren\u0026rsquo;t covered yet or even better yet collaborate with others. A single lonely developer just cannot reasonably compete.\nIn terms of commit process Hadoop is running a review-than-commit protocol. Also it makes things seemingly more secure it also means that the development process is a lot slower, that things get neglected, that there is a lot of frustration also on the committers\u0026rsquo; side when having to update a patch oftentimes. In addition tests running for a long time doesn\u0026rsquo;t make contributing substancially easier. With lots of valuable and critical data being stored in HDFS makeing radical changes there also isn\u0026rsquo;t something that happens easily. The best way to really get started is to get trusted and have a track record: The maintanance cost for abandoned patches that are large, hard to grasp and no longer supported by the original author is just to high.\nGet a track record by getting known on dev@ and meetups. Show your knowledge by helping out others, help with patches that are not your own, don\u0026rsquo;t rewrite core initially, help with building plug-in points (like for shuffling implementations), help with testing across the whole configuration space, testing in unusual settings. Delegate the test at scale to those that have the huge clusters - there will always be issues revealed in their setup that do not cause any problems on a single machine or in a tiny cluster. Also make sure to download the package in the beta phase and test that it works in your problem space. Another way to get involved is by writing better documentation - either online or as a book. Share your experience. One major challenge is work on major refactorings - there is the option to branch out, switch to a commit-than-review model but that only post-pones work to merge time. For independent works it\u0026rsquo;s even more complicated to get the changes in. Also integrating post graduate work that can be very valuable isn\u0026rsquo;t particularly simple - especially if there already is a lack in helping hand s- who\u0026rsquo;s going to spend the work to mentor those students.\nSome ideas to improve the situation would b the help with spreading the knowledge - in local university partnerships, google hangouts, local dev workshops, using git and gerrit for better distributed development and merging.\n"},{"id":89,"href":"/apacheconeu-part-0390/","title":"ApacheConEU - part 03","section":"Inductive Bias","content":" ApacheConEU - part 03 # Tuesday started early with a plenary - run by the sponsor, not too many news there, except for the very last slide that raised a question that is being discussed often also within the ASF - namely how to define oneself compared to non-ASF projects. What is the real benefit for our users - and what is the benefit for people to go with the ASF. The speaker concentrated on pointing out the difference to github. Yes tooling changes are turning into a real game changer - but that is nothing that the foundation could not adopt over time. What I personally find interesting is not so much what makes us different from others but more what can be learnt from other projects - not only on github but also in a broader scope from the KDE, Python, Gnome, Debian, Open/Libre-Office communities, from people working on smaller but non-the-less successful projects as well as the larger foundations, maybe even the corporate driven projects. Over time lots of wisdom has accumulated within and outside of the foundation on how to run successful open source projects - now the question is how to transfer that knowledge to relatively young projects and whether that can help given the huge amount of commercial interest in open source - not only in using it but also in driving individual projects including all the benefits (more people) and friction around it.\nThe first talk I went to was an introduction to was Rainer Jung’s presentation on the new Apache httpd release. Most remarkably the event mpm available as “experimental” feature is now marked as default for the Apache distribution - though it is not being used for ssl connections. In addition there is support for async write completion, better support for sizing and monitoring. In particular when sizing the event mpm the new scoreboard comes in handy. When using it, keep in mind to adjust the number of allowed open file handles as well.\nIn order to better support server-to-client communication there is html5 web socket standardisation on it’s way. If you are interested in that check out the hybi standardisation list. Also taking a look at the Google SPDY could be interesting. Since 2.4 dynamic loadable modules are supported and easy to switch. When it comes to logging there is now support for sub second timestamp precision, per module log levels. Process and thread ids are kept in order to be able to untwist concurrent connection handling. There are unique message tokens in the error log to track requests. Also the error log format is configurable\nincluding trace levels one to eight, configurable per directory, location and module.\nThey’ve added lots of trace messages to core, correlation ids between error and access log entries (format entry %L). In addition there is a mod_log_debug module to help you log exactly what you want when you want. Speaking of modules - in order to upgrade them from 2.2 to 2.4 it’s in general sufficient to re-compile. With the new version though not all modules are going to be loaded as default anymore. New features include dynamic configurations based on mod_lua. AAA was changed again, there are filters to rewrite content before it’s sent out to clients (mod_substitute, mode_sed, mod_proxy_html). mod_remoteip helps to keep the original ip in your logs instead of the procy ip.\nWhen it comes to documentation better check the English documentation - or better yet provide patches to it. mod_rewrite and mod_proxy improved a lot. In addition the project itself now has a new service for it’s users: via comments.apache.org you can send documentation comments to the project without the need to register for a bugzilla account and provide documentation patches. In addition there is now syntax highlighting in the documentation. One final hint: the project is very open and actively looking for new contributors - though they may be slow to respond on the user and dev list - they definitely are not unfriendly ;)\n"},{"id":90,"href":"/apachecon-eu-part-0286/","title":"ApacheCon EU - part 02","section":"Inductive Bias","content":" ApacheCon EU - part 02 # For me the week started with the Monday Hackathon. Even though I was there early the room quickly filled up and was packed at lunch time. I really liked the idea of having people interested in a topic register in advance - it gave the organisers a chance to assign tables to topics and put signs on the tables to advertise the topic worked on. I\u0026rsquo;m not too new to the community anymore and can relate several faces to names of people I know are working on projects I\u0026rsquo;m interested in - however I would hope that this little bit of extra transperancy made it easier for newcomers to figure out who is working on what. Originally I wanted to spend the day continuing to work on an example showing what sort of pre-processing is involved in order to get from raw html files to a prediction of which Berlin Buzzwords submission is going to be accepted. (Un-?)fortunately I quickly got distracted and drawn into discussions on what kind of hardware works best for running an Apache Hadoop cluster, how the whole Hadoop community works and where the problem areas are (e.g. constantly missing more helping hands to get all things on the todo list done).\nThe evening featured a really neat event: Committers and Hackathon participants were invited to the committer reception in the Sinsheim technical and traffic museum. One interesting observation: There\u0026rsquo;s an easy way to stop geeks from rushing over to the beer, drinks and food: Just put some cars, motor cycles and planes in between them and the food ;)\n"},{"id":91,"href":"/apacheconeu-part-0189/","title":"ApacheConEU - part 01","section":"Inductive Bias","content":" ApacheConEU - part 01 # Apache Con EU in Germany - in November, in Sinsheim (in the middle of nowhere): I have to admit that I was more than skeptical whether that would actually work out very well. A day after the closing session it\u0026rsquo;s clear that the event was a huge success: Days before all tickets were sold out, there were six sessions packed with great talks on all things related to Apache Software Foundation projects - httpd, tomcat, lucene, open office, hadoop, apache commons, james, felix, cloud stack and tons of other projects were well covered. In addition the conference featured a separate track on how the Apache community works.\nThe venue (the Hoffenheim soccer team home stadium) worked out amazingly well: The conference had four levels rented with talks hosted in the press room, a lounge and two talks on each of the first and second floor in an open space setup. That way entering a talk late or leaving early was way less of a hazzle than when having to get out the door - sneaking into interesting talks on the second floor was particularly easy: From the third floor that was reserved for catering one could easily follow the talks downstairs. Speaking of catering: Yummy and available all the time - and that not only counts for water but for snacks (e.g. cake between breaks), coffee, soft-drinks, tea etc. On top of that tasty lunch buffet with all sorts of more or less typical regional food. You\u0026rsquo;ve set high standards for upcoming conferences ;)\n"},{"id":92,"href":"/teddy-in-london399/","title":"Teddy in London","section":"Inductive Bias","content":" Teddy in London # While I was at the conference – Teddy spent some time exploring the surroundings of the conference hotel. Looks like in particular Hyde park was attractive:\n"},{"id":93,"href":"/strata-eu-part-4385/","title":"Strata EU - part 4","section":"Inductive Bias","content":" Strata EU - part 4 # The rest of the day was mainly reserved for more technical talks: Tom Wight introducing the merits of MR2, also known as YARN. Steve Loughran gave a very insightful talk on the various failure modes of Hadoop – though the Namenode is like the most obvious single point of failure there are a few more traps waiting for those depending on their Hadoop clusters: Hadoop does just find with single harddisks failing. Failing single machines usually also does not create a huge issue. However what if the switch one of your racks is connected with fails? Suddenly not just one machine has to be re-replicated but a whole rack of machines. Even if you have enough space in your cluster left, can your network deal with the replication traffic? What if your cluster is split in half as a result? Steve gave an introduction to the various HA configurations available for Hadoop. There\u0026rsquo;s one insight I really liked though: If you are looking for SPOFs in your system – just carry a pager … and wait.\nIn the afternoon I joined Ted Dunning\u0026rsquo;s talk on fast kNN soon to be available in Mahout – the speedups gained really do look impressive – just like the fact that the algorithm is all online and single pass.\nIt was good to meet with so many big data people in two days – including Sean Owen who joined the Data Science Meetup in the evening.\nThanks to the O\u0026rsquo;Reilly Strata team – you really did an awesome job making Strata EU an interesting and very well organised event. If you yourself are still wondering what this big data thing is and in what respect it might be relevant to your company Strata is the place to be to find out: Though being a tad to high-level for people with a technical interest the selection of talks is really great when it comes to showing the wide impact of big data applications from IT, the medical sector right up to data journalism.\nIf you are interested in anything big data, in particular who to turn the technology into value make sure you check out the conferences in New York and Santa Clara. Also all keynotes of London were video taped and are available on YouTube by now.\n"},{"id":94,"href":"/strata-eu-part-3384/","title":"Strata EU - part 3","section":"Inductive Bias","content":" Strata EU - part 3 # The first Tuesday morning keynote put the hype around big data into historical context: According to wikipedia big data apps are defined by their capability of coping with data set sizes that are larger than can be handled with commonly available machines and algorithms. Going from that definition we can look back to history and will realize that the issue of big data actually isn\u0026rsquo;t that new: Even back in the 1950s people had to deal with big data problems. One example the speaker went through was a trading company that back in the old days had a very capable computer at their disposal. To ensure optimal utilisation they would rent out computing power whenever they did not need it for their own computations. One of the tasks they had to accomplish was a government contract: Freight charges on rails had been changed to be distance based. As a result the British government needed information on the pairwise distances between all trainstations in GB. The developers had to deal with the fact that they did not have enough memory to fit all computation into it – as a result they had to partition the task. Also Dijkstra\u0026rsquo;s algorithm for finding shortest paths in graphs wasn\u0026rsquo;t invented until 4 years later – so they had to figure something out themselves to get the job done (note: Compared to what Dijkstra published later it actually was very similar – only that they never published it). The conclusion is quite obvious: The problems we face today with Petabytes of data aren\u0026rsquo;t particularly new – we are again pushing frontiers, inventing new algorithms as we go, partition our data to suit the compute power that we have.\nWith everyday examples and a bit of hackery the second keynote went into detail on what it means to live in a world that increasingly depends on sensors around us. The first example the speaker gave was on a hotel that featured RFID cards for room access. On the card it was noted that every entry and exit to the room is being tracked – how scary is that? In particular when taking into account how simple it is to trick the system behind into revealing some of the gathered information as shown a few slides later by the speaker. A second example he have was a leaked dataset of mobile device types, names and usernames. By looking at the statistics of that dataset (What is the distribution of device types – it was mainly iPads as opposed to iPhones or Android phones. What is the distribution of device names? - Right after manufacturer names those contained mainly male names. When correlating these with a statistic on most common baby name per year they managed to find that those were mainly in their mid thirties.) The group of people whose data had leaked used the app mainly on an iPad, was mainly male and in their thirties. With a bit more digging it was possible to deduce who exactly had leaked the data – and do that well enough for the responsible person (an American publisher) to not be able to deny that. The last example showed how to use geographical self tracking correlated with credit card transactions to identify fraudulent transactions – in some cases faster than the bank would discover them. The last keynote provided some insight into the presentation bias prevalent in academic publishing – but in particular in medical publications: There the preference to publish positive results is particularly detrimental as it has a direct effect on patient treatment.\n"},{"id":95,"href":"/strata-eu-part-2383/","title":"Strata EU - part 2","section":"Inductive Bias","content":" Strata EU - part 2 # The second keynote touched upon the topic of data literacy: In an age in which growing amounts of data are being generated being able to make sense of these becomes a crucial skill for citizens just like reading, writing and computing. The speaker\u0026rsquo;s message was two-fold: a) People currently are not being taught how to deal with that data but are being taught that all that growing data is evil. Like an enemy hiding under their bed just waiting to jump at them. b) When it comes to getting the people around you literate the common wisdom is to simplify, simplify, simplify. However her approach is a little different: Don\u0026rsquo;t simplify. Instead give people the option to learn and improve. As a trivial comparison: Just because her own little baby does not yet talk doesn\u0026rsquo;t mean she shouldn\u0026rsquo;t talk to it. Over time the little human will learn and adapt and have great fun communicating with others. Similarly we shouldn\u0026rsquo;t over-simplify but give others a chance to learn.\nThe last keynote dealt gave a really nice perspective on information overload and the history of information creation. Starting back in the age of clay tablets where writing was to 90% used for accounting only – tablets being tagged for easier findability. Continuing with the invention of paper – back then still as roles as opposed to books that facilitated easy sequential reading but made random access hard. The obvious next step being books that allow for random access read. Going on to initial printing efforts in an age where books were still a scarce resource. Continuing to the age of the printing press with movable types when books became ubiquitous – introducing the need for more metadata attached to books like title pages, TOCs and indexes for better findability. As book production became simpler and cheaper people soon had to think of new ways to cope with the ever growing amount of information available to them. Compared to that the current big data revolution does not look to familiar anymore: Much like the printing press allowed for more and more books to become available , Hadoop allows for more and more data to be stored in clusters. As a result we will have to think about new ways to cope with the increasing amount of data at our disposal, time to start going beyond the mere production processes and deal with the implications for society. Each past data revolution left both – winners and loosers – mainly unintentioned by those who invented the production processes. Same will happen with today\u0026rsquo;s data revolution.\nAfter the keynotes I joined some of the nerdcore track talks on Clojure for data science and Cascalog for distributed data analysis, briefly joined the talk on data literacy for those playing with self tracking tools to finally join some friends heading out for an Apache Dinner. Always great to meet with people you know in cities abroad. Thanks to the cloud of people who facilitated the event!\n"},{"id":96,"href":"/oreilly-strata-london-part-1306/","title":"O'Reilly Strata London - part 1","section":"Inductive Bias","content":" O\u0026rsquo;Reilly Strata London - part 1 # A few weeks ago I attended O\u0026rsquo;Reilly Strata EU. As I had the honour of being on the program committee I remember how hard it was to decide on which talks to accept and which ones to decline. It\u0026rsquo;s great to see that potential turned into an awesome conference on all things Big Data.\nI arrived a bit late as I flew in only Monday morning. So I didn\u0026rsquo;t get to see all of the keynotes and plunged right into Dyson\u0026rsquo;s talk on the history of computing from Alan Turing to now including the everlasting goal of making computers more like humans, making them what is generally called intelligent.\nThe next keynote was co-presented by the Guardian and Google on the Guardian big data blog. Guardian is very well known for their innovative approach to journalism that more and more relies on being able to make sense of ever growing datasets – both public and not-yet-published. It was quite interesting to see them use technologies like Google Refine for cleaning up data, see them mention common tools like Google spreadsheets or Tableau for data presentation and learn more on how they enrich data by joining it with publicly available datasets.\n"},{"id":97,"href":"/teddy-in-down-under397/","title":"Teddy in Down Under","section":"Inductive Bias","content":" Teddy in Down Under # The last two September weeks Teddy was in Down Under. He spent the first few days exploring Sydney: Taking the ferry from Manly to the city each morning, followed by beautiful sunny weather, warm enough to already go swimming.\nThe following days took him to the Blue Mountains and into Kangaroo Valley for some hiking, animal watching and kayaking:\nOf course Teddy also made some new friends:\nA huge thanks to Tatjana, Steve and Ash for hosting us in Sydney. Thanks also to Brett, Laura, Samantha and Tobi for hosting us in the Blue Mountains. And thanks to the folks joining us on our very last evening for a Apache Dinner. Was great meeting you – looking forward to see you again soon.\nAlso thanks to Thoralf, Anja, Astro, Douwe, Stefan, Nick, Brett and everyone else who provided us with lots of hints and recommendations on what to do in and near Sydney. As usual it was too little time for too much to do and see.\n"},{"id":98,"href":"/fourth-recsys-stammtisch-berlin191/","title":"Fourth #Recsys Stammtisch Berlin","section":"Inductive Bias","content":" Fourth #Recsys Stammtisch Berlin # This evening the 4th #recsys Stammtisch (German for \u0026ldquo;a meetup involving beer\u0026rdquo;) was kindly organised by Alan Said, Zeno Gantner and Till Plumbaum. The event was hosted by Aklamio with beers and drinks provided by Plista. They had three talks:\n@AlanSaid gave an overview of the topics covered in this year\u0026rsquo;s RecSys conference in Dublin. Instead of going into too much technical detail the presentation gave a whirl-wind tour of the topics that are currently under discussion, the competitions to participate in and links to people relevant to the topic to follow up with. He put his slides online already.\nAs second speaker the meetup had @zenogantner give a tour to his MyMedialight recommender system library. Though written in c# there is no need for a deep c# knowledge to use the system - it comes with useful command line tools out of the box, supports all common algorithms and evaluation setups. One of the few talks where life demos actually worked.\nThe third talk - one of the rare \u0026ldquo;slide-free\u0026rdquo; presentations - covered Plista and it\u0026rsquo;s relation to recommender systems. After going into some more detail on where they came from (from a big over-arching solution down to the narrow, sharp focus of doing ad recommendations), where they want to go (back to an over-arching solution to be offered as a service with the goal of bringing interaction data of many services together in one hosted system). Most interesting news to me: They are working on an open source web-service layer for Apache Mahout that seems to be already in production. Definitely something to watch.\nOverall a good crowd of over 20 people from various startups, universities and larger companies in Berlin joined the meetup. There were even some people travelling there from Magdeburg. Pretty good to know that there are so many people knowledgeable in the general area of recommender systems in and close to Berlin - and good to see some of those I knew already before the meetup again. Looking forward to the next event - any volunteers for organising one? "},{"id":99,"href":"/teddy-in-meisen400/","title":"Teddy in Meißen","section":"Inductive Bias","content":" Teddy in Meißen # Taken earlier this year in Meißen the picture shows a detail of the beautiful cathedral:\nTeddy after a busy day:\n"},{"id":100,"href":"/note-to-self-basic-r-operations297/","title":"Note to self: Basic R operations","section":"Inductive Bias","content":" Note to self: Basic R operations # After searching for that all too often and for too long (in particular the \u0026ldquo;add a column as index\u0026rdquo; bit):\nTo read a file: d \u0026lt;- read.table(\u0026rsquo;/home/isabel/input\u0026rsquo;, sep=\u0026rsquo;,\u0026rsquo;, header=T, quote=\u0026rsquo;\u0026rsquo;)\nUseful for getting an overview of the data:summary(d); head(d); tail(d)\nFor sorting some data frame: s \u0026lt;- d[order(d[,2]),];\nFor adding a column to a data frame: s$idx \u0026lt;- seq(0, nrow(s) - 1, 1)\nFor plotting a column: ggplot(s, aes(idx, engagement)) + geom_point() +scale_x_log10()\n"},{"id":101,"href":"/note-to-self-link-to-3d-maps296/","title":"Note to self - link to 3D maps","section":"Inductive Bias","content":" Note to self - link to 3D maps # After searching for the link the third time today - just in case I happen to be again looking for Nokia\u0026rsquo;s 3d maps: http://maps3d.svc.nokia.com/webgl/index.html is the non-plugin link that works in Firefox.\n"},{"id":102,"href":"/some-thoughts-on-a-conf-taxonomy380/","title":"Some thoughts on a conf taxonomy","section":"Inductive Bias","content":" Some thoughts on a conf taxonomy # One common way for open source developers to meet face-to-face is to attend conferences relevant to their subject of interest. A common way to have one near you if there ain\u0026rsquo;t none yet is to go and organise one yourself. The most obvious stuff to resolve for that task:\nMost likely there will be some financial transactions involved - sponsors wanting to support you, attendees paying for their tickets, you paying for the venue and for food.\nSomeone will have to choose which speakers to invite.\nHow to scale if there are more speakers and attendees than you can reasonably welcome yourself.\nSo far I\u0026rsquo;ve come across a multitude of ways to deal with these two issues alone. Some encountered at events with \u0026gt;200 attendees are listed below. Feel free to add your context.\nName\nContent selection\nFor profit\nTickets\nFood\nScaling model\nFOSDEM/ Brusselsopen CfP, decision by organisersNope - it\u0026rsquo;s hosted by a university, organised by a couple of students and an incredible multitude of volunteers.Access is completely free though attendees are being asked to support the conference with a donation.Food is on sale through the organisersIn addition to two main tracks there\u0026rsquo;s a multitude of independently but affiliated and co-located so-called dev rooms that are completely community organised e.g. for Debian, Java, Embedded, KDE and others\nFrosconopen CfP, decision by organisersNope - again hosted by a university, organised by a couple of students and volunteersTickets are cheap - in the 5 Euro rangeFood is on-sale at the event.There are workshops and related events that are community organised. Those are starting to get more visible in the main program as well.\nLinux Tage Chemnitzopen CfP, decision by organisers + committee.Nope - hosted by TU Chemnitz with huge local support.Cheap - in the 5 Euro range.On sale at the event (soup and related stuff).Stable number of attendees so far.\nChaos Communication Congressopen CfP, decision by organisers + committeeyesfor four days slightly less than 100,- Euroon sale in the venue as well as aroundmove to different location\nChaos Campopen CfP, decision by organisers + committeeyes100 \u0026lt; prize \u0026lt; 500,- range for whole week including camping groundon sale at the locationnot needed so far\nBerlin Buzzwordsopen CfP, decision by volunteersyesmore than 300,- Euros in early birdincluded in the priceaffiliated workshops\nApacheConopen CfP, decision by volunteersyesin EU \u0026gt;200,-, in US usually \u0026gt;1k$included in priceaffiliated meetups\nLucene Revolutionopen CfP, decision by organisersmore or less, mainly PR for organiser\u0026gt;500,-included in pricenot needed so far\nGoTo Coninvitation onlyyes\u0026gt;500,- rangeincluded in priceturn the \u0026ldquo;one location\u0026rdquo; only conference into a series that moves across Europe with the help of some locals that are interested in having the event\nStrataopen CfP, decision made by committee - final decision by organisersyesin the \u0026gt;500 Euro rangeincluded in pricesplit in different locations, organisers remain the same still\nFrom the above table to me it seems that most conferences differ in whether they are fully non profit solely for the sake of education. In contrast to that there are events that are for profit (as in support the organisers financially), or some kind of self-marketing where profit is indirect in terms of more contracts signed. They also differ in whether submissions are open or invited talks only. In addition there are those that have paid talks (usually clearly marked as such) or accept talks through the submission form only. In terms of cost one model is to go extremely low-cost with no money paid for venue or food vs. those that include catering in the ticket price.\nMe personally I have a strong preference to events that feature an open CfP - mainly because talks tend to be more diverse and - given a strong program committee - also of decent quality as only the best make it through. In addition the events tend to be less formal when fully community organised - over time regulars among speakers, attendees and exhibition participants tend to know each other generating a rather friendly athmosphere.\n"},{"id":103,"href":"/speaking-at-apachecon-eu-2012381/","title":"Speaking at ApacheCon EU 2012","section":"Inductive Bias","content":" Speaking at ApacheCon EU 2012 # I\u0026rsquo;ll be at ApacheCon EU in November. Looking forward to an interesting conference on all things Apache that is finally returning back to Europe. Go there if you want to learn more on Tomcat, Hadoop, httpd, HBase, Camel, Open Office, Mahout, Lucene and more.\nNow on to prepare the two talks I submitted:\n\u0026ldquo;Choosing the right tool for your data analysis task - Apache Mahout in context\u0026rdquo;\n\u0026ldquo;I was voted to be committer. Now what?\u0026quot;\nLooking forward to see you there.\n"},{"id":104,"href":"/learning-german261/","title":"Learning German","section":"Inductive Bias","content":" Learning German # For some reason I got that question multiple times now from people that moved to Germany but work in companies where English is the language to use for communication - how to best learn German (in addition finding people to talk to).\nWhen thinking about how I got started with English there were a few things that helped: As a child I got some \u0026ldquo;made for learning English\u0026rdquo; crime stories to read. In 11th/12th grade we got a Newsweek subscription. When at university I quickly learnt that translations of any man pages or help files to German were not really helpful so I switched my Locale to English. In addition the dubbed versions of Futurama were no good - same for most movies you get to see in cinema. Finally getting into open source meant that there was no other way for communication.\nSo what sites are there that provide value to the average geek but are available only in German?\nBlogs and online resources\nLawblog publishes posts on legal related matters - despite the name mainly in German.\nFefe\u0026rsquo;s blog with all things IT\nNetzpolitik\nDigitale Gesellschaft\nTim Pritlove\u0026rsquo;s blog - one of the first people who\u0026rsquo;s income is dominated by Flattr donations\nNewspapers etc.\nZeit\nSüddeutsche\nHeise Newsticker\nBrand Eins (better to ignore the articles on copyright and digital life though\u0026hellip;)\nPodcasts:\nAlternativlos\nChaosradio\nPentacast\nSome music and movies:\nKnorkator - if you are into fun metal.\nÄrzte - if you are into punk rock.\nRainald Grebe - if you are into cabaret.\nLola rennt\n23\nSonnenallee\nIf you have children: At least in eastern Germany the Sandmännchen famously helped generations of children to go to sleep.\nIf you happen to live in Berlin - also take a look at local magazines pointing out current events and special exhibitions. There\u0026rsquo;s also quite a \u0026lt;a href=\u0026ldquo; http://www.amazon.de/Berlin-literarisch-J%C3%BCrgen-Engler/dp/3746628105/ref=sr_1_2?ie=UTF8\u0026qid=1347351223\u0026sr=8-2\"\nfew books in and about Berlin. Make sure to safe some time to vist the Bundestag and book one of their lectures.\nUpdate:\nOne hint from Thomas Koch: Die Deutsche Welle hat unglaublich viel Material, auch einen kompletten mehrsemestrigen Deutschkurs als Podcast + begleitende PDFs.\nThomas, thanks! On a similar note - the archive of DRadio is also well worth a look.\n"},{"id":105,"href":"/moving-to-a-new-domain287/","title":"Moving to a new domain","section":"Inductive Bias","content":" Moving to a new domain # Executive summary: This is to warn those of you who are subscribed to this blog - the domain to reach this blog w/o redirects will soon change to by isabel-drost-fromm.de - you might want to adjust your rss subscription accordingly.\nLonger version: This blog post is scheduled to go live some time after lunch-time on September 12th 2012. You might have heart rumors before - that date Ms. Isabel Drost and Mr. Thilo Fromm are supposed to get married. There were times when war and conflicts between kingdoms were settled by having children of the reigns get married. Today this old tradition is being continued on a much smaller scale by having a couple get married that is comprised of one half being passionate about Linux Kernel hacking and a strong proponent of GPL/LGPL open source licensing and the other half coming from the Java world, mainly contributing to ASL projects.\nAs a bit of \u0026ldquo;showing of good will\u0026rdquo; both agreed to the proposal of Matthias Kirschner: Girls that are FSFE fellows really should only marry other FSFE fellows. So we got Thilo a fellowship membership setup very quickly.\nPS: Now looking forward to dancing into a new part of life this evening ;)\nPps: Thanks to photomic for the DLSR fotos, and to masq for taking the above picture and mailing it to my server. Having a secure shell on your mobile phone rocks!\n"},{"id":106,"href":"/video-up-dragan-milosevic-on-robust-communication-mechanisms423/","title":"Video up: Dragan Milosevic on \"Robust Communication Mechanisms\" ","section":"Inductive Bias","content":" Video up: Dragan Milosevic on \u0026ldquo;Robust Communication Mechanisms\u0026rdquo; # Dragan Milosevic: \"Robust Communication Mechanisms in zanox Reporting Systems\" from David Obermann on Vimeo.\n"},{"id":107,"href":"/froscon-on-teaching194/","title":"FrOSCon - on teaching","section":"Inductive Bias","content":" FrOSCon - on teaching # The last talk I went to during FrOSCon was Selena\u0026rsquo;s keynote on \u0026ldquo;Mistakes were made\u0026rdquo;. She started by explaining how she taught computer science (or even just computer-) concepts to teachers herself - emphasizing how exhausting teaching can be, how many even trivial concepts were unknown to her students. After that Selena briefly sketched how she herself came to IT - emphasizing how providing mostly the information she needed to accomplish the current task at hand and telling how to get more information helped her make her first steps a great deal.\nThe main point of her talk however was to highlight some of the underlying causes for the lack of talented cs students. Some background literature is online at her piratepad on the subject.\nThe discussion that followed the keynote (and included contributions from two very interested, refreshingly dedicated teachers) was quite lively: People generally agreed that computer science/ computing or even just logical and statistical thinking plays a sadly minor role in current education. Students are mainly forced to memorize large amounts of facts by heart but are not taught to question their environment, discover relations or rate sources of information. The obvious question that seemed to follows was that on what to remove from the curriculum when introducing computing as a subject. My personal take on that is that maybe there is no need for removing anything - instead changing the way concepts are taught might already go a long way: Put arts, maths, natural sciences and music into context, have kids evaluate statistics and rate them not only in maths but also in e.g. biology by letting them examine some common statistical fallacies in the subject area.\nAnother problem stated was the common lack of technical understanding, the common lack of time for preparation and the common lack of understanding for the concept of open source or creative commons content. Taken together this makes sharing teaching material and improving it together with others incredibly hard.\nSelena\u0026rsquo;s call to action was for geeks to get involved and educate the people near and dear to them instead of giving up. On thing to add to that: Most German universities have some sort of visitors\u0026rsquo; days to prospective students - some even have collaborations with schools to do projects together with younger ones - make sure to check out your own university - you might well find out that teaching is not only exhausting but also particularly rewarding especially when teaching students that really want to know and participate in your project just because they want to.\nIf you know any teachers who are open to the idea of having externals take over some their lessons or at least provide input get them connected with your peers that are interested in educating others. Also keep in mind that most open source projects, hacker spaces and related organisations in Germany are so-called \u0026ldquo;gemeinnütziger e.V.\u0026rdquo; - a status that in many cases was achieved by declaring the advancement of education as at least one of their goals.\n"},{"id":108,"href":"/froscon-understanding-linux-with-strace196/","title":"FrOSCon - understanding Linux with strace","section":"Inductive Bias","content":" FrOSCon - understanding Linux with strace # Being a Java child I had only dealt with strace once before: Trying to figure out whether any part of the Mahout tests happens to use /dev/random for initialisation in a late night debugging session with my favourite embedded Linux developer. Strace itself is a great tool to actually see what your program is doing in terms of system calls, giving you the option to follow on a very detailed level what is going on.\nIn his talk Harald König gave a very easy to follow over view on how to understand Linux with strace. Starting with the basic use cases (trace file access, program calls, replay data, analyse time stamps, do some statistics) the quickly moved on to showing some more advanced tricks you can do with the little tool: Finding sys-calls that take surprisingly long vs. times when user code is doing long-running computations. Capturing and replaying e.g. networking related calls to simulate problems that the application runs into. Figuring out bottlenecks (or just plain weird stuff) in the application by figuring out the most frequent syscall. Figuring out which configuration files an application really touches - sorting them by last modified date with a bit of shell magic might give an answer to the common question of whether the last update or the last time the user tinkered with the configuration turned his favourite editor to appear green instead of white. On the other hand it can also reveal when configurations have been deleted (in the presentation he moved away the user-emacs configuration. As a result emacs tried \u0026gt;30 times to find it for various configuration options during startup: Is it there? No. \u0026hellip; Is it now there? No. \u0026hellip; Maybe now? Nope. \u0026hellip; ;) ).\nWhen looking at strace, you might also want to take a look at ltrace that traces library calls - the output there might be a bit more readable in that it\u0026rsquo;s not just system calls but also library calls. Remember though that tracing everything can not only make your app pretty slow but also quickly generates several gigabytes of information.\n"},{"id":109,"href":"/froscon-git-goodies193/","title":"FrOSCon - Git Goodies","section":"Inductive Bias","content":" FrOSCon - Git Goodies # In his talk on Git Goodies Sebastian Harl introduced not only some of the lesser known git tooling but also gave a brief introduction as to how git organises its database. Starting with an explanation of how patches essentially are treated as blobs identified by SHA1 hashes (thus avoiding duplication not only in the local database but allover the git universe), pointed to by trees that are in turn generated and extended by commits that are in turn referenced by branches (updates on new commits) and tags (don\u0026rsquo;t update on new commits). With that concept in mind it suddenly becomes trivial to understand that HEAD simply is a reference to wherever you next commit is going to in your working directory. It also becomes natural to understand that HEAD pointing just to a commit-id but not to a branch is called a de-tached head.\nCommits in git are tracked in three spaces: In the repository (this is where stuff goes after a commit), in the index (this is where stuff goes after an add or rm) and in the working directory. Reverting is symetric: git checkout takes stuff from the repository and puts it into the current working copy. reset \u0026ndash;mixed/\u0026ndash;hard only touches the index.\nWhen starting to work more with git start reading the man and help pages. They contain lots of goodies that make daily work easier: There are options that allow for colored diffs, setting external merge tools (e.g. vimdiff), setting the push default (just current branch or all matching branches). There are options to define aliases for commands (diff here has a large variety of options that can be handy like coloring only different words instead of lines). There are options to set the git-dir (where .git lies) as well as the working directory which makes it easy to track your website in git but not have the git directory lie in your public_html folder.\nThere is a git archive to checkout your stuff as tar.gz. When browsing the git history tig can come in handy - it allows for browsing your repository with an ncurses interface, show logs, diffs and the tree of all commits. You can ask it to only show logs that match a certain pattern. Make sure to also look at the documentation of ref-parse that explains how to reference commits in an even more flexible manner (e.g. master@{yesterday}). Also checkout the git reflog to take a look at the version history of your versioning. Really handy if you ever mess up your repository and need to get back to a sane state. Also a good way to recover detached commits. Take a look at git-bisect to learn more on how to binary-search for commits that broke your build. Use a fine granular way to add changes to your repository with git add -p - do not forget to take a look at git stash as well as cherry-pick.\n"},{"id":110,"href":"/froscon-robust-linux-embedded-platform195/","title":"FrOSCon - Robust Linux embedded platform","section":"Inductive Bias","content":" FrOSCon - Robust Linux embedded platform # The second talk I went to at FrOSCon was given by Thilo Fromm on Building a robust embedded Linux platform. For more information on the underlying project see also projec HidaV on github. Slides of the talk Building a robust Linux embedded platform are already online.\nInspired by a presentation on safe upgrade prodedures in embedded devices by Arnaut Vandecappelle in the Embedded Dev Room FOSDEM earlier this year Thilo extended the scope of the presentation a bit to cover safe kernel upgrades as well as package updates in embedded systems.\nThe main goal of the design he presented was to allow for developing embedded systems that are robust - both in normal operation but also when upgrading to a new firmware version or a set of new packages - the design included support for upgrading and rolling back to a known working state in an atomic way. Having systems deployed somewhere in the wild to power a wind turbine, inside of busses and trains or even within satellites pretty much forbids relying on an admin to press the \u0026ldquo;reset button\u0026rdquo;.\nOriginal image xkcd.com/705\nThe reason for putting that much energy into making these systems robust also lies in the ways they are deployed. Failure vectors include not only your usual software bugs, power failures or configuration incompatibilities. Transmission errors, storage corruption, temperature, humidity add their share to increase the probability of failure.\nAchieving these goals by building a custom system isn\u0026rsquo;t too trivial. Building a platform that is versatile enough to be used by others building embedded systems adds to the challenges: Suddenly having easy to use build and debug tools, support for software life-cycle management and extend-ability are no longer nice-to-have features.\nThilo presented two main points to address the requirements: The first is to avoid trying to cater every use case. Setting requirements for a platform in terms of performance, un-brickability (see also urban dictionary, third entry as of this writing). Even setting a requirement for dual boot support or to the internal storage technology used. As a result designing the platform can become a lot less painful.\nThe second step is to harden the platform itself. Here that means that upgrading the system (both firmware and packages) is atomic, can be rolled-back atomically and thus no longer carries the danger of taking the device down for longer than intended: A device that does no longer perform it\u0026rsquo;s task in the embedded world usually is considered broken and shipped back to the producer. As a result upgrading may be necessary but should never render the device useless.\nOne way to deal with that is to store boot configurations in a round robin manner - for each configuration a \u0026ldquo;was booted\u0026rdquo; (set by the bootloader on boot) and a \u0026ldquo;is healthy\u0026rdquo; (set by the system after either a certain time of stability or after running self tests) flags are needed. This way at each boot it is clear what the last healthy configuration was.\nTo do the same with your favourite package management system is slightly more complicated: Imagine running something like apt-get upgrade with the option to switch back to the previous state in an atomic way if anything goes wrong. One option to deal with that presented is to work with transparant overlay filesystems that allow for having a read-only base layer - and a \u0026ldquo;transparent\u0026rdquo; r/w layer on top. If a file does not exist in the transparent layer, the filesystem will return the original r/o version. If it does exist it will return the version in the transparent overlay. In addition there\u0026rsquo;s also an option to mark files as deleted in the overlay.\nWith that upgrading becomes as easy as installing the upgraded versions into some directory in your filesystem and mounting said directory as transparent overlay. With that roll-back as well as snapshots are easy to do.\nThe third ingredient to achieving a re-usable platform presented was to use Open Embedded. Including an easy to extend layer-based concept, support for often recent software versions, versioning and dependency modelling, some BSP layers officially supported by hardware manufacturers building a platform on top of Open Embedded is one option to make it easily re-useable by others.\nIf you want to know more on the concepts described join HiDaV platform project - many of the concepts described are already - or soon to be - implemented. "},{"id":111,"href":"/froscon-2012-rest198/","title":"FrOSCon 2012 - REST","section":"Inductive Bias","content":" FrOSCon 2012 - REST # Together with Thilo I went to FrOSCon last weekend. Despite a few minor glitches and the \u0026ldquo;traditional\u0026rdquo; long BBQ line the conference was very well organised and again brought together a very diverse crowd of people including but not limited to Debian developers, OpenOffice people, FSFE representatives, KDE and Gnome developers, people with background in Lisp, Clojure, PHP, Java, C and HTML5.\nThe first talk we went to was given by JThijssen on REST in practice. After briefly introducing REST and going a bit into Myths and false believes about REST he explained how REST principles can be applied in your average software development project.\nTo set a common understanding of the topic he first introduced the four steps REST Maturity Model: Step zero means using plain old xml over http for rpc or SOAP. Nothing particularly fancy here - even to some extend breaking common standards related to http. Going one level up means modeling your entities as resources. Level two is as simple as using the http verbs for what they are intended - don\u0026rsquo;t delete anything on the other side just by using a GET request. Level three finally means using hypermedia controls, HATEOS and providing navigational means to decide on what to do next.\nMyths and legends\nRest is always http - well, it is transport agnostic. However mostly it using http for transport.\nRest equals CRUD - though not designed for that it is often used for that task in practice.\nRest scales - as a protocol yes, however of course that does not mean that the backend you are talking to does. All Rest does for you is to give you a means to horizontally scale without having to worry too much about server state.\nCommon mistakes\nUsing Http verbs - if you\u0026rsquo;ve ever dealt with web crawling you probably know those stories of some server\u0026rsquo;s content being deleted just be crawling a public facing web site just because there was a \u0026ldquo;delete\u0026rdquo; button somewhere that would trigger a delete action through an innocent looking GET request. The lesson learnt of those: Use the verbs for what they are intended to be used. One commonly confused thing is the usage of PUT vs. POST. Common rule of thumb that also applies to the CouchDB REST API: Use PUT if you know what the resulting URL should be (e.g. when storing an entry to he database and you know the key that you want to use). Use POST if you do not care about which URL should result from the operation (e.g. if the database should automatically generate a unique key for you). Also make sure to use the error codes as intended - never return error code 2?? only to add an xml snippet to the payload that explains to the surprised user that an error occurred including an error code. If you really need an explanation of why this is considered bad practice if not plain evil, think about caching policies and related issues.\nWhen dealing with resources a common mistake is to stuff as much information as possible into one single resource for one particular use case. This means transferring a lot of additional information that may not be needed for other use cases. A better approach could be to allow clients to request custom views and joins of the data instead of pre-generating them.\nWhen it comes to logging in to your API - don\u0026rsquo;t design around HTTP - use it. Sure you can give a session id into a cookie to the user. However than you are left with the problem of handling client state on the server - which was supposed to be stateless so clients can talk to any server. You could store the logged in information in the client cookie - signing and encrypting that might even make it slightly less weird. However the cleaner approach would be to authenticate individual requests and avoid state altogether.\nWhen it comes to URL design keep in mind to keep them in a format that is easy to handle for caches. An easy check would be to try and bookmark the page you are looking at. Also think about ways to increase the number of cache hits if results are even slightly expensive to generate. Think about an interface to retrieve the distance from Amsterdam to Brussels. The URL could be /distance/to/from - however given no major road issues the distance from Amsterdam to Brussels should be the same as from Brussels to Amsterdam. One easy way to deal with that would be to allow for both requests but to send a redirect to the first version in case a user requests the second. The semantics would be slightly different when asking for driving directions - there the returned answers would indeed differ.\nThe speaker also introduced a concept for handling asynchronous updates that I found interesting: When creating a resource hand out a 202 accepted response including a queue ticket that can be used to query for progress. For as long as the ticket is not yet being actively dealt with it may even contain cancellation methods. As soon as the resource is created requesting the ticket URL will return a redirect to the newly created resource.\nThe gist of the talk for me was to not break the Rest constraints unless you really have to - stay realistic and pragmatic about the whole topic. After all, most likely you are not going to build the next Twitter API ;)\n"},{"id":112,"href":"/video-stefan-hubner-on-cascalog428/","title":"Video: Stefan Hübner on Cascalog","section":"Inductive Bias","content":" Video: Stefan Hübner on Cascalog # Stefan Hübner: \"Introducing Cascalog: Functional Data Processing for Hadoop\" from David Obermann on Vimeo.\n"},{"id":113,"href":"/video-accessing-hadoop-data-with-hcatalog-and-postgresql424/","title":"Video: \"Accessing Hadoop data with HCatalog and PostgreSQL\"","section":"Inductive Bias","content":" Video: \u0026ldquo;Accessing Hadoop data with HCatalog and PostgreSQL\u0026rdquo; # Manuel Meßner, Stephan Friese, Dr. Stefanie Huber: \"Accessing Hadoop data with HCatalog and PostgreSQL\" from David Obermann on Vimeo.\n"},{"id":114,"href":"/open-source-meetup-berlin319/","title":"Open Source Meetup Berlin","section":"Inductive Bias","content":" Open Source Meetup Berlin # This evening the (to my knowledge first) Berlin Open Source Meetup took place at Prater (Bier-)garten in Berlin. There are lots of project specific meetings, a monthly Free Software meeting, quite some stuff on project management. However this was one of the rare occasions where you get Linux kernel hackers, Wikidata project members, Debian developers, security people, mobile developers as well as people writing on free software or making movies related to the topic around one table.\nDespite the heat (over 30 degrees Celcius in Berlin today) over 30 people gathered for some food, cold beer, some drinks and lots of interesting discussions. Would be great to see another edition of this kind of event.\n"},{"id":115,"href":"/spotted-this-morning382/","title":"Spotted this morning...","section":"Inductive Bias","content":" Spotted this morning\u0026hellip; # in front of my office:\nEver wondered how accurate navigable map data for your Garmin, your in-car navigation system (most likely), or maps.nokia.com are created? One piece of the puzzle is the car above collecting data for Navteq, a subsidary of Nokia.\n"},{"id":116,"href":"/apache-hadoop-get-together-berlin-august-201256/","title":"Apache Hadoop Get Together Berlin - August 2012","section":"Inductive Bias","content":" Apache Hadoop Get Together Berlin - August 2012 # Despite beautiful summer weather roughly 50 people gathered at ImmobilienScout24 for the August 2012 edition of the Apache Hadoop Get Together (Thanks again for hosting the event and sponsoring drinks and pizza to ImmoScout as well as to David Obermann for organising the meetup.\nToday there were three talks: In the first presentation Dragan Milosevic (also known from his talk at the Hadoop GetTogether and his presentation at Berlin Buzzwords) provided more insight as to how Zanox is managing their internal RPC protocols in particular when it comes to versioning and upgrading protocol versions. Though in principle very simple to do this sort of problem still is very common when starting to roll out distributed systems and scaling them over time. The concepts he described were not unlike what is available today in projects like Avro, Thrift or protocol buffers. However by the time they needed versioning support for their client server applications neither of these projects was a really good fit. This also highlights one important constraint: With communication being a very central component in distributed systems, changing libraries after an implementation went to production can be too painful to be followed through.\nIn the second presentation Stefanie Huber, Manuel Messner and Stephan Friese showed how Gameduell is using Hadoop to provide better data analytics for marketing, BI, developers, product managers et.al. Founded in 2003 they have a accumulated quite a bit of data consisting of micro transactions (related to payment operations), user activities, gaming results that need to be used for balancing games. Their team turned a hairy, complex system into a pretty clean, Hadoop based solution: By now all actions end up in a Hadoop cluster (with an option to subscribe to a feed for realtime events). Typically from there people would start analysis jobs either in plain map reduce or in pig and export the data to external databases for further analysis by BI people who preferred Hive as a query language as it is much closer to SQL than any of the alternatives. As of late they introduced HCatalog to support providing a common view on data for all three analysis options - in addition to allowing for a more abstract view of the data available that does not require knowing the exact filesystem structure to access the data.\nAfter a short break in the last talk of the evening Stefan Hübner introduced Cascalog to the otherwise pretty Java-savvy crowd. Being based on Cascading Cascalog provides for a concise way of formulating queries to a Hadoop cluster (compared to plain map reduce). Also when contrasted with Pig or Hive what stands out is the option to easily and seemlessly integrate additional functions (both map- and reduce-side) into Cascalog scripts without switching languages or abstractions. Note: When testing Cascalog scripts, one project to look at is Midje. Overall a really interesting evening with lots of new input, interesting discussions and new input. Always amazing to see what other big data applications people in Berlin are developing. It\u0026rsquo;s awesome to see so many development teams adopt seemingly new technologies (some even still in the Apache Incubator) for production systems. Looking forward to the next edition - as well as to the slides and videos of today\u0026rsquo;s edition.\n"},{"id":117,"href":"/data-scientists-researchers-persectives145/","title":"Data Scientists - researchers' persectives","section":"Inductive Bias","content":" Data Scientists - researchers\u0026rsquo; persectives # \u0026ldquo;Data scientist\u0026rdquo; as a term has caught quite some attention as of late (together with all the big data, scalability and cloud hype). Instead of re-hashing arguments seen in other sources I thought it might make more sense to link to a few of the thought provoking posts I came across recently.\nIn his post Mikio Braun analyses the factors motivating research in academia vs. engineering in the industry. Nice background material on why work looks so different in these fields. If you\u0026rsquo;ve never worked at academia this might be an interesting read to understand why research groups and research project look so different than your average open source project.\nSome work on why data science really is just a sub-branch of statistics.\nAn analysis on reasons for hiring a data scientist, tools and skills of a data scientist and the common problem of people pretending to be great data scientists.\nFinally a list of skills a data scientist should have according to Matthew Hurst.\nAlso interesting: The different perspectives of statistics and machine learning\n"},{"id":118,"href":"/on-reading-code308/","title":"On Reading Code","section":"Inductive Bias","content":" On Reading Code # “If you don’t have time to read, you don’t have the time or the tools to write.” –Stephen King\nQuite a while ago GeeCon published the video taped talk of Kevlin Henney on \"Cool Code\". This keynote is great to watch for everyone who loves to read code - not the one you encounter in real world enterprise systems - but the one that truely teaches you lessons:\nGeeCON 2012: Kevlin Henney - Cool Code from GeeCON Conference on Vimeo.\n"},{"id":119,"href":"/apache-con-coming-to-europe22/","title":"Apache Con returns to Europe","section":"Inductive Bias","content":" Apache Con returns to Europe # In November Apache Con will come back to Europe. The event will take place in Sinsheim inviting foundation members, project committers, contributors and users to meet, discuss and have fun during the one week event.\nSeveral meetups will be held the weekend before the main conference kicks off, watch out for announcements on your favourite project mailing list.\nApacheCon is still open for submissions until August 3rd - head over to the Call for submissions for more information. The conference is split into several tracks that are being handled individually: Apache Daily - Tools frameworks and components used on a daily basis, Apache Java Enterprise projects, Big Data, Camel in Action, Cloud, Linked Data, Lucene, Modular Java Applications, NoSQL Database, OFBiz (The Apache Enterprise Automation project), Open Office and finally Web Infrastructure (covering HTTPD, TomCat and Traffic Server, the heart of many Internet projects).\nMake sure to mark the date in your calendar to meet with the people behind the ASF projects, learn more on how the foundation works and what makes Apache projects so particular compared to others. Join us for a week of fun and dense talks on all things Apache.\nThe Apache Feather logo is a trademark of The Apache Software Foundation.\n"},{"id":120,"href":"/froscon-2012197/","title":"FrOSCon 2012","section":"Inductive Bias","content":" FrOSCon 2012 # On August 25th/26th the Free and Open Source Conference (FrOSCon) will again kick off in Sankt Augustin/ Germany. The event is completely community organised, hosted by the FH Sankt Augustin. It covers a broad range of free software topics like Arduino microcontrollers, git goodies, politics, strace, open nebula, wireshark and others.\nThree highlights that are on my schedule:\nI\u0026rsquo;ll make sure I do not miss Thilo Fromm\u0026rsquo;s presentation on building a platform project on top of Open Embedded.\nBeing an FSFE fellow I\u0026rsquo;ll hang out for a bit at their booth. On Saturday there\u0026rsquo;s the traditional FrOSCon BBQ - most likely I\u0026rsquo;ll go there. Would be great to also meet some other Apache people there.\nLooking forward to interesting talks and discussions at FrOSCon.\n"},{"id":121,"href":"/oreilly-strata-coming-to-london304/","title":"O'Reilly Strata coming to London","section":"Inductive Bias","content":" O\u0026rsquo;Reilly Strata coming to London # O\u0026rsquo;Reilly Strata is coming to London. The first edition of Strata back in 2011 brought Big Data developers, designers, scientists and decision makers together to discuss all things scalable. This year in October the conference comes to Europe: O\u0026rsquo;Reilly Strata EU will take place in London.\nDate: October 1st - 2nd 2012\nVenue: Hilton London Metropole, 225 Edgware Road, London W2 1JU, UK\nThe schedule covers a great deal of use cases and war stories that involve big data and data driven development. Both days are packed with both deep technical but also strategy level presentations that can help drive your projects. Having been on the program committee I got a glimpse of the diversity and high quality of the submissions received. Choosing the best wasn\u0026rsquo;t easy, but there\u0026rsquo;s only so much content you can sqeeze in two conference days.\nLooking forward to London.\nPS: If you have any interesting war stories and anti-patterns involving big data to share consider adding your input online.\n"},{"id":122,"href":"/book-search-patterns127/","title":"Book: Search Patterns","section":"Inductive Bias","content":" Book: Search Patterns # I got the book months ago during FOSDEM - the O\u0026rsquo;Reilly book table always is a pretty dangerous place as a meeting point for me: Search Patterns - Design for Discovery is one of those small, deceivingly beautiful books that manages to explain effective search engine design by focusing on the end user needs but going into some detail concerning the basics of search engine backends as well.\nWe use them on a daily basis not only for finding content on the web but also for navigating shopping sites, discovering news content and even finding articles on blogs and open source project pages. Many discovery tasks can be easily expressed as a search problem and as a result tackled with by now standard off the shelve software like Apache Lucene - or event the commercial counterparts from the enterprise search market. Still oftentimes search is perceived as being made up of simple a small box that users type (typically one or two term) queries into and that as a result show a list of some ten links.\nAfter setting the stage for search in the first chapter the book goes into some more detail in \u0026ldquo;The anatomy of search\u0026rdquo;. In a very approachable way it explains all the components from user constraints, graphical interface, the basics of retrieval and evaluating search performance in terms of precision and recall. The third chapter shows some bahavioural patterns that make discovery easier for users - from incrementally constructing the answer, progessively disclosing more and more detail up to being predictable.\nFinally the design patterns as identified by the authors are introduced. Pretty obvious to those working in the field but well explained to those not intimately familiar with the topic:\nThough perceived as a mere convenience to type less by users, autocomplete can actually help guide the user\u0026rsquo;s search in case of ambiguities and can help avoid imprecise results.\nExpected as it might be by users, presenting the best result first actually goes a long way when building credibility for a search engine. Having more precise queries to guide e.g. as a result of autocomplete helps here. So does having strong ranking criteria to build up a compelling ranking function that is used by default (even though others might be offered as an alternative for users to explore more and different results).\nFederated search has both - advantages (integrating otherwise isolated silos of knowledge) but also disadvantages (it\u0026rsquo;s speed being dominated by the slowest connected search engine).\nFacetted navigation is pretty much standard for any major search engine - giving the user the option to start with a broad query that returns an overwhelming amount of results but guiding the user when refining the query is one major way of driving searches.\nOffering personalisation tends to be one beloved feature though it is particularly hard to implement and needs a good deal of user data to work well. Usually there are features that require less work to get done that are more promising to start with.\nPageination is as much standard to be expected by users - though its implementation can differ: Though we are used to clicking the next button, this actually may not make much sense and just lead to interrupting the user\u0026rsquo;s flow. Much more appealing - but sometimes also confusing - can be interfaces that allow for simply extending the result page when scroling to it\u0026rsquo;s end.\nStructured results provide a way to give the user more than just an outlink - triggered by specific searches it may be possible to directly answer the user\u0026rsquo;s question instead of linking to content that answers it.\nActionable results are a way for the user to get active\neither by voting on results, bookmarking them or sharing them with others.\nUnified discovery is about accepting that search always plays a role in a bigger context and has to play well with the discovery mode the user is in: When searching for \u0026ldquo;apple\u0026rdquo; while browsing the category \u0026ldquo;electronics\u0026rdquo; it\u0026rsquo;s rather unlikely that I am looking for the fruit. Similarly search should take context into account and support me seamlessly when switching from discovery to directed search and back to discovery mode.\nThe book concludes by going into some detail on example search engines and presenting some features that are not yet commonplace but might change the world by employing search in new and creative ways.\nEasy to read, well written, several nice examples to make the technical points simpler to understand. Definitely a good read for domain experts planning to build a search engine, designers trying to understand the basics of building effective search engines and engineers struggling for words to explain why a seemingly little box can cause a whole lot of pain when done wrong but a whole lot of joy when done right. "},{"id":123,"href":"/teddy-in-sweden404/","title":"Teddy in Sweden","section":"Inductive Bias","content":" Teddy in Sweden # Some picture taken all the way up in northern Sweden:\nThose picture were taken mid-June. That means what looks like Teddy sitting in the afternoon sun actually was taken 20min before midnight some 40km south of the arctic circle at Camp Frevisören - an incredible spot to start the day on a canoo:\nView Larger Map\n(That little half-isle that stretches into the ocean.)\nIf you ever travel that far north, make sure to stop by at Hulkoff.se. We got the tip only a few days before we left asking a Swedish friend where to go to see midsummer. Though he is not from that very area he recommended going there if we get a chance - and that turned out a beautiful idea: Not only is the restaurant/ conference venue nicely located just a few km before Finland - they also serve most tasty meals!\nPS: In case you\u0026rsquo;re wondering what that monkey on the pictures is - it\u0026rsquo;s Teddy\u0026rsquo;s new friend \u0026ldquo;Herr Nielson\u0026rdquo; - the little squirrel monkey that is the best friend of the strongest girl in the world.\n"},{"id":124,"href":"/recsys-meetup-berlin330/","title":"Recsys meetup Berlin","section":"Inductive Bias","content":" Recsys meetup Berlin # Planning a meetup in Berlin: 8 people register, a table for 14 people is booked, 16+ people arrive - all of that even if no pre-defined topic or talk is announced. Seems like building recommender systems is a hot topic currently in Berlin.\nThanks to Zeno Gantner from MyMedialight for organising the event - looking forward to the next edition.\n"},{"id":125,"href":"/apache-hadoop-get-together-berlin-446/","title":"Apache Hadoop Get Together Berlin","section":"Inductive Bias","content":" Apache Hadoop Get Together Berlin # As seen on Xing - the next Apache Hadoop Get Together is planned to take place in August:\nWhen: 15. August, 18 p.m. Where: Immobilien Scout GmbH, Andreasstr. 10, 10243 Berlin As always there will be slots of 30min each for talks on your Hadoop topic. After each talk there will be time for discussion. It is important to indicate attendance. Only registered visitors will be permitted to attend. Register here: https://www.xing.com/events/hadoop-get-together-1114707\u0026lt;/ a\u0026gt;\nTalks scheduled thus far: Speaker: Dragan Milosevic Session: Robust Communication Mechanisms in zanox Reporting Systems It happened an annoying number of times that we wanted to improve only one particular component in our distributed reporting system, but often had to update almost everything due to the RPC version-mismatch, which occurred in a communication between the updated component and the rest of our system. To mitigate this problem and to significantly simplify the integration of new components, we extended the used RPC protocol to perform a version handshake before the actual communication starts. This RPC extension is accompanied with serialisation/deserialization methods, which are downward compatible due to being able to successfully deserialise any serialised older version of exchanged objects. Putting together these extensions makes it possible for us to successfully operate multiple versions of frontend and backend components, and to have the power to autonomously decide what and when should be updated/improved in our distributed reporting system. Two other talks are planned and I will provide you with further information soon. A big Thank You goes to Immobilien Scout GmbH for providing the venue at no cost for our event and for sponsoring the videotaping of the presentations. Looking forward to seeing you in Berlin, David\n"},{"id":126,"href":"/need-your-input-failing-big-data-projects-experiences-from-the-wild291/","title":"Need your input: Failing big data projects - experiences from the wild","section":"Inductive Bias","content":" Need your input: Failing big data projects - experiences from the wild # A few weeks ago my talk on \u0026ldquo;How to fail your big data project quick and rapidly\u0026rdquo; was accepted at O\u0026rsquo;Reily Strata conference in London. The basic intention of this talk is to share some anti-patterns, embarrassing failure modes and \u0026ldquo;please don\u0026rsquo;t do this at home\u0026rdquo; kind of advice with those entering the buzzwordy space of big data.\nInspired by Thomas Sundberg\u0026rsquo;s presentation on \u0026ldquo;failing software projects the talk will be split in five chapters and highlight the top two failure-factors for each.\nI only have so much knowledge of what can go wrong when dealing with big data. In addition no one likes talking about what did not work in their environment. So I\u0026rsquo;d like to invite you to share your war stories in a public etherpad - either anonymously or including your name so I can give credit. Some ideas are already sketched up - feel free to extend, adjust, re-rank or change.\nLooking forward to your stories.\n"},{"id":127,"href":"/note-to-self-clojure-with-vim-and-maven298/","title":"Note to self: Clojure with Vim and Maven","section":"Inductive Bias","content":" Note to self: Clojure with Vim and Maven # Steps to get a somewhat working Clojure environment with vim:\nInstall the current vimclojure plugin.\nGet and install a nailgun client.\nAdd vimclojure to your clojure project pom.xml.\nStart the nailgun server from within your maven project with mvn clojure:nailgun with the maven clojure plugin.\nFinally start vim, open your favourite clojure file - you can open a REPL with \\sr, when in a function definition you can evaluate that with \\et - see also tamining vim clojure\nNote: There is more convenient tooling for emacs (see also getting started with clojure and emacs) - its just that my fingers are more used to interacting with vim\u0026hellip;\n2nd note: This post is not an introduction or walk through on how to get Clojure setup in vim - it\u0026rsquo;s not even particularly complete. This is intentional - if you want to start tinkering with Clojure: Use Emacs! This is just my way to re-discover through Google what I did the other day but forgot in the mean time.\n"},{"id":128,"href":"/apache-sling-and-jackrabbit-event-coming-to-berlin84/","title":"Apache Sling and Jackrabbit event coming to Berlin","section":"Inductive Bias","content":" Apache Sling and Jackrabbit event coming to Berlin # Interested in Apache Sling and/or Apache Jackrabbit? Then you might be interested in hearing that on September 26th to 28th there will be an event in town on these two topics - mainly organised by Adobe, but labeled as community event, meaning that there will be a number of active community members attending the conference: adaptTo().\nFrom their website:\nIn late September 2012 Berlin will become the global heart beat for developers working on the Adobe CQ technical stack. pro!vision and Adobe are working jointly to set up a pure technical event for developers that will be focused on Apache Sling, Apache Jackrabbit, Apache Felix and more specifically on Adobe CQ: adaptTo(), Berlin. September 26-28 2012.\n"},{"id":129,"href":"/preparation-done-clock-is-ticking327/","title":"Preparation done - clock is ticking","section":"Inductive Bias","content":" Preparation done - clock is ticking # The clock is ticking - only one more weekend to go before Berlin Buzzwords opens its doors for the main conference (check out the Wiki for the Sunday evening Barcamp and the Sunday Movie Hackday). Looking forward to an amazing week with awesome speakers and great attendees.\nOne word of warning before: Given all the buzz around that conference as of now until mid-next week I won\u0026rsquo;t take any major decisions, most likely I won\u0026rsquo;t be able to follow through with any additional organisation, probably I won\u0026rsquo;t remember everyone I meet on-site.\nIn case I do take decisions - don\u0026rsquo;t trust any of them. If you do need help organising some meetup or dinner - I\u0026rsquo;m happy to help out with recommendations on where to go and who to ask, I\u0026rsquo;m also happy to get you in touch with people relevant to your area of interest. However when it comes to selecting the restaurant, deciding on the day and time, booking a table and informing everyone involved you are on your own. In case you have any questions, requests or advise please make sure to send a copy to my inbox to make sure it will be dealt with (though it might take some time for me to get back to today\u0026rsquo;s inbox zero level I\u0026rsquo;ll make sure I\u0026rsquo;ll get through all of it).\nOther than that\nthanks to ntc and Nick the Barcamp is all setup, the conference is well on track, thanks to many external helping hands we\u0026rsquo;ve again got a convincing line-up of satellite events. In addition I made sure the Apache Mahout people got a time and place to meet, I managed to review all proposals that sounded interesting at Strata London (great stuff on the business side of big data - go there if you want to learn more on the business side of the topics covered by Berlin Buzzwords and more). Everything else will have to wait at least until end next week.\nCU in Berlin - bring sun and warm weather with you :)\n"},{"id":130,"href":"/last-minute-getting-around-information-for-berlin-buzzwords260/","title":"Last minute Getting Around information for Berlin Buzzwords","section":"Inductive Bias","content":" Last minute Getting Around information for Berlin Buzzwords # I\u0026rsquo;ve been sharing information on how to get around in Berlin more often than I\u0026rsquo;d like to type it out - putting it here for future reference.\nBefore going to Berlin make sure to put an app on your phone that helps with finding the right public transport mix to use for going from one place to another:\nNokia Public Transit for WP7 phones\nÖffi for Android\nThere certainly is a comparable app for iPhones - put them in the comments and I\u0026rsquo;ll add them here.\nIf you want to get around for sight seeing - other than making sure to pack a travel guide consider renting a bike for a day or two. It\u0026rsquo;s rather safe to ride one in Berlin, there are several routes that are all green and calm. Checkout bbbike.de to plan your routes - though not the prettiest website it does have comprehensive information on road conditions and lets you avoid cobble stones or less well lit streets. Try it out\nit served me very well.\nTo actually rent a bike - ask your hotel, usually they have decent offers or can point you at a local bike shop that has rental offers. Prizes should be roughly 10,- Euros a day or 50,- a week.\nOne warning to pedestrians and anyone renting a car: Bicycles are very common in Berlin in particular in summer. Watch out when turning, don\u0026rsquo;t underestimate their speed. When walking on the sidewalks watch out for lanes reserved for bikes - usually they are red with white stripes but can look slightly different - see also some images on flickr. "},{"id":131,"href":"/teddy-in-zurich405/","title":"Teddy in Zürich","section":"Inductive Bias","content":" Teddy in Zürich # A few beautiful sunny though windy days in Zurich late April:\nView from the path between Ütliberg and Adliswil/Felsenegg:\nStrolling through the city and sitting next to Zürichsee enjoying the sun afterwards:\nA boat trip to Rapperswil - started cold and cloudy, finished warm and sunny:\n"},{"id":132,"href":"/teddy-in-poznan402/","title":"Teddy in Poznan","section":"Inductive Bias","content":" Teddy in Poznan # Some images taken in Poznan after GeeCon - big Thanks! to Dawid for giving advise on where to go for sightseeing, exhibitions and going-out.\nThe tour started close to river Warta - it being a sunny day it seemed like a perfect fit to just walk through the city, starting along the river headed towards the cathedral:\nAfter that Poznan Citadel was a great place to spend lunch time - sitting somewhere green and shady:\nAfternoon was dedicated to discovering the city center, several local churches and the national galery:\n"},{"id":133,"href":"/geecon-testing-hell-and-how-to-fix-it208/","title":"GeeCon - Testing hell and how to fix it","section":"Inductive Bias","content":" GeeCon - Testing hell and how to fix it # The last regular talk I went to was on testing hell at Atlassian – in particular the JIRA project. What happened to JIRA might actually be known to developers who have to deal with huge legacy projects that predate the junit and dependency injection era: Over time their test base grew into a monster that was hard to maintain and didn\u0026rsquo;t help at all with making developers confident on checkin time that they would not break anything.\nOn top of 13k unit tests they head accumulated 4k functional tests, several hundreds of selenium user interface tests in 65 maven modules depending on 554 dependencies that represented quite some technology mix from old to new, ranging across different libraries for solving the same task. They used 60+ remote agents for testing, including AWS instances that were orchestrated by a Bamboo installation, had different plants for every supported version branch, tested in parallel.\nMost expensive were platform tests that were executed every two to four weeks before each release – those tested JIRA with differing CPU configurations, JVMs, Browsers, databases, deployment containers. Other builds were triggered on commit, by dependencies or nightly.\nProblem was that builds would take for 15 min for unit tests, one hour for functional tests, several hours for all the rest – that means developers get feedback only after they are home essentially blocking other developers\u0026rsquo; work. For unit tests that resulted in fix turnaround times of several hours, for integration tests several days. Development would slow down, developers became afraid of commits, it became difficult to release – in summary morale went down.\nTheir problems: Even tiny changes caused test avalanches. As tests were usually red, noone would really care. Developers would not run tests for effort reasons and got feedback only after leaving work.\nSome obvious mistakes: Tests were separate from the code they tested – in their case in a separate maven module. So on every commit the whole suite has to run. Also back when the code was developed dependency injection only just started to catch up which meant the code was entangled, closely coupled and hard to test in isolation. There were opaque fixtures hard coded in xml configuration files that captured application scope but had to be maintained in the tests.\nTheir strategy to better testing:\nIntroduce less fragile UI tests based on the page objects pattern to depend less on the actual layout and more on the functionality behind.\nThey put test fixtures into the test code by introducing REST APIs for modification and an introduction of backdoors, only open in the test environment.\nFlickering tests were put to quarantine and either fixed quickly or deleted – if noone fixes them, they are probably useless anyway.\nAfter those simple measures they started splitting the software into multiple real modules to limit scope of development and raise responsibility of development teams. That comes with the advantage of having tests close to the real code. But it comes with the cost of a more complex CI hierarchy. However in well organised software in such a project hierarchy commits turned out to tend to go into leaves only – which did lessen the number of builds quite a bit.\nThere is a tradeoff between speed vs. control: Modularizing means you no longer have all in one workspace, in turn it means faster development for most of your tasks. For large refactorings noone will stop you to put all code in one idea workspace.\nThe goal for Atlassian was to turn the pyramid of tests upside down: Have most but fast unit tests, have less REST/html tests and even less Selenium tests. Philosophy was to only provide REST tests if there is no way at all to cover the same function in a unit test.\nIn terms of speeding up execution they started batching tests against one instance to avoid installation time, merged tests, used in-process databases, mocked IO and webservers where possible. Also putting more hardware in does help, so does avoiding sleeping in tests.\nIn terms of splitting code – in addition to responsibility that can also be done by maturity to keep what is evolving quickly close together until it is stable.\nThe day finished with a really inspiring keynote by Kevlin Henney on Cool Code – showing several both either miserably failing or incredibly cool pieces of software. His intention when reading code is to extend a coders vocabulary when it comes to programming. That\u0026rsquo;s why even the obfuscated c code competition does make for an interesting read as it tells you things about language features you otherwise might never have learned about before. One very important conclusion from his talk: “If you don\u0026rsquo;t have the time to read, you have neither time nor tools to write.” - though being made by Stephen King on literature this statement might as well apply to software, after all to some extend what we produce is some kind of art, is some kind of literature in it\u0026rsquo;s own right.\n"},{"id":134,"href":"/geecon-solr-at-allegro206/","title":"GeeCon - Solr at Allegro","section":"Inductive Bias","content":" GeeCon - Solr at Allegro # One particularly interesting to me was on Allegro\u0026rsquo;s (polish Ebay) Solr usage. In terms of numbers: They have 20Mio offers in Poland, another 10Mio active offers in partnering countries. In addition in their index there are 50Mio inactive offers in Poland and 40 Mio closed offers outside that country. They serve 8Mio updates a day, that is 100 updates a second. Those are related to start/end of bidding phase, buy now actions, cancelled bids, bids themselves.\nPer day they have 105Mio requests per day, on peak time in the evening that is 3.5k requests per second. Of those 75% are answered in less than 5ms, 90% in less than 20ms.\nTo achieve that performance they are using Solr. Coming from a database based system, going via a proprietary search product they are now happy users of Solr with much better customer support both from the community as well as from contractors than with their previous paid for solution.\nThe speakers went into some detail on how they solved particular technical issues: They had to decide to go for an external data feeder to avoid putting the database itself under too much load even when just indexing the updates. On updates they need to deal with having to reconstruct the whole document as updates for Solr right now mean deleting the old document and indexing the new one. In addition commits are pretty expensive, so they ended up delaying commits for as long as the SLA would allow (one minute) and committing them as batch.\nThey tried to shard indexes by category facetted by – that did not work particularly wrong as with their user behaviour it resulted in too many cross-shard requests. Index size was an issue for them so they reduced the amount of data indexed and stored in Solr to the absolute minimum – all else was out-sourced to a key-value store (in their case MongoDB).\nWhen it comes to caching that proved to be the component that needed most tweaks – they put a varnish in front (Solr speaks xml over http which is simple enough to find caches for) – in relation with the index delay they had in place they could tune eviction times. Result were cache hit rates of about 30 to 40 percent. When it comes to internal caches: High eviction and low hit rates are a problem. Watch the Solr Admin Console for more statistics. Are there too many unique objects in your index? Are caches too small? Are there too many unique queries? They ended up binding users to solr backends by having a routing be sticky with the user\u0026rsquo;s cookie – as users tend to drill down on the same dataset over and over again in their case that raised hit rates substancially. When tuning filter queries: Have them as independent as possible – don\u0026rsquo;t use many unique combinations of the same filtering over and over again. Instead filter individually to better use that cache.\nFor them Solr proved to be a stable, efficient, flexible, easy to monitor and maintain and change system that ran without failure for the first 8 months with the whole architecture being designed and prototyped (at near production quality) by one developer in six months.\nCurrently the system is running on 10 solr slaves (+ power backup) compared to 25 nodes before. A full index takes 4 hours, bottlenecked at the feeder, potentially that could be pushed down to one hour. Updates of course flow in continuously.\n"},{"id":135,"href":"/geecon-managing-remote-projects204/","title":"GeeCon - managing remote projects","section":"Inductive Bias","content":" GeeCon - managing remote projects # In his talk on visibility in distributed teams Pawel Wrzeszcz motivated why working remotely might be benefitial for both, employees (less commute time, more family time) as well as employers (hiring world wide instead of local, getting more talent in). He then went into more detail on some best practices that worked for his company as well as for himself.\nWhen it comes to managing your energy the trick mainly is to find the right balance between isolating work from private live (by having a separate area in your home, having a daily routine with fixed start and end times) and integrating work into your daily live and loving what you do: The more boring your job is, the less likely you are going to succeed when working remotely.\nThere are three aspects to work remotely successfully: a) having distributed meetings – essentially: minimize them. Have more 1 on 1 meetings to clear up any questions. Have technology support you where necessary (Skype is nice for calls with up to ten people, they also tried google hangouts, teamspeak and others. Take what works for you and your colleagues). b) For group decisions use online brainstorming tools. A wiki will do, so do google docs. There\u0026rsquo;s fancier stuff should you need it. Asynchronous brainstorming can work. c) Learn to value asynchronous communication channels – avoid mail, wikis, issue trackers etc. are much better suited for longer documentation like communication.\nEssentially what will happen is that issues within your organisation are revealed much more easily than working on-site.\n"},{"id":136,"href":"/geecon-failing-software-projects-fast-and-rapidly203/","title":"GeeCon - failing software projects fast and rapidly","section":"Inductive Bias","content":" GeeCon - failing software projects fast and rapidly # My second day started with a talk on how to fail projects fast and rapidly. There are a few tricks to do that that relate to different aspects of your project. Lets take a look at each of them in turn.\nThe first measures to take to fail a project are organisational really: Refer to developers as resources – that will demotivate them and express that they are replaceable instead of being valuable human beings.\nSchedule meetings often and make everyone attend. However cancel them on short notice, do not show up yourself or come unprepared.\nMake daily standups really long – 45min at least. Or better yet: Schedule weekly team meetings at a table, but cancel them as often as you can.\nAlways demand Minutes of Meeting after the meeting. (Hint: Yes, they are good to cover your ass, however if you have to do that, your organisation is screwed anyway.)\nPlans are nothing, planning is everything – however planning should be done by the most experienced, estimation does not have to happen collectively (that only leads to the team feeling like they promissed something), rather have estimations be done by the most experienced manager.\nControl all the details, assign your resources to tasks and do not let them self-organise.\nWhen it comes to demotivating developers there are a few more things than the obvious critizing in public that will help destroy your team culture:\nDon\u0026rsquo;t invest in tooling – the biggest screen, fastest computer, most comfortable office really should be reserved for those doing the hard work, namely managers.\nMake working off-site impossible or really hard: Avoid having laptops for people, avoid setting up workable VPN solutions, do not open any ssh ports into your organisation.\nDemand working overtime. People will become tired, they\u0026rsquo;ll sacrifice family and hobbies, guess how long they will remain happy coders.\nBlindly deploy coding standards across the whole company and have those agreed upon in a committee. We all know how effective committee driven design (thanks to Pieter Hintjens for that term) is. Also demand 100% test coverage, forbid test driven development, forbid pair programming, demand 100% Junit coverage. And of course check quality and performance as the very last thing during the development cycle. While at that avoid frequent deployments, do not let developers onto production machines – not even with read only access. Don\u0026rsquo;t do small releases, let alone continuous deployment.\nAs a manager when rolling out changes: Forget about those retrospectives and incremental change. Roll out big changes at a time.\nAs a team lead accept broken builds, don\u0026rsquo;t stop the line to fix a build – rather have one guy fix it while others continue to add new features.\nWhen it comes to architecture there are a few certain ways to project death that you can follow to kill development:\nEnforce framework usage across all projects in your company. Do the same for editors, development frameworks, databases etc. Instead of using the right tool for the job standardise the way development is done.\nEmploy a bunch of ivory tower architects that communicate with UML and Slide-ware only.\nRemember: We are building complex systems. Complex systems need complex design. Have that design decided upon by a committee.\nCommunication should be system agnostic and standardised – why not use SOAP\u0026rsquo;s xml over http?\nUse Singletons – they\u0026rsquo;ll give you tightly coupled systems with a decent amount of global state.\nWhen it comes to development we can also make life for developers very hard:\nDon\u0026rsquo;t establish best practices and patterns – there is no need to learn from past failure.\nWe need not definition of done – everyone knows when something is done and what in particular that really means, right?\nWe need not common language – in particular not between developers and business analysts.\nDon\u0026rsquo;t use version control – or rely on Clear Case.\nDon\u0026rsquo;t do continuous integration.\nHave no code ownership – in contrast have a separate module modified by a different developer and forbid others to contribute. That leaves us with a nice bus factor of 1.\nDon\u0026rsquo;t do pair programming to spread the knowledge. See above.\nDon\u0026rsquo;t do refactoring – rather get it right from the start.\nDon\u0026rsquo;t do non-functional requirements – something like “must cope with high load” is enough of a specification. Also put any testing at the end of the development process, do lots of manual testing (after all machines cannot judge quality as well as humans can, right?), post-pone all difficult pieces to the end, with a bit of luck they get dropped anyway. Also test evenly – there is no need to test more important or more complex pieces heavier than others.\nDisclaimer for those who do not understand irony: The speaker Thomas Sundberg is very much into the agile manifesto, agile principles and xp values. The fun part of irony is that you can turn around the meaning of most of what is written above and get some good advise on not failing your projects.\n"},{"id":137,"href":"/geecon-tdd-and-its-influence-on-software-design207/","title":"GeeCon - TDD and it's influence on software design","section":"Inductive Bias","content":" GeeCon - TDD and it\u0026rsquo;s influence on software design # The second talk I went to on the first day was on the influence of TDD on software design. Keith Braithwaite did a really great job of first introducing the concept of cyclomatic complexity and than showing at the example of Hudson as well as many other open source Java projects that the average and mean cyclomatic complexity of all those projects actually is pretty close to one and when plotted for all methods pretty much follows a power law distribution. Comparing the properties of their specific distribution of cyclomatic complexities over projects he found out that the less steep the curve is, that is the more balance the distribution is, that is the less really complex pieces there are in the code the more likely are developers happy with the current state of the code. Not only that, also that distribution would be transformed into something more balanced after refactorings.\nNow looking at a selection of open source projects he analyzed what the alpha of the distribution of cyclomatic complexity is for projects that have no tests at all, have tests and those that were developed according to TDD. Turns out that the latter ones were the ones with the most balanced alpha.\n"},{"id":138,"href":"/geecon-randomized-testing205/","title":"GeeCon - Randomized testing","section":"Inductive Bias","content":" GeeCon - Randomized testing # I arrived late during lunch time on Thursday for GeeCon – however just in time to listen to one of the most interesting talks when it comes to testing. Did you ever have the issue of writing code that runs well in your development environment but crashes as soon as it\u0026rsquo;s rolled out at customers only to find out that their Locale setting was causing the issues? Ever had to deal with random test failure because against better advise your tests did depend on execution order that is almost guaranteed to be different on new JVM releases?\nThe Lucene community has encountered many similar issues. In effect they are faced with having to test a huge number of different configuration combinations in order to make sure that their software runs in all client setups. In recent months they developed an approach called randomised testing to tackle this problem: Essentially on each run “random tests” are run multiple times, each time with a slightly different configuration, input, in a different environment (e.g. Locale settings, time zones, JVMs, operating systems). Each of these configurations are pseudo random – however on test failure the framework will reveal the seed that was used to initialize that pseudo random number generator and thus allow you to reproduce the failure deterministically.\nThe idea itself is not new: published in a paper by Ntafos, used in fuzzers to identify security holes in applications this kind of technique is pretty well known. However applying it to write tests is a new idea used at Lucene.\nThe advantage is clear: With every new run of the test suite you gain confidence that your code is actually stable to any kind of user input. The downside of course is that you will discover all sorts of different issues and bugs not only in your code but also in the JVM itself. If your library is being used in all sorts of different setups fixing these issues upfront however is crucial to avoid users being surprised that it does not work well in their setup. Make sure to fix these failures quickly though – developers tend to ignore flickering tests over time. Adding randomness – and thereby essentially increasing the number of tests in your testsuite – will add the amount of effort to invest in fixing broken code.\nDawid Weiss gave a great overview of how random tests can be used to harden a code base. He introduced the testframework written at carrot search that isolated the random test features: It comes with a RandomizedRunner implementation that can be used to subsitute junit\u0026rsquo;s own runner. It\u0026rsquo;s capable of tracking test isolation by tracking spawned threads that might be leaking out of tests. In addition it provides utilities for instance for creating random strings, locals, numbers as well as annotations to denote how often a test should run and when it should run (always vs. nightly).\nSo when having tests with random input – how do you check for correctness? The most obvious thing to do is when being able to check the exact output. When testing a sorting method, not matter what the implementation and the input is – the output should always be sorted, which is easy enough to check. Also checking against simpler, but maybe in practice more expensive algorithms is an option.\nA second approach is to do sanity checks: Math.abs() at least should always return positive integers. The third approach is to do no checking at all in some cases. Why would that help? You\u0026rsquo;d be surprised by how many failures and exceptions you get by actually using your API in unexpected ways or giving your program unexpected input. This kind of behaviour checking does not need any assertions.\nNote: Really loved the APad/ iMiga that Dawid used to give his talk! Been such a long time since I last played with my own Amiga\u0026hellip;\n"},{"id":139,"href":"/geecon-2012-part-1209/","title":"GeeCon 2012 - part 1","section":"Inductive Bias","content":" GeeCon 2012 - part 1 # Devoxx, Java Posse, Qcon, Goto Con, an uncountable number of local Java User Groups – aren\u0026rsquo;t there enough conferences on just Java, that weird programming language that “makes developers stupid by letting them type too much boiler plate” (Keith Braithwaite)? I spent Thursday and Friday last week in Poznan at a conference called GeeCon – there main focus is on anything Java, including TDD, Agile and testability. It\u0026rsquo;s all community organised – switching between Poznan and Krakow on a yearly basis, backed by two corresponding Java User groups with a clear focus on good speakers and interesting content: Really well done, wish they could have fit more talks into each of these days: Five tracks in parallel left one with just around 4 regular talks + keynotes each day. That does make for a very human start and end time – but it feels like there\u0026rsquo;s so much going on in parallel that most likely you miss some of the particularly interesting content. Looking forward to the videos!\nOne note: If you are ever invited as a speaker to GeeCon: Do accept! It\u0026rsquo;s really well organised, an incredibly friendly atmosphere, and a really tasty speaker\u0026rsquo;s dinner. One thing that caught me be surprise this morning: My room was all paid for even though I stayed longer and had offered to cover the additional nights myself - Thanks guys, you rock!\nWatch this space for more details on the talks in the coming days.\n"},{"id":140,"href":"/presentation-shortening328/","title":"Presentation shortening","section":"Inductive Bias","content":" Presentation shortening # In an effort to make more room for more talks in our schedule for this year\u0026rsquo;s Berlin Buzzwords we\u0026rsquo;ve asked quite a few people to shorten their presentation from 40min down to 20min. The thought behind it is to not only give more people a chance to talk on their work but also have those shorter talks focused down to the absolute essential information for people to learn.\nHowever I\u0026rsquo;ve seen people give awesome 45min presentations fail miserably when forced to cut down their talk - and have myself delivered a very weak presentation at a 5min Ignite presentation.\nAs a result I thought it might be a good idea to share some thoughts on how to go about shortening your talk and still deliver a convincing performance:\nFirst of all, don\u0026rsquo;t take your usual 40min talk and cut away slides. As obvious as it may seem that this will result in poor slides it\u0026rsquo;s still all too tempting to take a working long presentation and just throw away some content to make it shorter in time. What really happens however is that people either cut out the meat - which leaves you with a shallow brief introduction and not much else left - or the meat is left in with not much around to help listeners understand what the talk is all about. Also speakers might be tempted to leave well working jokes in: Don\u0026rsquo;t without thinking twice - there are things that do take long to prepare, if you cut away all preparation the fun is gone as well. Some people cut down demos to just briefly skip to the browser and than switch back to the slides - if you like the demo and think it\u0026rsquo;s worthwhile: Take your time to demo and shorten elsewhere. Noone benefits from briefly seeing a browser window with not much like an application in there.\nSo how to go about when asked to cut down your slides? First of all: Think about what is the main message that you want to deliver. What is the core piece of knowledge people should know when leaving your talk. From there build up your story and provide all the necessary detail for the audience to understand your talk.\nThat does not necessarily mean throwing out all greek symbols because math is just to hard to explain briefly - if they are needed, leave them in, take the time for explanation and build up equations as you go.\nAlso it doesn\u0026rsquo;t mean that you should cover the very basics only. Clearly label your talk as advanced whenever that is both appropriate and possible - build on your audience\u0026rsquo;s knowledge without repeating all nitty gritty details. It can help to openly ask at the beginning simple yes/no questions and ask people to raise their hands to find out whether they are familiar with a certain technology or not. Knowing your attendees background can save you a lot of time when preparing a talk.\nOne final piece of advise: There\u0026rsquo;s one book that once helped my a lot improve my own talks called Presentation Zen - if you don\u0026rsquo;t know it yet, it certainly is well worth reading.\nPS: Dear speakers, if you are reading this but have not yet fully read the speaker acceptance notification mail - please do so now - I promise it does contain information that is valuable for you to know in particular if your employer happens to sponsor your travel to the conference.\n"},{"id":141,"href":"/traveling-to-berlin-in-june-note-the-airport-changes413/","title":"Traveling to Berlin in June? Update: No airport changes!","section":"Inductive Bias","content":" Traveling to Berlin in June? Update: No airport changes! # Update: Seems like there won\u0026rsquo;t be any airport changes for Berlin Buzzwords: German article at Tagesspiegel on postponing airport opening.\nIf you are planning to travel to Berlin in June – e.g. to attend Berlin Buzzwords – note that there is a major change to airports happening on June 2nd:\nSaturday, June 2nd will be the last day, both Schönefeld Airport (SXF) as well as Tegel Airport (TXL) are going to be open. All planes departing TXL that day will arrive at SXF in the evening.\nThe morning after (Sunday, June 3rd) airport Berlin Brandenburg International (also known as BBI, IATA code BER) is going to open. This airport is located very close to Schönefeld, there will be trains and busses connecting it to the city.\nAirlines should handle this change transparently. However when arriving at TXL make sure to check which airport you are departing from to avoid ending up in front of closed doors ;) Also should you be arriving from the US keep in mind that there will be a few more direct connections to Berlin starting June 3rd – e.g. Air Berlin will offer multiple daily flights to and from New York and Miami.\nWhen travelling from the airport to the conference by public transport, keep in mind that for TXL you only need a ticket covering zones A and B – for SXF and BER your need to purchase a ticket that is valid for zones A, B and C.\nTravelling from TXL to the conference venue and speaker hotel by cab is roughly 30 Euros. For BER the fare is roughly 50 Euros.\n"},{"id":142,"href":"/berlin-buzzwords-schedule-online-book-your-ticket-now117/","title":"Berlin Buzzwords Schedule online - book your ticket now","section":"Inductive Bias","content":" Berlin Buzzwords Schedule online - book your ticket now # As of beginning of last week the Berlin Buzzwords schedule is online. The Program Committee has completed reviewing all submissions and set up the schedule containing a great lineup of speakers for this years Berlin Buzzwords program. Among the speakers we have Leslie Hawthorn (Red Hat), Alex Lloyd (Google), Michael Busch (Twitter) as well as Nicolas Spiegelberg (Facebook). Checkout our program in the online schedule.\nBerlin Buzzwords standard conference tickets are still available. Note that we also offer a special rate for groups of 5 and more attendees with a 15% discount off the standard ticket price. Make sure to book your ticket now: Ticket prizes will rise by another 100 Euros for last minute purchases in three weeks!\n“Berlin Buzzwords is by far one of the best conferences around if you care about search, distributed systems, and NoSQL\u0026hellip;” says Shay Banon, founder of ElasticSearch. Berlin Buzzwords will take place June 4th and 5th 2012 at Urania Berlin. The 3rd edition of the conference for developers and users of open source projects, again focuses on everything related to scalable search, data-analysis in the cloud and NoSQL-databases. We are bringing together developers, scientists, and analysts working on innovative technologies for storing, analysing and searching today\u0026rsquo;s massive amounts of digital data. Berlin Buzzwords is organised by newthinking communications GmbH in collaboration with Isabel Drost (Member of the Apache Software Foundation, PMC member Apache community development and co-founder of Apache Mahout), Jan Lehnardt (PMC member Apache CouchDB) and Simon Willnauer (Member of the Apache Software Foundation, PMC member Apache Lucene). More information including speaker interviews, ticket sales, press information as well as \u0026ldquo;meet me at bbuzz\u0026rdquo; buttons are available on the official Berlin Buzzwords website.\nLooking forward to meeting you in June.\nPS: Did I mention that Berlin is all beautiful in Summer?\n"},{"id":143,"href":"/berlin-hadoop-get-together-april-2012-videos-are-up119/","title":"Berlin Hadoop Get Together (April 2012)- videos are up","section":"Inductive Bias","content":" Berlin Hadoop Get Together (April 2012)- videos are up # Sebastian Schelter: Introducing Apache Giraph for Large Scale Graph Processing\nDr. Falk-Florian Henrich: Applying Compiler Technology to Event Stream Processing Dr. Mikio Braun: TWIMPACT: On Real-Time Twitter Analysis\n"},{"id":144,"href":"/second-steps-with-git362/","title":"Second steps with git","section":"Inductive Bias","content":" Second steps with git # Leaving this here in case I\u0026rsquo;ll search for it later again - and I\u0026rsquo;m pretty sure I will.\nThe following is a simplification of the git workflow detailed earlier - in particular the first two steps and a little background.\nWhen dealing with remotes the git remote documentation is very useful.\nWhen sharing your changes with others the git tutorial on sharing changes is very helpful.\nInstead of starting by cloning the upstream repository on github and than going from there as follows:\n#clone the github repository\ngit clone git@github.com:MaineC/mahout.git\n#add upstream to the local clone\ngit remote add upstream git://git.apache.org/mahout.git\nyou can also take a slightly different approach and start with an empty github repository to push your changes into instead:\n#clone the upstream repository git clone git://git.apache.org/mahout.git\n#add upstream your personal - still empty - repo to the local clone\ngit remote add personal git@github.com:MaineC/mahout.git\n#push your local modifications branch mods to your personal repo\ngit push personal mods\nThat should leave you with branch mods being visible in your personal repo now.\n"},{"id":145,"href":"/music-in-berlin-early-june289/","title":"Music in Berlin early June","section":"Inductive Bias","content":" Music in Berlin early June # A little bit of inspiration on what to do the weekend before and after Buzzwords in Berlin:\nBallet in June at the Staatsballett Berlin\nConcerts in June at \u0026lt;a href=\u0026ldquo; http://www.konzerthaus.de/programm/?datetimeAnf=1338501600\u0026datetimeEnd=1341093600\u0026id_language=1\u0026month=6\u0026year=2012\"\nKonzerthaus Berlin\nOpera in June at Staatsoper Berlin\nOpera in June at Deutsche Oper Berlin\nConcerts in June at the Philharmoniker Berlin\nOpera in June at Komische Oper Berlin\nTheater (German-only, sorry, but open-air) at Hexenkessel Hoftheater\nA summary of what else is available on Berlin stages in June: Berliner Bühnen\nOpera and ballet in Dresden in June at Semper Oper Dresden\nConcerts in Leipzig at Gewandhaus Leipzig (select June as month, interface isn\u0026rsquo;t restful unfortunately)\nWith just a tiny bit of luck there is no need to pre-book your tickets - in most cases there are several seats left even an hour before the official starting time. Pre-ordering tickets does have an advantage though when it comes to prizing. One easy way to get your ticket it to book via Eventim.\nIf you happen to be younger than thirty consider buying yourself a Classic Card - it costs 15 Euros but allows you access to several locations for 8 Euros only (no pre-booking, tickets can be purchased only an hour before the official start).\n"},{"id":146,"href":"/berlin-buzzwords-scheduling-behind-the-scenes118/","title":"Berlin Buzzwords scheduling - behind the scenes","section":"Inductive Bias","content":" Berlin Buzzwords scheduling - behind the scenes # Since roughly a week the Berlin Buzzwords schedule is available online. Tickets are still available at the regular rate - make sure to book your ticket now - you\u0026rsquo;ve got another three weeks to purchase tickets at the regular rate, last minute rate will up the prize by another 100 Euros starting May 20th.\nI thought it might be interesting to share some background on how Berlin Buzzwords scheduling worked out this year. We changed it quite a bit - adding more people to the conference committee, upping the acceptance rate while at the same time reducing speaking time for quite a few talks. This is to share some background information on some of the reasons and provide some detail on how rating was done.\nLet me first state some constraints: We are hosting the conference in a venue where we can have 3 tracks at most - there aren\u0026rsquo;t any other large rooms. We don\u0026rsquo;t want to do another round of well- or rather not-so-well-informed random guessing of which talks will be un-popular stashing them in the small room. Switching schedule during the conference itself really isn\u0026rsquo;t particularly professional nor is it very simple to do when you have to move about 200 people around to have them go to a different room than what the printed schedule says.\nWe are trying to keep the prize for the conference as low as possible to be able to attract the average developer who is not able to pay some 1.5k Euros to go to a conference. We are tech focused, no business involved - our attendees don\u0026rsquo;t have big budgets for travelling to expensive conferences. With current attendee numbers for each day every attendee has to pay roughly 50% of the current regular ticket prize to make the budget work out. That means two things: a) We need all of you to pay for all days to make the budget work. b) If you would like to add another conference day because talks are so interesting, add another 50% of the current ticket prize and decide whether you\u0026rsquo;d be willing to pay that extra money. c) Increasing the number of tracks obviously means increasing the ticket prize which we would rather avoid.\nBerlin Buzzwords was established as an event for professionals - quality of talks is high, attendees joining the conference know what they are talking about, we are happy to have students as well (did you notice there\u0026rsquo;s a student ticket?) However that focus means that we are different from pure-open-source-community events. If you think there is too few coverage on scalability topics at existing community-only events please talk to them to increase that coverage or lead the effort of establishing such an event yourself - that isn\u0026rsquo;t easy, but neither is it impossible. You could get started by hosting one of our meetups/workshops/hackathons - or alternatively run e.g. one of FOSDEM\u0026rsquo;s DevRooms.\nBuzzwords is organised by a team of several people. On the one hand there are volunteers (as in people not making a profit from the conference, working on it during working hours donated by their employer at best - Thank You Nokia**! Thank You Searchworkings!). They are familiar with what\u0026rsquo;s going on in the search/store/scale space - you can find them on the program committee page. All administrative work is being done by newthinking communications - they have people very dedicated to what they are doing (there\u0026rsquo;s even one girl who joined a Ruby-On-Rails getting started course last weekend to learn more on what Buzzwords people are working on*) - their main focus is that the whole conference runs as smoothly as possible.\nSome of the assumptions above mean that we have to limit the number of talks we accept. Acceptance rate of last year was roughly 30%. Doing that again this year would have meant sending out decline mails to quite a few vital developers - many of them committers on the project they were talking about. That\u0026rsquo;s not because the talks were bad or anything, it\u0026rsquo;s just that there were way too many good talks. So we did an experiment this year: We upped that acceptance rate to 50% - but in turn had to reduce the length of many of the talks that were submitted as 40min versions. The result was that in order to fit more talks into the same space and time we had to shorten quite a few submissions. I did a bit of math this morning, of those reduced to 20min we would have had to reject 70% had we gone with a different schedule format w/o shortening submissions.\nTalks selection was done according to a very simple algorithm: Each talk was reviewed by at least three members of our program committee. Talk to reviewer assignment was done according to a pseudo random number generator - more precisely this one. Reviewers assigned scores ranging from 5 (want to have and am going to fight for it) to 1 (don\u0026rsquo;t want to see and am going to fight against). After looking at the schedule constraints we decided to accept n talks in total, x of which would be 40min, y of which would be 30 and z of which would be 20.\nWe sorted all talks by mean score and selected the top n for acceptance. Of those we took the first x/3 tagged as search, x/3 tagged as scale and x/3 from store to be accepted as 40 min talks. Same was done for the 30 and for the 20min slots. A mixture sort, grep, awk, head, and cut was quite helpful here and gave us n - 2 talks accepted. In our list of scores the following 5 talks had equal score, so we chose 2 of those at (pseudo-) random. Finally acceptance notification were sent out (Thanks to the Python mail support - that made things easier!). We asked speakers to confirm that they would still be available. Most got back right away, about 12 needed another nag mail or sms a week later to actually confirm.\n\u0026lt; br\u0026gt;Scheduling itself was done in a purely analog way: Take a pen, write all n talks on little pieces of paper, add information on track and length. After that those pieces of paper were arranged into the pre-defined schedule grid on a kitchen table: Re-arranging paper is just so much faster than anything you can do digitally - if only it wasn\u0026rsquo;t for the creation of post-it notes beforehand ;)\nFinally the schedule went out earlier this week - together with an appropriate press release, tweet etc. Again Buzzwords is a two day only conference. Most likely we won\u0026rsquo;t grow the main conference beyond that any time soon. However in effect you yourself can extend that conference to any length you want. We have asked local companies to provide us with meeting space for at least 20 people each for free. We have several community members organise workshops, meetups, hackathons, code-retreats and barcamps in these areas already. If you think your topic is not covered well enough at the main conference, you\u0026rsquo;d like to learn more on a particular topic - please talk to us on how to organise one of those meetups yourself. You don\u0026rsquo;t need to talk there if you don\u0026rsquo;t want to - all you need to do is get an interesting schedule together that draws people to your meetup. Also if you think your talk should have been accepted - talk to us to get a meetup going on your topic and related themes to get them covered.\nThe main goal of Berlin Buzzwords is to involve you. We are very open to any ideas on how to collaborate or grow the conference. We do have several partner events throughout Europe this year. We offer companies the option to co-located and co-promote their trainings after Buzzwords. We offer community members the option to co-locate and co-promote their meetup with the conference. However we do need your time and dedication to make this work. Or to use a phrase that is well-known at least in the Apache world: Patches welcome!\n* Her conclusion: Even w/o prior coding knowledge the course was easy enough to follow and at least made clear to her the difference between frontend and backend work. Observation: Buzzwords is very clearly backend. :)\n** In particular Hannes Kruppa and the whole search recommendations team!\n"},{"id":147,"href":"/clojure-berlin-march-2012139/","title":"Clojure Berlin - March 2012","section":"Inductive Bias","content":" Clojure Berlin - March 2012 # In today\u0026rsquo;s Clojure meetup Stefan Hübner gave an introduction to Cascalog - a Clojure library based on Cascading for large scale data processing on Apache Hadoop without hassle.\nAfter a brief overview of what he is using the tool for to do log processing at his day job for http://maps.nokia.com Stefan went into some more detail on why he chose Cascalog over other project that provide abstraction layers on top of Hadoop\u0026rsquo;s plain map/reduce library: Both Pig and Hive provide easy to learn SQL-like languages to quickly write analysis jobs. The major disadvantage however comes when in need for domain specific operators - in particular when these turn out to be needed just once: Developers end up switching back and forth between e.g. Pig Latin and Java code to accomplish their analysis need. These kinds of one-off analysis tasks are exactly where Cascalog shines: No need to leave the Clojure context, just program your map/reduce jobs on a very high level (Cascalog itself is quite similar to datalog in syntax which makes it easy to read and simple to forget about all the nitty-gritty details of writing map/reduce jobs).\nWriting a join to compute persons\u0026rsquo; age and gender from a trivial data model is as simple as typing:\n;; Persons\u0026rsquo; age and gender\n(?\u0026lt;- (stdout)\n[?person ?age ?gender]\n(age ?person ?age)\n(gender ?person ?gender)\nMultiple sorts of input generators are implemented already: Reading text files, using files in HDFS as input are both common use cases. Of course it is possible to provide your own implementation for that as well to integrate any type of data input in addition to what is available already.\nIn my view Cascalog combines the speed of development that was brought by Pig and Hive with the flexibility of being able to seemlessly switch to a powerful programming language for anything custom. If you yourself have been using or even contributing to either Cascalog or Cascading: I\u0026rsquo;d love to see your submission to Berlin Buzzwords - remember, the submission deadline is this week on Sunday MEZ.\n"},{"id":148,"href":"/visiting-berlin-buzzwords-where-to-go-for-drinks-and-food430/","title":"Visiting Berlin Buzzwords - where to go for drinks and food","section":"Inductive Bias","content":" Visiting Berlin Buzzwords - where to go for drinks and food # There are literally hundreds of bars and restaurants in easy walking distance to the conference venue. And if that is now enough for you, hop on U-Bahn and head east to either Kreuzberg or Friedrichshain to find more. For inspiration check out Tip Berlin - they have a decent, reliable restaurant list.\nFor quick orientation: Berlin is no one city center but many districts that all have their own look and feel to them. Those most interesting for eating and drinking:\nSchöneberg is a bit more calm, well suited for eating out until late evening. The two areas that are most interesting are around Akazien-/Golzstr (head north from Hauptstraße up until Nollendorfplatz), Crellestraße, as well as the area around Bayrischer Platz.\nFriedrichshain is the area to go for drinks in the evening and to see the young, urban Berlin. Get lost in the famous \u0026ldquo;Simon-Dach\u0026rdquo; quarter (\u0026ldquo;Simon-Dach-Kiez\u0026rdquo; as we say in Berlin) with its cobble stone streets, wide sidewalks, bars, restaurants and cool little shops. If the weather is as nice as it has been on the weekend, it might be worth walking or cycling a little farther to Holzmarktstraße. Between the streets \u0026ldquo;An der Schillingbrücke\u0026rdquo; and \u0026ldquo;Michaeliskirchstr.\u0026rdquo; (see http://bit.ly/cNqLZq) there are a few really nice outdoor beach bars right on the banks of the River Spree.\nKreuzberg comes in at least two flavours: For coffee and food head over to Bergmannstraße, for drinks at night go see Oranienstraße, for young and vibrant head over to Wrangelstrasse (do not miss Heinz Minki, Freischwimmer and Club der Visionäre), for a relaxed \u0026ldquo;down by the river\u0026rdquo; evening head over to Maybachufer (do not miss Van Loon, also check out Bethanien close by).\nPrenzlauer Berg - young, family friendly, slowly being turned into a German Kleins tadt ;)\nMitte - a bit more fancy, gentrified, great if you love culture, museums, ballet, concerts. Remember to explore the city by boat. If you are hungry head over to Linienstrasse and explore the little streets around. There is tasty cheese fondue available at Nolas am Weinberg. Go dance at Clärchens Ballhaus, get a coffee and code while drinking at web2.0 cafe Sankt Oberholz.\nTwo special recommendations for breakfast:\nOn the weekend before the conference days are best started with a long and tasty brunch. My personal recommendation if you love tea is to head down to TTT - apart from serving best tea in town you can also get really tasty food there. And best of all, buy tea, tea cups and pots. I tend to take keynote speakers to that place - so far none has complained ;)\nAnother option is to start your day on top of Bundestag - enjoy the view of the city, take an audio, tour, eat breakfast in the Käfer Restaurant and maybe add a brief lecture on German legislation afterwards. Make sure to book about a month in advance!\nFor burgers there is no better place than Burgermeister in Kreuzberg. Best Falafel is on sale at Habibi. Judging on where to get the best ice cream actually is a bit harder: Aldemir is the location in Kreuzberg, Pinguin Club is the location in Schöneberg (Inka Eis beats that only if you are more for unusual types of ice cream), if you are in Mitte close to Brandenburger Tor consider visiting Der Eisladen - lots of different types and really tasty.\nWhen it comes to cocktails there are various locations - large and small that people tend to frequent. Some places to start and feel welcome: Salut, Green Door and Stagger Lee.\n"},{"id":149,"href":"/walking-through-berlin431/","title":"Walking through Berlin","section":"Inductive Bias","content":" Walking through Berlin # Ever made the mistake of booking a flight to a city and trying to decide on what to do only after you arrived? That type of planning does work for Berlin - though you may end up with quite a different schedule than originally intended.\nThe only thing that needs a bit of planning ahead (about a month) is visiting the Bundestag - fast way to discover it is to just go up to it\u0026rsquo;s dome. You can book a table at the restaurant up there if you want to have breakfast above Berlin. In addition the visitor service offers various presentations for free that can be booked from their web page.\nSome hints in addition to visiting a tourist information after your arrival:\nWhen I have guests I usually recommend to either buy a day (or week) BVG ticket - you can use public transport as often as you like with these tickets. That includes S-Bahn, tram, busses, tube and ferries (but not the tourist roundtrip boats with moderation). If you know you\u0026rsquo;ll be going to several museums, a Welcome ticket might be worth it\u0026rsquo;s prize. Alternatively just get a bike - unless you want to reach destinations outside the s-bahn-ring or want to visit in winter (don\u0026rsquo;t) all distances should be easy to do by bike. To plan your trips use bbbike.de - they know road conditions to e.g. let you exclude larger streets or prefer green routes.\nYour best bet to see most of the attractions for less than five Euro is to take the regular bus line 100 from the Bhf. Zoo train station down to Alexanderplatz and line 200 back. Though no audio guide is known to me there should be guides available for sale in local tourist information offices.\nFor guide books: Lonely Planet is a good start. If you speak German the city box might serve you well. It contains 30 cards with proposed walking tours including brief explanations. Also the book \u0026ldquo;Die schönsten Berliner Stadtspaziergänge\u0026rdquo; has been great to discover areas that are less known. The city has two bi-weekly magazines that feature lists of concerts (both modern and classical), exhibitions, markets and more: For one there is Zitty, the other one Tip Berlin. Both are quite good, which one to prefer depends on personal taste. In addition both publish restaurant guides, books on where to go shopping, special issues on where to go and what to do. In addition their online restaurant reviews are quite decent.\nTwo final hints: If you happen to know locals (or anyone who moved their a while ago) - make sure to ask them for recommendations. Also, try to stay at one of the many B\u0026amp;B locations - in general you host will know several local recommendations.\n"},{"id":150,"href":"/berlin-hadoop-get-together-videos-are-up120/","title":"Berlin Hadoop Get Together - videos are up","section":"Inductive Bias","content":" Berlin Hadoop Get Together - videos are up # Markus Andrezak on Queue management with Kanban:\nQueue Management in Product Development with Kanban - enabling flow and fast feedback along the value chain from David Obermann on Vimeo.\nMartin Scholl on Storm:\nOn Firehoses and Storms: Event Thinking, Event Processing from David Obermann on Vimeo.\nFabian Hüske on Stratosphere:\nLarge-Scale Data Analysis Beyond Map/Reduce from David Obermann on Vimeo.\n"},{"id":151,"href":"/apache-hadoop-get-together-february-201242/","title":"Apache Hadoop Get Together - February 2012","section":"Inductive Bias","content":" Apache Hadoop Get Together - February 2012 # Today the first Hadoop Get Together Berlin 2012 took place - David got the event hosted by and at Axel Springer who kindly also paid for the (soon to be published) videos. Thanks also to the unbelievable Machine company for the tasty buffet after the meetup. Another thanks to Open Source Press for donating three of their Hadoop books.\nToday\u0026rsquo;s selection was quite diverse: The event started with a presentation by Markus Andrezak who gave an overview of Kanban and how it helped him change the development workflow over at eBay/mobile. Being well suited for environments that require flexibility Kanban is well suited to decrease risk associated with any single release by bringing the number of features released down to an absolute minimum. At Mobile his team got release cycles down to once a day. More than ten times a day however aren\u0026rsquo;t unheard of as well. The general goal for him was to reduce the risk associated with releases by reducing the number of features released per release, reducing the number of moving parts in one release and as a result reducing the number of potential sources for problems: If anything goes wrong, rolling back is no issue - nor is narrowing down on the potential sources of bugs in the changed software that were introduced in that particular release.\nThis development and output focused part of the process is complemented by an input focused Kanban cycle for product design: Products are going from idea to vision to a more detailed backlog to development and finally live the same as issues in development itself move from Todo to in progress, under review and done.\nWith both cycles the main goal is to keep the number of items in progress as low as possible. This will result in more focus for each developer and greatly reduce overhead: Don\u0026rsquo;t do more than one or two things at a time. Only catch: Most companies are focused on keeping development busy at all times - their goal is to reach 100% utilization. This however is in no way correlated to actual efficiency: By having 100% utilization there is not way you can deal with problems along the way, there is no buffer. Instead the idea should be to concentrate on a constant flow of released and live features instead.\nNow what is the link of all that to Hadoop? (Hint: No, this is no pun on the project\u0026rsquo;s slow release cycle.) The process of Kanban allows for frequent releases, it allows for frequent feedback. This enables a model of development that starts out from a model of your business case (no matter how coarse that may be), start building some code, measure your performance with that code based on actual usage data and adjust the model accordingly. Kanban lets you iterate very quickly on that loop getting you ahead of competitors eventually. In terms of technology one strong tool in their toolbox to really do data analytics on their incoming data is to use Hadoop and scale up analysing business data.\nIn the second talk Martin Scholl started out by drawing a comparison from music vs. printed music sheets to the actual performance of musicians in a concert: The former represents static, factual data. The latter represents a process that may be recorded, but may not by copied itself as it lives by the interactions with the audience. The same holds true for social networks: Their current state and the way you look at them is deeply influenced by your way of interacting with the system in realtime.\nSo in addition to data storage solutions for static data, he argues, we also need a way to process streaming data in an efficient and fault tolerant way. The system he uses for that purpose is Storm that was open-sourced by Twitter late last year. Built on top of zeroMQ it allows for flexible and fault tolerant messaging. Example applications mentioned are event analysis (filtering, aggregation, counting, monitoring), parallel distributed rpc based on message passing.\nTwo concrete examples include setting up a live A/B testing environment that is dynamically reconfigurable based on it\u0026rsquo;s input as well as event handling in a social network environment where interactions might trigger messages being sent by mail and instant message but also trigger updates in a recommendation model.\nIn the last talk Fabian Hüske from TU Berlin introduced Stratosphere - an EU founded research project that is working on an extended computational model on top of HDFS that provides more flexibility and better performance. Being developed before the rise of Apache Hadoop YARN unfortunately essentially what they did was to re-implement the whole map/reduce computational layer and put their system into that. Would be interesting to see how a port to YARN performs and what sort of advantages it gives in production. Looking forward to seeing you all in June for Berlin Buzzwords - make sure to submit your presentation soon, call for presentations won\u0026rsquo;t be extended this year.\n"},{"id":152,"href":"/howto-meetups-in-berlin237/","title":"HowTo: Meetups in Berlin","section":"Inductive Bias","content":" HowTo: Meetups in Berlin # I get that question once in a while - and need the list below myself every now and then: How to actually setup a meetup in Germany. Essentially it all boils down to three questions: Which channels to use for PR? Where to do the meeting? What other benefits to offer to attendees?\nWhen it comes to PR there are several options: Announce the meetup on relevant mailing lists\nUse social networking sites relevant to your project - in Germany Xing works best, Twitter, Facebook, Linked.In and Google+ are other options\nAsk anyone you know personally for help with spreading the word\nIf you have one post information on your personal blog\nWhere to go for the meetup:\nThe venue usually is the biggest question mark. After deciding on how big you\u0026rsquo;d like to shoot for initially you can start looking for a location. For your first meetup don\u0026rsquo;t rent a room - with a bit of creativity there are lots of options that are free of charge. If you are a student or have active relations to any university going there usually is the cheapest and least complicated version.\nAnother option is to just book a table in a restaurant that has a reasonably large room. Simply choose your favourite one - knowing the owner helps in getting extra space.\nThird option is to go to any co-working space that also has a meeting area. In general they are very open to hosting community events - co-up Berlin, Betahaus are just two options.\nIf you are planning a less formal event, your local hacker spaces might be an option: c-base Berlin, in Berlin e.V. are two Berlin examples. Hackers Dojo and Noisebridge are two Bay Area examples.\nLast but not least look out for local startups that are currently hiring new people: They tend to be very open to hosting events. See Berlin Buzzwords Hackathon providers list for some examples.\nWhat else?\nMake sure attendees can register themselves - xing works for that, so do Google forms\nSetup a mailing list or some other notification service to help people track future events (Google Groups works, so does a dedicated Twitter Account)\nProvide some background online - meetup.com works but does charge a small fee. Setting up a blog on wordpress or blogger works as well, though it is not quite as interactive as the meetup.com site.\nGet in touch with attendees and local companies - usually they are quite happy to provide some financial support to your meetup for free drinks or videos.\nIf you want videos: Recording audio is trivial, putting it online is extremely simple if you use soundcloud\u0026rsquo;s app. Recording video also is rather simple but can be time consuming. Finding sponsors to pay for them if you offer to brand the videos is reasonably simple. For the Hadoop Get Together we usually hire Martin Schmidt. Sites to put videos online: Vimeo works but has rather low upload limits, blip.tv is a bit better in this respect.\nSponsoring in general: Companies looking for developers related to the meetup\u0026rsquo;s technology as well as those providing consulting for that technology tend to be open to supporting local events. What works best is to contact people you already know there - they will know best who to ask internally.\nOne final note: Being the organiser of such a meetup puts you at the center of a local community. Over time people will start remembering your face and name. Make sure you do the same - you should at least be able to remember faces, affiliations and names of your regular attendees.\n"},{"id":153,"href":"/happy-valentine234/","title":"Happy Valentine","section":"Inductive Bias","content":" Happy Valentine # Free Software developers can be very critical: Every single line of code gets scrutinized, every design is reviewed by several often opinionated people. Even the way communities are supposed to work sometimes gets restricted. Sometimes a simple Thank You can make all the difference for any contributor or committer.\nFSFE proposed a really nice campaign: Celebrate the \u0026ldquo;I love Free Software\u0026rdquo; - Day on February 14th. In the hope that some of the readers of this blog actively develop or contribute to free software projects - this is a thank you for you! It\u0026rsquo;s your contributions that make all the difference - be it code, documentation, help for users or code reviews.\n"},{"id":154,"href":"/february-14th-i-love-free-software-day170/","title":"February 14th: \"I love free software day\"","section":"Inductive Bias","content":" February 14th: \u0026ldquo;I love free software day\u0026rdquo; # This year FSFE is once again running their I love free software campaign on February 14th: The goal they put up is to have more love reports, hugs and Thank You messages sent out than bug reports filed against projects.\nThey have put online a few ideas on what to do that day. I\u0026rsquo;d like to add one additional option: If you are using any free software and you feel the urgent need to file a bug report on that day, use the opportunity to submit a patch as well: Make sure to not only describe what is going wrong but add a patch that contains a test to show the issue and a code modification that fixes the issue, is compatible with the project\u0026rsquo;s coding guidelines, doesn\u0026rsquo;t break anything else in the project. Any other contribution (documentation, increasing test coverage, help to other users) welcome as well of course.\n"},{"id":155,"href":"/note-to-self-java-heap-analysis295/","title":"Note to self - Java heap analysis","section":"Inductive Bias","content":" Note to self - Java heap analysis # As I keep searching for those URLs over and over again linking them here. When running into JVM heap issues (an out of memory exception is a pretty sure sign, so can be the program getting slower and slower over time) there\u0026rsquo;s a few things you can do for analysis:\nStart with telling the effected JVM process to output some statistics on heap layout as well as thread state by sending it a SIGQUIT (if you want to use the number instead - it\u0026rsquo;s 3 - avoid typing 9 instead ;) ).\nMore detailed insight is available via jConsole - remote setup can be a bit tricky but is well doable and worth the effort as it gives much more detail on what is running and how memory consumption really looks like.\nFor an detailed analysis take a heap dump with either jmap, jConsole or by starting the process with the JVM option -XX:+HeapDumpOnOutOfMemoryError. Look at it either with jhat or the IBM heap analyzer. Also netbeans offers nice support for searching for memory leaks.\nOn a more general note on diagnosing java stuff see Rainer Jung\u0026rsquo;s presentation on troubleshooting Java applications as well as Att ila Szegedi\u0026rsquo;s presentation on JVM tuning.\n"},{"id":156,"href":"/apache-mahout-06-released66/","title":"Apache Mahout 0.6 released","section":"Inductive Bias","content":" Apache Mahout 0.6 released # As of Monday, February 6th a new Apache Mahout version was released. The new package features\nLots of performance improvments:\nA new LDA implementation using Collapsed Variational Bayes 0th Derivative Approximation - try that out if you have been bothered by the way less than optimal performance of the old version.\nImproved Decision Tree performance and added support for regression problems\nReduced runtime of dot product between vectors - many algorithms in Mahout rely on that, so these performance improvements will affect anyone using them.\nReduced runtime of LanczosSolver tests - make modifications to Mahout more easily and have faster development cycles by faster testing.\nIncreased efficiency of parallel ALS matrix factorization\nPerformance improvements in RowSimilarityJob, TransposeJob - helpful for anyone trying to find similar items or running the Hadoop based recommender\nNew features:\nK-Trusses, Top-Down and Bottom-Up clustering, Random Walk with Restarts implementation\nSSVD enhancements\nBetter integration:\nAdded MongoDB and Cassandra DataModel support\nAdded numerous clustering display examples\nMany bug fixes, refactorings, and other small improvements. More information is available in the Release Notes.\nOverall great improvements towards better performance, better stability and integration. However there are still quite some outstanding issues and issues in need for review. Come join the project, help us improve existing patches, improve performance and in particular integration and streamlining of how to use the different parts of the project.\n"},{"id":157,"href":"/clojure-in-berlin140/","title":"Clojure in Berlin","section":"Inductive Bias","content":" Clojure in Berlin # Though I had the chance to tinker with some Clojure code only briefly it\u0026rsquo;s programming model and the resulting compact programs do fascinate me. As the resulting code runs on a JVM and does integrate well with existing Java libraries migration is comparably cheap and easy.\nToday I finally managed to attend the local Berlin Clojure meetup, co-organised by Stefan Hübner and Fronx. Timing couldn\u0026rsquo;t have been much better: In this evenings event Philip Potter from Thoughtworks introduced Overtone - a library for making music with Clojure.\nAfter installing and configuring jack for sound output, supercollider, and overtone outputting your first tone is as simple as registering the overtone library and typing\n(definst foo [] (saw 220))\n(foo)\nTo stop it type (stop).\nOther types of waves of course are supported as well, so is playing different waves simultaneously and modifying them at runtime. Also expressing sounds as notes (c, d, e, f, g) that may have a certain length is possible of course – which makes it so much easier to design music than having to thing in frequencies.\nA sample of what can easily be done with Overtone:\n\u0026lt;/iframe\nOriginal sound way better - this sample was taken with a mobile phone, compressed, re-coded and then put online. Checkout Overtone project for the real thing - and don\u0026rsquo;t even try to listen to the sample with low-end laptop speakers ;)\nOverall a well organised meetup (Thanks to Soundcloud for hosting it, to the organisers for putting it together and to the speaker for a really well done introduction to Overtone) and an interesting way to get started with Clojure with very fast (audio) feedback.\n"},{"id":158,"href":"/february-2012-apache-hadoop-get-together-berlin171/","title":"February 2012 Apache Hadoop Get Together Berlin","section":"Inductive Bias","content":" February 2012 Apache Hadoop Get Together Berlin # The upcoming Apache Hadoop Get-Together is scheduled for 22. February, 6 p.m. - taking place at Axel Springer, Axel-Springer-Str. 65, 10888 Berlin. Thanks to Springer for sponsoring the location!\nNote: It is important to indicate attendance. Due to security restrictions at the venue only registered visitors will be permitted. Get your ticket here: https://www.xing.com/events/hadoop-22-02-859807\nTalks scheduled thus far:\nMarkus Andrezak : \u0026ldquo;Queue Management in Product Development with Kanban - enabling flow and fast feedback along the value chain\u0026rdquo; - It\u0026rsquo;s a truism today that fast feedback from your market is a key advantage. This talk is about how you can deliver smallest product increments or MVPs (minimal viable products) quickly to your market to get fastest possible feedback on cause and effect of your product changes. To achieve that, it helps to provide a continuous deployment infrastructure as well as all you need for A/B testing and other feedback instruments. To make the most of these achievements, Kanban helps to limit work in progress, thus manage queues and speed up lead times (time from order to delivery or concept to cash). This helps us speed through the OODA Loop, i.e. Eric Ries\u0026rsquo; (The Lean Startup) Model -\u0026gt; Build -\u0026gt; Code -\u0026gt; Measure -\u0026gt; Data -\u0026gt; Validate -\u0026gt; Model. The more we can go through the loop, the more we have a chance to fine tune and validate our model of the business and finally make the right decisions.\nMarkus is one of Germany’s leading Kanban practitioners - writing and presenting talks about it in numerous publications and conferences. He will provide a brief view into how he is achieving fast feedback in diverse contexts.\nCurrently he is Head of mobile commerce at mobile.de.\nMartin Scholl : \u0026ldquo;On Firehoses and Storms: Event Thinking, Event Processing\u0026rdquo; - The SQL doctrine is still in full effect and still fundamentally affects the way software is designed, the state it is stored in as well as the system architecture. With the NoSQL movement people have started to realize that the manner in which data is stored affects the full stack \u0026ndash; and that reduction of impedance mismatch is a good thing(TM). \u0026ldquo;Thinking in events\u0026rdquo; follows this tradition of questioning what is state-of-the-art. Modeling a system not in mutable entities (as with data stores) but as a stream of immutable events that incrementally modify state, yields results that will exceed your expectations. This talk will be about event thinking, event software modeling and how Twitter\u0026rsquo;s Storm can help you process events at large.\nMartin Scholl is interested in data management systems. He is also a Founder of infinipool GmbH.\nFabian Hüske : \u0026ldquo;Large-Scale Data Analysis Beyond Map/Reduce\u0026rdquo; - Stratosphere is a joint project by TU Berlin, HU Berlin, and HPI Potsdam and researches \u0026ldquo;Information Management on the Cloud\u0026rdquo;. In the course of the project, a massively parallel data processing system is built. The current version of the system consists of the parallel PACT programming model, a database inspired optimizer, and the parallel dataflow processing engine, Nephele. Stratosphere has been released as open source. This talk will focus on the PACT programming model, which is a generalization of Map/Reduce, and show how PACT eases the specification of complex data analysis tasks. At the end of the talk, an overview of Stratosphere\u0026rsquo;s upcoming release will be given.\nFabian has been a research associate at the Database Systems and Information Management (DIMA) group at the Technische Universität Berlin since June 2008. He is working in the Stratosphere research project, focusing on parallel programming models, parallel data processing, and query optimization. Fabian started his studies at the University of Cooperative Education, Stuttgart, in cooperation with IBM Germany in 2003. During that course, he visited the IBM Almaden Research Center in San Jose, USA, twice and finished in 2006. Fabian undertook his studies at Universität Ulm and earned a master\u0026rsquo;s degree in 2008. His research interests include distributed information management, query processing, and query optimization.\nA big Thank You goes to Axel Springer for providing the venue at no cost for our event and for paying for videos to be taped of the presentations. A huge thanks also to David Obermann for organising the event.\nLooking forward to seeing you in Berlin.\n"},{"id":159,"href":"/dorkbot-berlin163/","title":"Dorkbot Berlin","section":"Inductive Bias","content":" Dorkbot Berlin # c-base - 8p.m. on a Monday evening - the room is packed (and pretty cloudy as well): Time for Dorkbot, a short series of talks on \u0026ldquo;People doing strange things with electricity\u0026rdquo; hosted by Frank Rieger.\nFirst talk up on stage was Gismo on Raumfahrtagentur - a Berlin maker-space located in Wedding. Originating from the presenter\u0026rsquo;s interest in electrical bikes a group of ten people interested in hardware hacking got together. Projects include but are not limited to 3D printing, 3D scanning, textile hacking, a collaborative podcast. Essentially the idea is to provide room and infrastructure to be used collaboratively by a group of members. From an organisational point of view the group is incorporated as a GmbH - however none of the projects is mainly targeted to commercialization: It\u0026rsquo;s main target group are hobbyists, researchers and open hardware/software people. If interested: Each Monday evening there is a \u0026ldquo;Sunday of the Kosmonauts\u0026rdquo; where externals are invited to come visit.\nSecond talk was on the project Drinkenlights (Klackerlaken) - a way for children to learn the basics of electronics without any soldering (hardware available for three Euros max). Experiences made with giving the ingredients for creating these toys to children of varying ages were interesting: From kids of about five years playing around up to ten/eleven year olds that when in school seemingly had to re-learn being creative without being given much direction or instruction on the task at hand.\nIn the third talk Martin Kaltenbrunner introduced his Tworse Key - a nice symbiosis of old technology (a morse key) and new media (Twitter). Essentially built on top of an Arduino Ethernet board it made it possible to turn morse messages into Tweets. Martin also gave a brief overview of related art projects and briefly touched upon the changes that open source and open hardware bring to art: There are projects that open all design and source code to the public to benefit from a wider distribution channel (without having to actually produce anything), working on designs in a collaborative way and get improvements back to the original project. All of these form a stark contrast to the existing idea of having one single author whose contribution is to build a physical object that is then presented in exhibitions - providing both, new possibilities and new challenges to artists.\nIn the last presentation Milosch introduced his new project ETIB whose goal it is to bring hardware hacking geeks together with textile geeks to work on integrating circuits into clothes.\nIf you are interested in hacking spaces in general and what is happening in that direction in Berlin, mark this Friday in your calendar: c-base will be hosting a Hackerspace meetup - so if you want to know how hackerspaces work or want to create one yourself, this event might be interesting to you.\n"},{"id":160,"href":"/scrumtisch-berlin357/","title":"Scrumtisch Berlin","section":"Inductive Bias","content":" Scrumtisch Berlin # After quite some time off I went to the Scrumtisch Berlin. The event was incredibly well visited - roughly 50 people filled the upper floor at Cafe Hundertwasser. Today\u0026rsquo;s event was organised such that participants first collected discussion topics, prioritised them together and then discussed the top three items in a timebox of 15 minutes each.\nTopics collected were:\nBest tricks to make teams self organised (20 votes)\nWhat is QA doing fulltime in a team (13 votes)\nOps and planning in a team (15 votes)\nPO disappears and takes backlog and vision with him - what now? (7 votes)\nWorking with non-Software teams (17 votes)\nPimp up my retrospective (12 votes)\nMultiple teams on one projects and vice versa (4 votes)\nIBM doing Scrum/ massive Scrum (9 votes)\nFeature knowledge vs market knowledge - what is more important in a PO if you have to choose due to people constraints (9 votes)\nHow to convince a team to do more (3 votes)\nWhy is agile good (10 votes)\nCompared to previous meetings quite some topics repeat. About half of the attendees were there for the first time - so it seems there is a common set of questions people usually run into when rolling out Scrum.\nSelf organising teams\nSeems like this is one of the most common questions run into when rolling out Scrum - how to really get to self organising teams. The question can be answered from two positions: What are the pre-requisites it takes to enable teams to become self organising? How to actually transform teams that are used to a command and control structure and are reluctant to transform?\nThe discussion, mainly led by Andrea Provaglio, CSM trainer focussed mainly on the first part of the question. Even when limiting discussion that way, the answer will still depend heavily on the organisation structure, number of management levels, team sizes.\nMarion made the topic a bit more concrete: Given the flexible vacation planning approach of Netflix, her question was whether that sort of loose approach could work in a typical German company (after all we have 30 instead of \u0026lt;20 vacation days, we have fixed holidays, she as a C?O of course wants to avoid customers being left alone when the whole team is on vacation.) Andrea re-phrased agility a bit here. His proposal was to not allow people to take their time off just anytime but to give them the freedom to figure out when to take time off. He identified five principles for leadership:\nClearly setting a goal (in that case: Everyone needs to have a vacation plan at a given date.)\nProvide the team with all resources, information and with the environment they need to accomplish their task.\nDefine constraints (\u0026ldquo;there must be at least one guy in the office on any given working day\u0026rdquo;)\nCheck back regularly\nMake yourself available to answer any questions\nThe discussion on teams reluctant to adopt self organisation was separated out and deferred. His point was mainly about enabling and encouraging self organisation. Enforcing self organisation however is not possible.\nScrum in non-Software teams\nThough phrased very broadly the topic quickly turned into a \u0026ldquo;how to do Scrum for hardware\u0026rdquo; discussion. Main problem here is that the further down you go the longer design generally takes. Even just routing lines on one decently sized circuit board can take several weeks. Mainly three possible ways out of the problem emerged from the discussion:\nLoosening the definition of done - \u0026ldquo;potentially shipable\u0026rdquo; may not mean sellable or even really shipable. I don\u0026rsquo;t think one should go down that slippery path. Only by actually shipping can I get the feedback I need to improve my design. So instead of loosening the definition of done we should instead start thinking about ways to get faster feedback, reduce risk and introduce shorter iterations.\nAnother way is to look for ways to reduce iteration length, even though we might not get down to software release cycles, and align releases such that integration can happen earlier.\nThe third way out could be to realize that maybe Scrum does not quite fit that way of working and use a different process instead that still provides the transparency and fast feedback that is needed (think Kanban).\nOverall the most important result of the discussion was that within 15min discussion the issues cannot be solved. After all the solution will depend on what exactly you are working on, who your suppliers are and what your team looks like. Most important is to recognize that there is a problem and to work on removing that impediment - most important is to identify issues and to improve your process.\nOperations and planning in Scrum\nThe last question discussed involved operations and Scrum planning: Given a team that does software development but is interrupted frequently with production issues - how should they work in a Scrum environment.\nThere are multiple facets to that problem: When it comes to deciding whether to deal with something immediately or not it makes sense to weigh size of the issue against amount of work it takes to resolve it. \u0026ldquo;Getting things done\u0026rdquo; states that the minimum size of an issue to deal with instantly is 2min of work. Issue with that is that the assumption of GTD was that issues flow into and inbox that is dealt is when there is time. In production environments however these issues usually trickle in instantly interrupting developers over and over again incurring a huge cost due to task switching.\nOne way out might be to have an event queue and assign developers (on a rotating basis) to deal with the issues and leave time for others to work in a focused way. Make sure to rotate frequently instead of by sprint - otherwise you run into the problem of making the team unstable thus delivering no stable amount of business value each sprint.\nAnother obvious way is to account for frequent interruptions and include a buffer for those in your plan. The most important benefit of that approach is to make the cost of this working mode clearly visible to management - leaving the decision how to deal with it up to them.\nOther simple fixes include introducing some level of indirection between the actual developer and the customer raising the issue, documenting solutions as well as incoming issues for better visibility, introducing a single point of contact capable of prioritising.\nComing back to vanilla Scrum however there is one interesting observation to be made: The main contract with iterations is for developers to be able to work in a focused way. Instead of having their tasks switched each day they are promised a fixed period of time to solve a given set of stories. In the end a sprint is a compromise between what management may need (change their mind on what is important frequently) and what developers need (working on a set of defined tasks not interrupted by re-priorisation). If the assumption of focus does not longer hold true, Scrum might be the wrong model. If what needs to be done changes daily, Kanban again might be the better option. Still making sure that the cost of task switching is visible is vitally important.\nTo sum up a very interesting Scrumtisch - in particular as agile methods really seem to become more and more common also in Berlin. Speaking of challenges: As user groups grow sometimes their character changes as well, in particular when built around participation and discussion. It will be interesting to watch Scrumtisch deal with that growth. Maybe at some point splitting the audience and having separate breakout sessions might make sense. Admittedly I\u0026rsquo;d also love to know more on the background of the audience: How many are actually using Scrum in the trenches vs. teaching Scrum as coaches? How long have they been using Scrum? In what kind of organisation? Maybe a topic for next time.\n"},{"id":161,"href":"/teddy-in-chicago396/","title":"Teddy in Chicago","section":"Inductive Bias","content":" Teddy in Chicago # Last week I spent several days in Chicago mainly to attend a few meetings at the local Nokia/Navteq office. Though the schedule was pretty packed, a few hours remained to explore the then frosty and windy city:\nTop three images: Some impressions of the city. Bottom left: Teddy\u0026rsquo;s new friend. Bottom right: Situation at ORD when flying out - fortunately both, the airport as well as the airline (Swiss) have quite some experience with challenging weather conditions so that we could leave without too much delay. As usual I wondered whether there are any Apache people close by. So before flying in I checked our committers map. As there were a few people in that general area I sent a brief heads-up to the greatly under-advertised, private, non-archived, committers only list p arty@apache.org. In case you\u0026rsquo;ve never heard about it: The main use case of that list is to provide a means for committers to arrange for meeting up with fellow Apache people and share travel details.\nAs a result I received a brief list of things to do in Chicago and got to attend a small but really nice meetup. Having a means to get in touch with locals can make such a difference - thanks for the warm welcome! Hopefully next time I\u0026rsquo;m there weather is as warm - would love to explore the (at least according to my travel guide book) beautiful nature of the great lakes.\n"},{"id":162,"href":"/scrum-done-wrong-scrum-done-wrong-scrum-done-wrong343/","title":"Scrum done wrong","section":"Inductive Bias","content":" Scrum done wrong # “Agile and Lean have a single purpose: to continually challenge the status quo. If you’re not doing that, you’re probably an impediment to it.” agile42.com\nJudging from the way some people become overly careful when discussing agile in general and Scrum in particular in my presence I seem to slowly have built up a reputation for being a strong proponent of these methods. Given the large number of flaky implementations as well as misunderstandings it seems to have become fashionable to blame Scrum for all badness and dismiss it altogether - up to the point where developers are proud to finally having abandoned Scrum completely - so that now they can work in iterations, accept new tasks only for upcoming but not for the current iteration, develop in a test-driven way, have daily sync meetings, mark tasks done only when they are delivered to and accepted by the customer, have regular “how to improve our work” meetings, estimate tasks in story points and only plan for as much work per iteration as was done in the past iteration\n… my personal take on that: Add in regular releases and you end up with a pretty decent scrum/agile implementation, no matter what your preferred name for it may be. Just for clarification: Though very often I write about what I call Scrum, I don’t use that particular method just because it is the latest fashion. It simply is a tool that has served me well on multiple occasions and given me working guidelines when I had no idea at all what software development in a professional setting should look like.\nSo where does all that friction with anything Scrum, agile, lean or whatever you call it come from? Recently I came across a blog post that jillesvangurp.com nicely identified some grave issues with current Scrum adoption. Unfortunately the blog post only lists the failures without going into a deeper analysis of the actual defects causing those failures.\nFirst of all, lets assume as working hypothesis that Scrum in itself does not solve any issues in your organisation but is a great tool to uncover deficiencies. The natural conclusion should be to use it as a tool to discover problems, but search for solutions for these problems elsewhere.\nWith that hypothesis, lets discern the the issues discussed in the post above and assign them to one of three defect categories.\nCategory one: Issues with the team\nProblem: You have a team of all-junior developers, or of all-mediocre developers.\nGoal: Turn them into a high performing team.\nSolution: Imagine you were not using Scrum at all, what would be the ideal solution? Well the obvious route probably is to re-adjust the team, add several seniors so that you end up with the right mix of people that have experience and share a vision - juniors than can learn and adapt what works from them.\nComparing that to our hypothesis: Scrum is all for short delivery cycles. You will uncover teams that perform badly much faster than in methods with longer iteration periods. So it should be reasonably simple to figure out teams that have a dysfunctional configuration. Changing that configuration however no longer is dictated by Scrum.\nCategory two: Bugs introduced during Scrum roll-out\nThe failures discussed in the blog post include people following Scrum mechanically: Only because your developers are moving post-it notes from left to right does not mean they are doing anything agile. It’s perfectly possible to do waterfall in Scrum. Whether that helps solve any of your issues is a different matter.\nInstead of mechanically going through the process what is more important is to understand the reasons and goals of each of the pieces that form Scrum. To make a rather far fetched comparison: When introducing Scrum without a deep understanding of why each piece is done, what you end up with is people following that process without understanding the meaning of each step. They end up mimicking behaviour without knowing the real world: To some extend seeing only the shadows of good development patterns without understanding the real items producing these shadows.\nAs a general rule: Understand why there is a retrospective meeting, remember why you need estimations, think about why there are daily stand-ups (instead of weekly meetings, instead of daily sit-togethers, instead of hourly stand-ups). Figure out why there is a product owner, what the role of a scrum master does. Pro-Tip: As soon as you really have understood Scrum, you don’t need a checklist of all meetings to hold for a successful iteration - they will just fit in naturally. If they don’t, you are probably missing an important piece of the puzzle - rather than rely on a pre-fabricated checklist, go bug your trainer or coach with questions to better understand the purpose of all the different bits and pieces. One very grave bug on roll-out is the general believe that Scrum is equal to a little bit of fairy dust you spread over your teams and all problems will automatically be solved afterwards. It is not - it’s not a silver bullet, it’s not fairy dust, it’s no magic - such things exist in fairy tales but have been seen nowhere in the real world. According to our working hypothesis above however Scrum does something really magical: By shortening delivery cycles it introduces short feedback loops which make it very easy to uncover problems in your organisation way faster than people are able to cover them up and hide them. Finding a solution on the other hand is still up to you.\nThe last roll-out issue mentioned is that of crappy certification - current certification programs are designed such that the naive organisation may believe that after two days of training their employers will magically turn into super heroes. Guess what - as with any certification training is just the very first step. Actual understanding comes with experience. Compare that to learning to drive: Only because you managed to get a drivers license does not turn you into a formula one winner. Instead that requires a whole lot of training. Same applies for any Scrum Master or Product Owner.\nCategory three: Organisation specifics\nAll other issues with Scrum mentioned in the blog post are either specific to the broken structures in the organisation under investigation or due to general Scrum mis-conceptions. Leaving these aside here.\nTo sum up: Scrum to me is nothing but a term to summarize good, proven development practices. I don’t care how you name them - however having any one name that is well defined makes it way easier to communicate. Scrum is not silver bullet - it does not solve all the issues your organisation may have. However it is a very effective debugger uncovering the issues employees and managers are trying to cover up. If you know all those issues very precisely already or you are certain that you don't have any, chances are you don't need Scrum.\n"},{"id":163,"href":"/reasons-for-you-to-visit-berlin-buzzwords329/","title":"Reasons for you to visit Berlin Buzzwords","section":"Inductive Bias","content":" Reasons for you to visit Berlin Buzzwords # I\u0026rsquo;ve heard of several people who are not quite sure yet whether they should visit Berlin Buzzwords or not - in particular when having to travel far and cross 9 time zones to attend. My general recommendation is to plan to spend some more days in Europe. The conference is conveniently scheduled on Monday and Tuesday which gives you one weekend before to explore the city and the whole week afterwards to go and see more either in the city or around.\nIn case you are wondering whether the city is a worthy destination when travelling with children - below is a list of things to do and places to go I sent to someone recently. Hope it helps with your decision as well. In general the city is pretty green, there are several locations specially amenable to a visit with kids - so treat the list below as what it is: An incomplete listing of some of the most obvious locations that might be of interest collected by someone who knows a few parents and their children. Also in case you speak German make sure to check out one of the many guide books for Berlin with children available in local book stores - Dussmann and Hugendubel generally have the largest selection though Chatwins is my preferred one for anything about travelling.\nIn the city\nIn case of good weather:\nTierpark Berlin - make sure you visit Tierpark (not Zoo) - it\u0026rsquo;s much larger and friendlier. See also images taken by Berlin Buzzwords fotographer Philipp, general images.\nThere is a huge park in walking distance of Brandenburger Tor: Tiergarten for recreation after sight seeing.\nFor swimming head over to either Wannsee or Schlachtensee\nFor exploring a NSA listening station head west to Teufels-berg\nOn warm evenings plan for some time at Maybachufer\nFor bad weather:\nIf your kids like tech go to Technik Museum (it features one of the first computer (the one built by Zuse that is))\nIf you kids like nature go to Naturkunde Museum\nIf you are interested in science - make sure to be here for the long night of science (web page may need google translate unless you speak German.)\nFor a city tour check out the following scribbles - they also include some interesting parts of the bus line 100 and 200\nClose to the city:\nIf you have some more time to spend make sure you also explore the closer surroundings:\n80km north: rent a canoo and explore Mecklenburg\n200km north: visit Rügen, spend some time swimming, some time to see the amazing chalk cliffs, some time to see the isle by bike\n250km south: go hiking or rafting in Elbsandsteingebirge\n80km south: rent a canoo and explore the canals in Spreewald\nRecommendations from friends\nDawid Weiss: Badeschiff - a pool-on-the-river thing. It\u0026rsquo;s not something you get in any ordinary city :)\nSteve Loughran: My son\u0026rsquo;s favourite part of a trip to berlin (age 9) was actually the Bauspielplatz: Smaller kids get a play area where they can use the sand + water to build streams, dam them and generally make a mess, while the 8+ get a playground where they actually help build it under adult supervision. They also run a good open air waffle/pancake/coffee shop. They\u0026rsquo;re open in the afternoons.\nHope to see you in Berlin in June. If you need more information or recommendations don\u0026rsquo;t hesitate to ask. "},{"id":164,"href":"/berlin-buzzwords-2012-call-for-submissions116/","title":"Berlin Buzzwords 2012 - Call for submissions","section":"Inductive Bias","content":" Berlin Buzzwords 2012 - Call for submissions # The countdown started several weeks ago - finally in the past days the date for Berlin Buzzwords was announced, the call for submissions published. It\u0026rsquo;s exciting to see that the first talk is in already. Looking forward to yours.\nCompared to last year there are two changes: Submissions are no longer evaluated by Jan, Simon and myself only. Due to the large number of talks submitted last year we reached out for help to be able to split the task of reviewing talks.\nAlso the conference itself grew quite a bit in the past two years. As a result it now takes several full time positions to handle not only ticketing, hosting and software development, sponsorships, venue management, travel support, but also external communication and marketing. The team of newthinking grew quite a bit and is helping substantially with tasks that before were handled by Jan, Simon and myself exclusively to keep some of our time reserved for the fun part of schedule curation. Please make sure to include info@berlinbuzzwords.de if you have questions that need a quick answer.\nWe are looking forward to a successful community conference on all things scalable - be it search, NoSQL or data analytics. Don\u0026rsquo;t be afraid to submit highly technical talks - Berlin Buzzwords always has been a place for developers to discuss new technologies, algorithms and implementations.\nIf your community need more than just a day to meet - please do talk to us. We will be providing room for meetups on Wednesday after the conference. Those are handed out on a first come first serve basis. If you are a local Berlin company and want to get Berlin Buzzwords into your offices, please talk to us - we are more than happy to get you in touch with one of the meetup organisers.\nIf you would like to co-locate trainings with Berlin Buzzwords - we are happy to co-promote you event. Talk to us to be included in our official schedule. In case you need any help organising your training, newthinking will be more than happy to provide their services for your event.\nLooking forward to June: It\u0026rsquo;s amazing how large that event grew in the past two years - and almost scary to return back online after a flu and see how things unfolded magically.\n"},{"id":165,"href":"/one-day-later312/","title":"One day later","section":"Inductive Bias","content":" One day later # "},{"id":166,"href":"/fun-little-new-toy202/","title":"Fun little new toy","section":"Inductive Bias","content":" Fun little new toy # Yesterday Thilo invited me to attend an \u0026ldquo;Electronics 101\u0026rdquo; workshop including an introduction to soldering that was scheduled to start at 7p.m. this evening at the offices of IN-Berlin e.V.. As part of my studies back in university I do have a little bit of background in Electronics, but never before had tried any serious soldering (apart from fixing one of our audio cables) so I thought, why not.\nThe workshop turned out to be a lot of fun: The organisers Mitch Altman and Jimmie Rodgers had brought several pre-packaged kits for people to work on. Quite a few of them based on Arduino so after putting them together you can actually continue having fun with writing little programs. After giving a brief but very well done, easy to understand introduction to digital electronics Mitch showed attendees how to use a soldering iron (make sure to check out his comic \u0026ldquo;soldering is easy\u0026rdquo; if you want to know more) and got everyone started. Both Jimmie and Mitch did a great job answering any upcoming questions, fixing issues and generally helping out with any problems. Even those that never used a soldering iron before quickly got up to speed and in the end went home with that nice experience of having built something that you cannot only program but can touch and hold in your hands.\nI got myself a LoL shield (still to be done), and a Diavolino. Still missing is the FTDI TTL-232R cable for getting the device hooked up to our laptops and be able to re-program it (though most likely that will be easier to find than a \u0026gt;1G Ohm resistor Thilo is looking for to be able to calibrate his Geiger counter).\nResults of my first session are below:\nThe boardFirst pins attachedLast pins attached\nAlso thanks to Sven Guckes organising and announcing this workshop on short notice. And thanks to Thilo for talking me into that.\nUpdate: Images of the event are available online.\n"},{"id":167,"href":"/talking-people-into-submitting-patches-results389/","title":"Talking people into submitting patches - results","section":"Inductive Bias","content":" Talking people into submitting patches - results # Back in November I gave a talk at Apache Con NA in Vancouver on talking friends and colleagues into contributing patches to open source projects. The intended audience for this talk were experienced committers to Apache projects, the goal was to learn more on their tricks for talking people into patching. First of all thanks for an interesting discussion on the topic - it was great to get into the room with barely enough slides to fill 10 min and still have a lively discussion 45min later.\nFor the impatient - the written feedback is available as Google Doc. Most common advise I heard involved patience, teaching, explaining, fast feedback and reward.\nOne warning before going into more detail on the talk: All assumptions and observations stated are highly subjective, influenced by my personal experience or by whatever the experience of the audience was. Do not expect an objective, balanced, well research analysis of the problems in general. That said, lets start with the talk itself. Before the talk I decided to limit scope to getting people in that have limited experience with open source. That intentionally excluded anyone downstream projects depending on one\u0026rsquo;s code. Though in particular interaction with common Linux distributions and their package maintainers is vital, that issue warrants for a separate talk and discussion.\nI divided those inexperienced with open source into three groups to keep discussion somewhat focused: Students learning about open source projects during their education and have neither background in software engineering nor in open source but are generally very eager to lean and open to new ideas.\nResearchers learning about the concept as part of a research grant who have some software engineering experience, some experience with open source - in particular with using it - but in general do not have writing open source software as their main objective, but have to participate as part of their research grant.\nSoftware engineers having experience with software engineering, some experience in particular with using open source and in general both strong opinions on what the right way of doing things is and who have a strong position in their team that helps them in no way when starting to contribute.\nOne very common way\nTo understand some of the issues below let me first highlight what seems to be the most common way to become involved with any Apache project: Usually it starts with using one of their software packages. After some time what is shipped does no longer fit your needs, reveals bugs that stop you from reaching your goals or is missing one particular feature - even if that is just one particular method being protected instead of private.\nPeople fix those issues. As the best software developers are utterly lazy the contribute stuff back to the project to avoid the work of having to maintain their private fork just for some simple modification. The more features of a project are being used, the more likely it gets that also larger contributions become possible. Overall this way of selecting issues to fix has a lot to do with scratching your own itch. In the end this kind of issue prioritisation also influences the general direction of a project: Whatever is most important to those actively contributing is driving the design and development. So the only way to change a project\u0026rsquo;s direction to better fit your needs is to start getting active yourself: Those that do are the ones that decide.\nStudents\nLets take a closer look at students aspiring to work on an open source project. They are very keen on contributing new stuff, learning the process and open to new ways of doing things. However for the most part they are no active users of the projects they selected so they do not directly see what is important to fix. In addition they have only limited software development experience - at least when looking at German universities, bug trackers, source version control, build systems, release management, maintaining backwards compatibility, unit test frameworks are on no schedule - and most likely shouldn\u0026rsquo;t be neither. So your average student has to learn to deal with checking out code, compiling it, getting it into their favourite editor, adding tests and making them pass.\nApart from teaching, giving even simple feedback it helps to provide the right links to literature at the right times, and generally mentor students actively. In addition it can be helpful to leave non-critical, easy to fix issues open and mark them as \u0026ldquo;beginner level\u0026rdquo; to make it easier for new-comers to get started. One last advise: Get students to publish what they do as early and as often as possible. Back in the days I used to do projects at TU Berlin with the goal of getting students to contribute to Mahout. In the first semester I left the decision on when to open up the code to the students - they never went public. In the second semester I forced them to publish progress on a weekly basis (and made that part of how their final evaluation was done) - suddenly what was developed turned into a patch committed to the code base.\nResearchers\nA second group of people that has an increasing interest in open source projects are researchers. In particular for EU project research grant the promise of providing results and software developed with the help of European tax-payers money under and open source license has become an important plus when asking for project grants.\nHowever before becoming all too optimistic it might make sense to take a closer look: Even though there is an open source check box on your average research grant that by no means leads to highly motivated, well educated new contributors for your project: With software development only being a means to reach the ultimate goal of influential publications researchers usually do not have the time and motivation to polish software to the level needed for a successful and useful contribution. In addition the concept of maintaining your contribution for a longer time usually does not fit the timeline and timeframe of a research project.\nApart from teaching and mentoring projects themselves should start asking for the motivation of the contribution. There are a few popular arguments to contribute patches back. However not all of them really work for the research use case: The cost of maintaining a fork is close to zero if you intend to never upgrade to a new version and do not need security fixes. Another common argument is an improved visibility of your work and an improved reputation of yourself as software developer. If software development for you is just a means to reach a much higher goal those arguments may not mean much to you. A third common argument is that of improving code quality by having more than one pair of eyes review it - and where would you get a better review than in the project bringing together the original code authors? However if ultimate stability, security and flexibility is not your goal than also that may not mean much to you.\nKey is to find out where the interest for working on open source comes from and build up arguments from there.\nSoftware engineers\nThe third group I identified was professional software developers - as clarified after a question from the audience: Yes, I consider people who are unable to create, read, apply patches as professional software developers. If I would exclude these people there would be noone left who earns his living with software development and does not already work on open source projects.\nIn contrast to the above groups these people have extensive software development experience. However that also means that after having seen a lot of stuff that works and that does not work they do have a strong position in their teams. Usually those fixing issues in libraries they use re the ones that have established work-flows that work for them very well and who are used to being pretty influential. When going into an open source community however no-one knows them. In general they are only judged based on their patch. They get open feedback - in the context of that project. Projects tend to have established coding guidelines, best practices, build systems - that may differ from what you are used to in your corporate environment.\nGetting up to speed in such an environment can be intimidating at best in particular if everything you do is public, searchable and findable by definition. All the more it is important to get involved and get feedback early by even putting online early sketches of what your plan is.\nHowever with everything being open there is also one major positive side to motivating contributors: Give credit where credit is due - add praise to the issue tracker by assigning issues to the one providing he patch, add the name of the contributor to your release notes. When substantial, mention the contribution with name in talks, presentations and publications.\nAnother important issue here is the influence of deadlines: If it takes half a year to get feedback on your particular improvement the reason why you made it may no longer exist - the project may have been cancelled, the developer moved to a different team, the patch applied internally as is fixing the existing issues. Fast feedback on new patches, in particular if they are clean and come with tests is vital. One positive example for providing feedback on formal issues quickly is the automated review bot at Apache Hadoop: It checks stuff like style, addition of tests, checks against existing tests and the like quickly after the patch is submitted in an automated way. Just one nitpick from the audience: The output of that bot could be either marked more clearly as \u0026ldquo;this is automated\u0026rdquo; or the text formulated a bit friendlier - if a human had done the review it would have mentioned the positive things first before criticising what is wrong.\nLast but not least (applies to researchers as well), there may be legal issues lurking: Most if not all contracts entail that at least what you do during working hours belongs to your employer - so it\u0026rsquo;s up to them what gets open sourced and what doesn\u0026rsquo;t. Suddenly your very technical new contributor has to convince management, deal with legal departments and work his way through the employers processes - most likely without deep prior knowledge on open source licenses - let alone contributor agreements (or did you know what the Apache CCLA entails, let alone being able to explain it to others before really getting active?)\nGeneral advise\nTo briefly summarise the most important points:\nGive feedback fast - projects only run for so long, interest only lasts for so long. The faster a contributor is told what is not too great about his patch, the more likely those issues are fixed as part of the contribution. (Inspired by Avro and Zookeeper who were amazingly fast in providing feedback, committing and in the case of Avro even releasing a fixed version).\nWhen it comes to new contributors be patient, remain friendly even when faced with seemingly stupid mistakes.\nGive credit where credit is due - or could be due. Mention contributors in publications, press releases, release notes, the bug tracker. Let them know that you do. (Inspired by Drools, Tomcat, Zookeeper, Avro). Pro-tip: Make sure to have no typo in people\u0026rsquo;s names even if checking takes one extra minute. (Learned from Otis).\nUse any chance you get to teach the uninitiated about the whole patch process. I know that this seems trivial to those who work with open source on a daily basis. However when getting dependencies through Maven it may already be cumbersome to figure out where to get the source from. When used to git in the daily workflow it may be a hurdle to remember how to checkout stuff from svn ;) Back in June we had a Hadoop Hackathon in Berlin that was well attended - mostly by non-committers. Jakob Homan proposed a rather unusual but very well received format: In the Hadoop bug tracker there are several issues marked as trivial (typos in documentation and the like). Attendees were asked to choose one of these issues, checkout the source, create a patch and contribute it back to the project. Optionally they got explained how the process continues from there on the committer side of things. It may seem trivial to mechanically go through the patch process, however it help lower the bar in case you have a real issue to fix to first get accustomed to just how it works. If instead of contributing to Apache you are more into working on the Linux kernel I\u0026rsquo;d like to advise you to watch Greg Kroah Hartman on writing and submitting your first Linux kernel patch (FOSDEM).\nLast but not least make sure to lower the bar for contribution - do not require people to jump through numerous loops, in general even just getting a patch ready is complicated enough. Provide a how to contribute page (e.g. see how to contribute and how to become a committer pages in the Apache Mahout wiki.\nIn particular when your project is still very young lower the bar by turning contributors into committers quickly - even if they are \u0026ldquo;just\u0026rdquo; contributing documentation fixes - in my view one of the most important contribution there is as only users spot areas for documentation improvement.\nIn case you yourself are thinking about contributing and need some additional advice as to why and for what purposes: Dr Dobbs has more information on reasons why developers tend to start to contribute to Apache software, Shalin explains why he contributes to open source, on the Mahout mailing list we hade a discussion on why also students should consider contributing, on the Apache community mailing list there was an interesting discussion on whether developers working on open source are happier than those that don\u0026rsquo;t.\n"},{"id":168,"href":"/28c33/","title":"#28c3","section":"Inductive Bias","content":" #28c3 # Restate my assumptions. One: Mathematics is the language of nature.\nTwo: Everything around us can be represented and understood through numbers.\nThree: If you graph the numbers of any system, patterns emerge. Therefore, there are patterns everywhere in nature.\nThe above is a quote from today's \"Hackers in movies\" talk at 28c3 - which amongst others also showed a brief snippet of the movie Pi. For several years I stayed well away from that one famous Hackers' conference in Berlin that takes place annually between Christmas and New Year. 23C3 was the last congress I attended until today. Though there were several fantastic talks and mean presentation quality was pretty good the standard deviation of talk quality was just too high for my taste. In addition due to limited room sizes with 4 tracks there were quite a few space issues.\nIn recent years much of that has changed: The maximum number of tickets is strictly enforced, there is an additional lounge area in a large tent next to the entrance, for the sake of having larger rooms the number of tracks was reduced to three. Streaming works for the most part making it possible for those who did not get one of the 3000 full conference tickets to follow the program from their preferred hacker space. In addition fem does an amazing job of recording, mastering, encoding and pushing videos online: Hacker Jeopardy - a show that wasn't over until early Thursday morning (about 3a.m.?) - was up on Youtube at least on Thursday at 7a.m if not earlier.\nSeveral nice geeks got me talked into joining the crowd briefly this evening for a the last three talks in \"Saal 1\" depicted above: You cannot be in Berlin during 28c3 and not see the so-called \"fnord Jahresrückblick\" by Fefe and Frank Rieger, creators of the Alternativlos podcast. Overall it is amazing to watch BCC being invaded by a large group of hackers. It's fun to see quite a few of them on Alexanderplatz, watch people have fun with a TV B Gone in front of large electronics stores. It's great to get to watch highly technical but also several political talks 4 days in a row from 11 a.m. until at least 2p.m. the following day that are being given by people who are very passionate about what they do and the projects they spend their time on.\nIf you are into tinkering, hacking, trying out sorting algorithms and generally having fun with technology make sure you check out the 28c3 Youtube channel. If you want to learn more on Hacker culture, mark the days between Christmas and New Year next year and attend 29c3 - don't worry if you do not speak German - the majority of talks is in English, most of the ones that aren't are being translated on the fly by volunteers. If you are good at translations, feel free to volunteer yourself for that task. Speaking of volunteering: My respect to all angels (helping hands), heralds (those introducing speakers), noc (network operating center), poc (phone operating center), the organisation team and anyone who helps keep make that event as enjoyable to attendees as it is.\nUpdate: Thank you to the geeks who after staying in our apartment for #28c3 helped get it back to a clean state - actually cleaner than it was before. You rock! "},{"id":169,"href":"/apache-mahout-video-collection262/","title":"Learning Machine Learning with Apache Mahout","section":"Inductive Bias","content":" Learning Machine Learning with Apache Mahout # Once in a while I get questions like Where to start learning more on machine learning. Other than the official sources I think there is quite good coverage also in the Mahout community: Since it was founded several presentations have been given that give an overview of Apache Mahout, introduce special features or even go into more details on particular implementations. Below is an attempt to create a collection of talks given so far without any claim to contain links to all videos or lectures. Feel free to add your favourite in the comments section. In addition I linked to some online courses with further material to get you started.\nWhen looking for books of course check out Mahout in Action. Also Taming Text and the data mining book that comes with weka are good starting points for practitioners.\nIntroductory, overview videos\nGrant Ingersoll: Mahout @ SF Bay Area Mahout meetup\nApache Mahout @ Apache Con Vancouver\nFrank Scholten: Configuring Mahout Clustering Jobs\nFrank Scholten: Introduction to collaborative filtering using Mahout\nFrank Scholten: Clustering @ DataDevRoom FOSDEM (starts at minute 18)\nApache Mahout @ Devoxx Antwerp\nApache Mahout @ Codebits Lisbon\nApache Con US 2009 Oakland\nTechnical details\nTed Dunning and Ellen Friedman explain logistic regression\nTed Dunning: Mahout @ SF Bay Area Mahout meetup\nApache Mahout for clinical research\nTed Dunning: Mahout @ LA-HUG\nSebastian Schelter: Scaling Apache Mahout recommendations\nMax Heimel: Adding HMM support to Apache Mahout\nSean Owen: Simple Co-Occurrence based recommendation on Hadoop\nTed Dunning: Building intelligent search applciations\nFurther course material\nML course 2011/ Stanford\nIntelligente Datenanalyse mit Matlab/ Potsdam\nLinear Algebra/ MIT\nVideolectures.net \u0026hellip; forgot about that one in the original post, sorry.\n"},{"id":170,"href":"/berlin-tech-meetups124/","title":"Berlin Tech Meetups","section":"Inductive Bias","content":" Berlin Tech Meetups # Berlin currently is of growing interest for software engineers, has a very active startup scene and as a result several community organised meetups. Listed below is a short, \u0026ldquo;highly objective\u0026rdquo; selection of local user groups - showing just the breadth of topics discussed.\nBerlin Hadoop Get Together\nBerlin DevOps\nBerlin Buzzwords (conference, no meetup)\nBerlin Scrumtisch\nJava User Group Berlin/Brandenburg\nBerlin Google Technology User Group\nAtlassian Usergroup Berlin/Brandenburg\nErlang meetup Berlin\nClojure Meetup Berlin\nDjango user group\nFSFE Berlin\nBerlin.js\nData Science Day (organised by Klaas Bollhöfer)\nRecommender Stammtisch\nBerlin Graph DB meetup\nElastic Search UG\nMongoDB User Group Berlin\nIf you want to discover new meetups: It helps attending one that is closest to your interest as usually people follow several user groups. In addition watching the scheduled event at co-working and hacker spaces like co-up Berlin, betahaus, c-base can help.\n"},{"id":171,"href":"/video-up-douwe-osinga422/","title":"Video up: Douwe Osinga","section":"Inductive Bias","content":" Video up: Douwe Osinga # Douwe Osinga: Overview of the Data Processing Pipeline at Triposo from David Obermann on Vimeo.\n"},{"id":172,"href":"/video-max-jacob-on-pig-for-nlp426/","title":"Video: Max Jacob on Pig for NLP","section":"Inductive Bias","content":" Video: Max Jacob on Pig for NLP # Max Jacob: Pig for Natural Language Processing from Isabel Drost on Vimeo.\n"},{"id":173,"href":"/slides-online375/","title":"Slides online","section":"Inductive Bias","content":" Slides online # Slides of this week\u0026rsquo;s Apache Hadoop Get Together Berlin are online at:\nDouwe Osinga: Overview of the Data Processing Pipeline at Triposo\nMax Jacob: Pig for Natural Language Processing\nOverall a great event, well organised - looking forward to seeing you at the next Get Together. If you want to get in touch with our participants, learn about new events or simply chat between meetups join our Apache Hadoop Get Together Linked.In Group.\n"},{"id":174,"href":"/apache-hadoop-get-together-berlin-december-201159/","title":"Apache Hadoop Get Together Berlin December 2011","section":"Inductive Bias","content":" Apache Hadoop Get Together Berlin December 2011 # First of all a huge Thank You to David Obermann for organising today\u0026rsquo;s Apache Hadoop Get Together Berlin: After a successful Berlin Buzzwords and a rather long pause following that finally a Christmas meetup took place today at Smarthouse, kindly sponsored by Axel Springer and organised by David Obermann from idealo. About 40 guests from Neofonie, Nokia, Amen, StudiVZ, Gameduell, TU Berlin, nurago, Soundcloud, nugg.ad and many others made it to the event.\nIn the first presentation Douwe Osinga from triposo went into some details on what Triposo is all about, how development for it differs in terms of scope and focus at larger corporations and what patterns they use for getting the data crawled, cleaned and served to users.\nThe goal of Triposo is to be able to build travel guides in a fully automated way. In contrast to simply creating a catalog of places to go to the goal is to have an application that is competitive to Lonely Planet books: Have tours, detailed background information, recommend places to visit based on wheather and seasonal signals, allow users to create travel books.\nJoining Triposo from Google, Douwe gave a rather interesting perspective on what makes a startup interesting for innovative ideas. There are four interesting aspects of application development that according to his talk matter for Google projects: First is embracing failure. Not only can single hard disks fail, but servers might be switched off automatically for maintenance, even entire datacenters going offline must not affect your application. Second is a strong focus on speed: Developers working with dynamic languages like Python that allow for rapid prototyping at the expense of slower runtime are generally frowned upon. Third building block is the focus on search that is ingrained in every piece of architecture and thinking. Fourth and last is a strong mentality to build your own which may lead to great software but leaves software developers in an isolated island of proprietary software that can limit but at least shapes your way of thinking.\nHe gave Youtube as an example: Though built on top of MySQL, implemented in Python and certainly not failure proof in every aspect they succeeded by concentrating on users\u0026rsquo; needs, time to market and iteratively improving their software with a frequent (as in one week) develop-release-deploy cycle. When entering new markets and providing innovative applications it often is crucial to be able to move quickly at the expense of speed and stability. It certainly is important to consider different architectures and chose the one that is appropriate to solve the problem at hand. Same reasoning applies for Apache Hadoop as well: Do not try to solve problems with it that it is not made to solve. Instead first think what is the right tool for your job.\nTriposo itself is built on top of 12 data sources. Most are freely available, integrated to build a usable and valuable travel guide application for iOS and Android. The features available in Triposo can be phrased in terms of a search and information retrieval problem setting and as such lend itself well for integrated sources. With offers from Amazon, Google itself, Dropbox and the like it has become easy to deploy applications in an elastic way and scale with your user base and demand for more country coverage. For them it proved advantages to go for an implementation based on dynamic languages for pure development speed. When it comes to QA they take a semi-manual approach: There are scripts checking recall (Brandenburger Tor must be found for the Berlin guide) as well as precision (there must be only one Brandenburger Tor in Berlin). Those rules need to be manually tuned.\nWhen integrating different sources you quickly run into a duplicate discovery problem. Their approach is pretty pragmatic: Merge anything that you are confident enough to say it is a duplicate. Kill everything that is likely a duplicate but you are not confident enough to merge. The general guideline is to rather miss a place than have it twice.\nFor the wikipedia source so far they are only parsing the English version. There are plans to also support other languages - in particular for parsing to increase data quality as e.g. for some places geo coordinates may be available in the German article but not in the English one. Though not going into too many technical details the talk gave some nice insights as to the strengths and weaknesses of different company sizes and mindsets when it comes to innovation as well as stabilization. Certainly a startup to watch, glad to hear that though incorporated in the US most developers actually live in Berlin now.\nThe second talk was given by Max Jakob from Neofonie GmbH (working on EU funded research project Dicode) gave an overview of their pipeline for named entity extraction and disambiguation based on a language model extracted from the raw German wikipedia dump. They used Pig to scale a pipeline down from about a week to 1.5 hours with not much development overhead: Quite some logic could be re-used from the open source project pignlproc initiated by Olivier Grisel. This project already features a Wikipedia loader, a UDF for extracting information from Wikipedia documents and additional scripts for training and building corpora.\nBased on that they defined the ML probability of a surface form being a named entity. The script itself is not very magical: The whole process can be expressed as a few steps involving grouping and couting tuples. The effect in terms of runtime vs. development time however is impressive.\nCheckout their DICODE github project for further details on the code itself.\nAfter the meetup about 20 attendees followed David to a bar nearby. It is always great to get a chance to talk to the speakers, exchange experiences with others and learn more on what people are actually working on with Hadoop after the event.\nSlides of all talks are going to be posted soon, videos go online as soon as they are post processed so stay tuned for further information.\nLooking forward to seeing you again for the next meetup. If you could not make it this time, there is a very easy way to not have that happen again next time: First speaker to submit a talk proposal to David sets the date and time of the meetup (taking into account any constraints with venue and video taping of course).\n"},{"id":175,"href":"/december-apache-hadoop-get-together-berlin-2149/","title":"December Apache Hadoop Get Together Berlin","section":"Inductive Bias","content":" December Apache Hadoop Get Together Berlin # First of all please note that meetup organisation is being transitioned over to our xing meetup group. So in order to be notified of future meetings, make sure to join that group. Please make also sure to register for the December event as in contrast to past meetups this time space will be limited, so make sure to grab a ticket. If you cannot make it, please let the organiser know so he can issue additional tickets.\nFor those of you currently following this blog only for announcements:\nWhen: December 7th 2011, 7 p.m.\nWhere: Smarthouse GmbH, Erich-Weinert-Str. 145, 10409 Berlin\nSpeaker: Martin Scholl\nTitle: On Firehoses and Storms: Event Thinking, Event Processing\nSpeaker: Douwe Osinga\nTitle: Overview of the Data Processing Pipeline at Triposo\nLooking forward to seeing you at the next Apache Hadoop Get Together Berlin in December.\n"},{"id":176,"href":"/apache-con-wrap-up25/","title":"Apache Con Wrap Up","section":"Inductive Bias","content":" Apache Con Wrap Up # First things first - slides, audio and abstracts of Apache Con are all online now on their Lanyrd page. So if you missed the conference or could not attend a session due to conflicting with another interesting session - that\u0026rsquo;s your chance to catch up.\nFor those of you who are speaking German, there\u0026rsquo;s also a summary of Apache Con available on heise Open. (If you don\u0026rsquo;t speak German, I have been told that the Google Translate version of the site captures the gist of the article reasonably well.)\n"},{"id":177,"href":"/cloudera-in-berlin142/","title":"Cloudera in Berlin","section":"Inductive Bias","content":" Cloudera in Berlin # Cloudera is hosting another round of trainings in Berlin in November this year. In addition to the trainings on Apache Hadoop this time around there will also be trainings on Apache HBase.\nRegister online via:\nhttp://www.eventbrite.com/event/2315126606\nhttp://www.eventbrite.com/event/2349094204\n"},{"id":178,"href":"/being-in-san-francisco111/","title":"Being in San Francisco","section":"Inductive Bias","content":" Being in San Francisco # I spent the last two weeks together with Thilo in San Francisco - and neighboring areas. I had asked beforehand for recommendations on where to go and what to do, had purchased a \u0026ldquo;Rough Guide to California\u0026rdquo; as well as a \u0026ldquo;Lonely Planet guide for San Francisco\u0026rdquo;. In addition I shared my arrival and departure times with a few people I know here. As to be expected our schedule quickly grew until it exploded, so we ended up doing greedy optimisation stuffing things in and putting them out again while we went along. Result of all that: An amazing two weeks that went by way to fast and the conclusion that we do need to return and bring way more time next time around.\nA huge thanks for the warm welcome, for shared local knowledge on where to go, invitations for lunch or dinner, as well as fun hours at the Castro during Halloween. Special thanks to Datameer who when asked for accommodations recommendations kindly offered to host us - it makes such a big difference to get the chance to stay in a local neighborhood and avoid hotels altogether. All in all it\u0026rsquo;s amazing to fly across an ocean, cross multiple time zones and arrive in a city that almost feels like being home.\nFollowing a brief overview of what our final schedule turned out to be - happy to share pictures we took privately. After getting our car on Sunday, we went over to Berkeley to see their impressive campus - and got to see part of the Berkeley occupy movement.\nDay one was reserved for sports\nAfter biking the bridge we went down to Sausalito. Got some delicious food at fish - a restaurant serving all sorts of healthy and tasty fish dishes. After that we went out on a canoe from Sea Trek Kayaking, taking a closer look at the house boat community and a brief tour over to the Sausalito ferry port. When back at the shore we cycled to Sausalito and took the ferry to SFO.\nDay 2 was booked for Highway #1 to Santa Cruz\nWe headed down scenic Highway#1, past the cliffs at Half Moon Bay. Went out for a hike in Butano park to hug some Redwood trees and take pictures of a fairy-tale like forest. Then headed over to Pigeon Point Lighthouse - even if you are not into staying at hostels you should stop there: this place is great for a nice view of the ocean and the lighthouse itself. Finally we went down to beautiful Santa Cruz.\nDay 3 Muir woods\nNot much to be said here: It\u0026rsquo;s always impressive to walk underneath huge Redwood trees and go hiking in the mountains around to get a better view.\nWeekend for Yosemite\n\u0026lt;img src=\u0026quot;/sfo_2011_day5.jpeg\u0026quot; height=\u0026ldquo;100\u0026rdquo; margin-right:20px;margin-top:10px\u0026quot;/\u0026gt;\nTook the route via Highway 140 - the drive itself was interesting already as it took us to quite a different country side than what we had seen until then. Got through Mariposa in the end over to Midpines. We stayed at friendly Bug Rustic Mountain - classified as Hostel, features a spa with sauna and whirlpool. We were lucky enough to arrive on Friday before Halloween to get to see the screening of \u0026ldquo;Rocky Horror Picture Show\u0026rdquo; - including a printed transcript with tag lines to be shouted at the screen for those who do not know these already.\nOn Saturday we went hiking in amazing Yosemite park. Only then I realized how close different landscapes can be: It took us only half a day by car to go from coast and ocean over to high mountains. We chose to hike to the upper Yosemite fall - returning after 7m of rather steep trails on a sunny but fortunately quite clear day we were absolutely tired.\nDay 7 Halloween\nWe spent Halloween morning over in St. Helena and went to the Francis Ford Coppola Winery. Hard to believe, but several tasty types of Californian wine do exist - at least that\u0026rsquo;s what Thilo confirmed when in St. Helena.\nEvening was booked for dressing up and going to the Castro, though presumably more calm than in past years the area is still a must-go for Halloween if you are into watching people walking around in impressing costumes.\nDay 8 for Alcatras\nNot much to be added here - don\u0026rsquo;t miss if you visit SFO.\nDay 9 for China town\nAfter several busy days we only went to China town that day and tried to recover for the rest of the afternoon - and went out for Dia de los Muertos in the evening.\nDay 10 for watching whales\nHappy I took sea-sickness pre-cautions on that one. It was a bit of a rough ride with the catamaran. But in the end we got to see whales close to Farallon Islands. The naturalist did a great job explaining not only the life of the whales but also provided some background on the islands.\nDay 11 for Point Reyes and North Beach\nIn the morning we followed the recommendation to get to see North Beach - if you like Friedrichshain/ Kreuzberg in Berlin or Dresden Neustadt - do not miss North Beach in San Francisco.\nThe afternoon was reserved for driving over to Point Reyes Lighthouse. Though we did not spot any whales in the water, the landscape already was amazing by itself.\nDay 12 - final walk through SFO\nWe took some time to walk along Hayes street, through the Golden Gate park up to Cliff House and went back with a Muni bus to the city - just too long to walk right back.\nThanks again to Stefan, St.Ack, Jon, JD, Ted, Ellen, Doug, Anne, Johan, Doris, Jens, Lance, Felix, Markus and everyone else who helped make this trip as awesome as it really was. Sorry to everyone who I did not manage to meet or get in touch - hopefully we can fix that next time I\u0026rsquo;m here - or next time you are in Berlin.\n"},{"id":179,"href":"/apache-hadoop-get-together-hand-over43/","title":"Apache Hadoop Get Together - Hand over ","section":"Inductive Bias","content":" Apache Hadoop Get Together - Hand over # Apache Hadoop receives lots of attention from large US corporations who are using the project to scale their data processing pipelines:\n“Facebook uses Hadoop and Hive extensively to process large data sets. [\u0026hellip;]” (Ashish Thusoo, Engineering Manager at Facebook), \u0026ldquo;Hadoop is a key ingredient in allowing LinkedIn to build many of our most computationally difficult features [\u0026hellip;]\u0026rdquo; (Jay Kreps, Principal Engineer, LinkedIn), \u0026ldquo;Hadoop enables [Twitter] to store, process, and derive insights from our data in ways that wouldn\u0026rsquo;t otherwise be possible. [\u0026hellip;]\u0026rdquo; (Kevin Weil, Analytics Lead, Twitter). Found on Yahoo developer blog.\nHowever the system\u0026rsquo;s use is not limited to large corporations only: With 101tec, Zanox, nugg.ad, nurago also local German players are using the project to enable new applications. Add components like Lucene, Redis, CouchDB, HBase and UIMA to the mix and you end up with a set of majour open source components that allow\ndevelopers to rapidly develop systems that until a few years ago were possible only either in Google-like companies or in research.\nThe Berlin Apache Hadoop Get Together started in 2008 allowed to learn more on how the average local company leveraged this software. It is a platform to get in touch informally, exchange knowledge and best practices across corporate boundaries.\nAfter three years of organising that event it is time to hand it over to new caring hands: David Obermann from Idealo kindly volunteered to take over organisation. He is a long-term attendee of the event and will continue it in the roughly the same spirit as before: Technical talks on success stories by users, new features by developers - not solely restricted to Hadoop only but also taking into account related projects.\nA huge Thank You for taking up the work of co-ordinating, finding a venue and a sponsor for the videos goes to David! If any of you attending the event think that you have an interesting story to share, would like to support the event financially or just help out please get in touch with David.\nLooking forward to the next Apache Hadoop Get Together Berlin. Watch this space for updates on when and where it will take place.\n"},{"id":180,"href":"/one-ring-to-rule-them-all313/","title":"One Ring to rule them all","section":"Inductive Bias","content":" One Ring to rule them all # One Ring to find them\nOne Ring to bring them all and in the darkness bind them:\n"},{"id":181,"href":"/apache-con-na21/","title":"Apache Con NA","section":"Inductive Bias","content":" Apache Con NA # Title: Apache Con NA\nLocation: Vancouver\nLink out: Click here\nStart Date: 2011-11-07\u0026lt;br /\u0026gt;End Date: 2011-11-11\n"},{"id":182,"href":"/see-you-in-vancouver-at-apache-con-na-2011363/","title":"See you in Vancouver at Apache Con NA 2011","section":"Inductive Bias","content":" See you in Vancouver at Apache Con NA 2011 # Mid November Apache hosts its famous yearly conference - this time in Vancouver/Canada. They kindly accepted my presentations on Apache Mahout for intelligent data analysis (mostly focused on introducing the project to new comers and showing what happened within the project in the past year - if you have any wish concerning topics you would like to see covered in particular, please let me know) as well as a more committer focused one on Talking people into creating patches (with the goal of highlighting some of the issues new-comers to free software projects that want to contribute run into and initiating a discussion on what helps to convince them to keep up the momentum and over come and obstacles).\nLooking forward to seeing you in Vancouver for Apache Con NA.\n"},{"id":183,"href":"/goto-con-amsterdam-day-2217/","title":"GoTo Con AMS - Day 2","section":"Inductive Bias","content":" GoTo Con AMS - Day 2 # Day two of GoTo Con Amsterdam started with a keynote by former Squeak developer Dan Ingalls. He introduced the Lively kernel - a component architecture for HTML5 that runs in any browser and allows easy composition, sharing and programming of items. Having seen Squeak years ago and being thrilled by its concepts even back then it was amazing to see what you can do with Lively kernel in a browser. If you are a designer and have some spare minutes to spend, consider taking a closer look at this project and dedicating some of your time to help them get better graphics and shapes for their system.\nAfter the keynote I had a hard time deciding on whether to watch Ross Gardler\u0026rsquo;s introduction to the Apache way or Friso van Vollenhoven\u0026rsquo;s talk on building three Hadoop clusters in a year - too many interesting stuff in parallel that day. In the end I went for the Hadoop talk - listening to presentations on what Hadoop is actually being used for is always interesting - especially if it involves institutions like RIPE who have the data to analyze the internet downtime in Egypt.\nFrise gave a great overview of Hadoop and how you can even use it for personal purposes: Apache Whirr makes it easy to use Hadoop in the cloud by enabling simple EC2 deployment, the Twitter API is a never ending source for more data to analyze (if you don\u0026rsquo;t have any yourself).\nAfter Jim Webber\u0026rsquo;s presentation on the graph database neo4j I joined the talk on HBase use cases by Michael Stack. He introduced a set of HBase usages, problems people ran into and lessons learned. HBase in itself is built on top of HDFS - and as such inherits its advantages, strengths and some of its weaknesses.\nIt is great for handling 100s of GB up to PB of data in an online, random access but strong consistency kind of model. It provides a ruby based shell, comes with a java api, map reduce connectivity, pig, hive and cascading integration, provides metrics through the hadoop metrics subsystem that are exposed via JMX and through Ganglia, provides server side filters and co-processors, hadoop security, versioning, replication and more.\nStumbleupon\nStumbleupon deals with 1B stumbles a month, has 20M users (growing), users spend approx 7h a month stumbling. For them HBase is the de-factor storage engine. It\u0026rsquo;s now 2.5years in production and enabled a \u0026ldquo;throw nothing away\u0026rdquo; culture, streamlined development. Analysis is done on a separate HBase cluster from the online version. Their lessons learnt: Educate engineering on how it works, study production numbers (small changes can make a for big payoff), over provisioning makes your life easier and gets your weekends back.\nOpenTSDB\n\u0026hellip; is a distributed, scalable time series database that collects, stores and serves metrics on the fly. For stumbleupon it is their ears and eyes into the system that quickly replaced the usual mix of ganglia, munin and cacti.\nFacebook\nAs announced earlier this year, Facebook\u0026rsquo;s messaging system is based on HBase. Also facebook metrics and analytics are stored in HBase. The full story is available in a SIGMOD paper by Facebook. In short - for Facebook Messaging HBase has to deal with 500M users, millions of messages and billions of instant messages per day. Most interesting piece of the system here was their migration path that by running both systems in parallel made switching over really smooth albeit still technologically challenging.\nTheir lessons learnt include the need to study production and adjust accordingly, to iterate on the schema to get it right. They also made the experience that there were still some pretty gnarly bugs - however with the help of the HBase community those could be sorted out bit by bit. They also concentrated on building a system that allows for locality - inter rack communication can kill you.\nYahoo\nThey keep their version of the bing webcrawl in HBase. They have high data ingest volumns (up to multiple TB/hour) from multiple streams. Atop their application also has a wide spectrum of access patterns (from scans down to single cell access). Yahoo right now runs the single larges known HBase cluster on top of 980 2.4 GHz nodes with 16 cores and 24GB Ram each in addition to 6x2TB of disk. Their biggest table has 50B documents, most of the data is loaded in bulk though.\nYFrog\n\u0026hellip; uses HBase as backend for their image hosting service. In contrast to the above HBase users they don\u0026rsquo;t have a dedicated dev team but are highly motivated and skilled ops. Being cost senstive and with a little bit of bad luck with them really everything went bad that could go bad - from crashing JVMs, bad RAM crashes, bad glibc with a race condition, etc. Their lessons learnt include that it\u0026rsquo;s better to run more smaller nodes than less big nodes. In addition lots of RAM is always great to avoid swapping.\nThe final talk I attended that day was on tackling the folklore around high performance computing. The speakers re-visited common wisdom that is generally known in the Java Community and re-evaluated it\u0026rsquo;s applicability to recent hardware architectures. Make sure to check out their slides for details on common mis-conceptions when it comes to optimization patterns. Basic take away from this talk is to know not only your programming language and framework but also the VM you are implementing your code for, the system your application will run on and the hardware your stuff will be deployed to: Hardware vendors have gone to great length optimizing their systems, but software developers have been amazing at cancelling out those optimizations quicker then they were put in.\nAll in all a great conference with lots of inspiring input. Thanks to the organizers for their hard work. Looking forward to seeing some of you over in Vancouver for Apache Con NA 2011.\n"},{"id":184,"href":"/goto-con-ams-day-1216/","title":"GoTo Con AMS - Day 1","section":"Inductive Bias","content":" GoTo Con AMS - Day 1 # Last week GoTo Con took place in Amsterdam. Being a sister conference to GoTo in Aarhus the Amsterdam event focused on the broad topics of agile development, architectural challenges, backend and frontend development, platforms like the JVM and .NET. In addition the Amsterdam event featured a special Apache track tailored towards presentations focusing on the development model at Apache and the technologies developed at Apache.\nKeynote: Dart\nThe first day started with the keynote by Kasper Lund who introduced Google\u0026rsquo;s new language Dart. Kasper was involved with developing V8 at Google. Based on his (and other project members\u0026rsquo;) experiences with large JavaScript projects the idea to create a new language for the browser was born. The goal was to build a language that had less pitfalls than JavaScript, makes it easier to provide tool support for and makes reasoning about code easier. Dart comes with class based single inheritance, lexical scoping, optional typing. It is by design single threaded. The concept of isolates cleanly introduces the concept of isolated workers that communicate through message passing only and thus can be run in parallel by the VM. One concept that seemed particularly interesting for an interpreted language was that of snapshots: An application can be serialized after it has loaded and initialized, the result can even be transferred, shortening load time substantially.\nSo far Dart is just a tech preview - on the agenda of the development team we find items such as better support for REST arguments, enums, reflection, pattern matching, tooling for test coverage and profiling. All code is freely available, also the language specification and tutorials are open. The developers would love to get more feedback from external teams.\nTwitter JVM tuning best practices\nIn his presentation on JVM tuning Attila Szegedi went into quite some detail on what kind of measures Twitter usually takes when it comes to optimizing code that run on the JVM and exhibits performance issues. Broadly speaking there are three dimensions along which the usual culprits for bad performance hide: Memory footprint of the applciation.\nLatency of requests.\nThread coordination issues.\nMemory footprint reduction\nA first step always should be to verify that memory is actually responsible for the issues seen. Running the JVM with verbosegc turned on helps identify how often and how effective full GC cycles happen on the machine. Next step is to take into account the simple solution: Evaluate whether the application can simply be given more memory. If that does not help or is impossible start thinking about how to shrink memory requirements: Use caching to avoid having to load all data im memory at once, trim down the data representation used in your implementation, when looking into what to trim know exactly what amount of memory various objects need and how many of these object you actually keep in memory - this analysis should also go into detail when using code generated from frameworks like thrift. Latency fights\nWhen taking a simple view latency optimization boils down to making a tradeoff between memory usage and time. A little less naive view is to understand that actually it is a set of three goals to optimize:\nTuning an application means to take the product of the three, shift focus but keep the product stable. Optimization is assumed to increase the resulting product.\nBiggest thread to latency are full gc cycles. Things to keep in mind when tuning and optimizing: Though the type of gc to run is configurable, this configuration does not apply to cleanup of eden space - Eden is always cleaned up with a stop-the-world gc. In general this is not too grave, as cleaning up objects that are no longer referenced is very cheap. However it can turn into a problem when there are too many surviving objects.\nWhen it comes to selecting GC implementations: Optimize for throughput by delaying GC for as long as possible. This is especially handy for bulk jobs. When optimizing for responsiveness use low pause collectors - they incur a somewhat constant penalty however those avoid having single requests with extremely large response time. This is most handy for online jobs.\nOther options to look into: Use adaptivesizepolicy and maxgcholdmillis to allow the jvm to size heap on its own based on your target characteristics. Use the printheapatgc option to view gc heap collection statistics - especially watch out for fromspace being less than 100%, use printtenuredistribution to keep an eye on number of ages, size distribution. In general, give an app as much memory as possible - when using concurrent mark and sweep implementation make sure to over-provision by about 25 to 30% to give the app a gc cushion for operation. If you can spare one cpu, set initiateoccupationfraction to 0 and let gc run all the time.\nThread coordination\nThe last issue in general causing delays are thread coordination issues. The facilities for multi-threaded programming in Java are still pretty low level - even worse, developers generally hardly know about synchronized - not so much about the atomic data types that are available - let alone other features of the concurrent package.\nMake sure you check out the speaker\u0026rsquo;s sli des they certainly contain valuable information for developers that want to scale their Java applications.\nAkka\nAnother talk that was pretty interesting to me was the introduction of Akka - a project I had only heard about before but did not have any deep technical background knowledge on. The goals when building it were fault tolerance, scalability and concurrency. Basically an easy way to scale up and out. Built in Scala, Akka also comes with Java bindings.\nAkka is built around the actor model for easier distribution. Actors are isolated, communicate only via messages and have no shared memory - making it easy to run them in a distributed way without having to worry about synchronization. Distribution across machines is currently based on protocol buffers and NIO. However the resulting network topology is still hard wired during development time.\nThe goal of new Akka developments is to make roll-out dynamic and adaptive. For that they came up with a zookeeper based virtual address resolution, configurable load balancing strategies and the option for reconfiguration during runtime.\nConcluding remarks\nThe first day was filled with lots of technical talks - so several remained more on the overview/introductory level - which is a good thing to learn about new technologies. In addition there were a few presentations on new features of upcoming and past releases for instance for Java 7 and Sprint 3.1 - it\u0026rsquo;s always nice to learn about the rational behind changes and improvements.\nAs for the agile talks - most of them propagated pretty innovative ideas that need a lot of courage to put into practice. However in several cases I could not help but get the feeling that either the processes presented were very specific to the environment they were established in and would not survive sudden stress - be it decline in revenue or team issues. In addition quite a few ideas that were introduced as novelties were already inherent in existing processes: Trust and natural communication really is the goal when establishing things like Scrum. In the end, the meetings are just the tool to get there. Clarity wrt to vision and even business value is core to prioritizing work to be done. Understanding and finding suitable metrics to measure and monitor business value of a product should be at the heart of any development project.\nOverall the first day brought together a good crowd of talented people exchanging interesting ideas, news on current projects and technical details of battle-field-stories. Being still rather small, the Amsterdam edition of GoTo con certainly made it easy to get in touch with speakers as well as other attendees over a cup of coffee and discuss the presented issues. Huge thanks to the organizers for putting together an interesting schedule, booking a really tasty meal and having a friendly answer to any question from confused attendees.\n"},{"id":185,"href":"/dear-lazy-web-travel-recommendations147/","title":"Dear lazy web: Travel recommendations","section":"Inductive Bias","content":" Dear lazy web: Travel recommendations # Finally getting to spend some days of vacation in the bay area soon - so I thought I might try to leverage the lazy web on recommendations on what not to miss while there. So far on our list:\nRent a canoo in Sausalito\nTake a tour to Alcatras\nGet to drive down along the coast towards Half Moon Bay\nSpend one or two days strolling through the city to take photos\nDo one of these touristic bike trips across the Golden Gate bridge\nSpend a day or two to get hiking in one of the red wood tree forests\nGot a recommendation to drive up to Lake Tahoe for two days\nGet to see Napa valley\nSpend a day on whale watching\nSo that already makes up for ten days - however if you spot anything important and beautiful missing, please let me know. If you have any recommendation on where to spend Halloween - please also get in touch. If you are up to meeting for a cup of coffee or dinner and talk about Mahout, HBase, Hadoop and friends - we certainly can fit that into the schedule somewhere.\n"},{"id":186,"href":"/night-trains292/","title":"Night Trains","section":"Inductive Bias","content":" Night Trains # It was several years ago that a frequent traveller told me about it being more comfortable and time saving to travel mid-sized distances (e.g. from Berlin to Amsterdam) by train - night train that is - instead of flying. Back then without decent knowledge of which combination of booking early and discount card make prizes of trips by train somewhat comparable to those offered by airlines that wasn\u0026rsquo;t really an option for me.\nIt took me years and a conference in Vienna to re-visit to his proposal: Trying to find ways to reduce the amount of times I fly I looked for alternatives. Going by car clearly is not an option as it\u0026rsquo;s more time consuming and on such long distances also more stressful. Going by regular train seemed like a waste of time as well. So I re-checked the offers for night trains. The idea of going to bed in Berlin and waking up at my destination just seemed too good.\nCurrently sitting in a CNL to Amsterdam I have to admit that for now this is the most relaxing way to travel I\u0026rsquo;ve found so far: Get on board, sleep heavenly (though not as comfortable as in your preferred hotel, beds are still ok), get your breakfast brought to bed in the morning. Mix in meeting friendly people (so far maybe I was just lucky) that are either discovering Europe as backpackers (met one from Australia and one from Canada yesterday) or happen to be taking that train as part of their weekly commute.\nI think at least for most European cities I\u0026rsquo;m converted now :)\n"},{"id":187,"href":"/goto-con215/","title":"GoTo Con","section":"Inductive Bias","content":" GoTo Con # Location: Amsterdam\nLink out: Click here\nStart Date: 2011-10-12\nEnd Date: 2011-10-14\nThis week late Tuesday night I am going to leave for GoTo con in Amsterdam. Train tickets are already booked - this is going to be my first trip with City Night line, will see how great they are.\nGoTo Amsterdam features a special Apache track as well as several talk on scaling up, searching, but also includes stuff in general architectural decisions. If you have not registered yet - use dros200 as promotion code to get a discount on the registration prize.\nLooking forward to seeing you in Amsterdam later this week.\n"},{"id":188,"href":"/design-thinking-scrumtisch-berlin151/","title":"Design Thinking @ Scrumtisch Berlin","section":"Inductive Bias","content":" Design Thinking @ Scrumtisch Berlin # This evening Mary Poppendieck gave a presentation on Design Thinking in Berlin. \u0026ldquo;What does the US Military have in common with Apple?\u0026rdquo; In her talk Mary went into more detail about how increasing complexity calls for a need to rethink design. Slides are available online. Thanks for sharing them.\nMary started with a question for the audience on who in your company decides what is to be developed: Is it the PO? Is it a \u0026ldquo;them\u0026rdquo; or is it \u0026ldquo;us\u0026rdquo;? In a world of growing complexities and faster innovation cycles, thinking of them doing the design more and more becomes a huge business risk. It\u0026rsquo;s time to start thinking of development teams as groups of people solving a business need - forgetting about the \u0026ldquo;that\u0026rsquo;s none of my business\u0026rdquo; attitude.\nLets first dig deeper into the three steps of design (according to the book The design of design) the three steps of design are:\nUnderstanding the problem: Actually finding out what is the problem, what is to be designed is the hardest part. It\u0026rsquo;s best done in a small team as opposed to by a single person.\nDesign the solution: This step involves uncovering specific requirements and identifying alternative solutions - before actually being able to judge whether an alternative is a viable way of solving a given problem, the harder part is identifying possible alternative solutions.\nImplementing the design: This step must involve getting users and developers of the resulting system work closely together.\nTo come up with a truly successful design these three steps most likely need to be iterated as what we learn in later steps has to influence earlier steps.\nWhen it comes to design, at traditional corporations there are various job titles for people involved with design. The graph distinguishes teams developing products vs. supporting internal processes. It makes a difference between software and hardware development. In addition there is a difference between people deciding on what to design (denoted in red) and those deciding how to develop (denoted in green). When looking at Scrum all of these roles are combined in the role of the product owner. When taking a closer look this seems to be a large over-simplification - especially when taking into account that the product owner in general is realized not as a role that has to be provided for but as a job description of one single person. It seems like this is too much work and too many hats for one single person.\nIn contrast Mary advocates a far more hollistic definition of the designers role: In current teams design is mostly being done up-front, in a very detailed way with results in the form of small user stories being handed over to the team afterwards. In contrast to that developers should be integrated in the design process itself - they need to participate, provide their input to come up with really innovative solutions.\nStep one: Understanding the problem\nThe presentation went into more detail on three different approaches to solving the first step in design - that is understanding the problem. The first way was coined as the military approach. In the The operations process a chapter on design was added to cope with complex situations that do not match the expected configuration.\nIn such situations a combination of collaboration, dialog, critical thinking and creative thinking seems more important than blindly following command. According to the US military\u0026rsquo;s procedures design again is an iterative three step process: From framing the problem, to experimenting and building prototypical solutions, to making sense of the situation and possibly going back to earlier steps based on what we have learned. One important feature of successful design teams is that the are composed of people with diverse backgrounds to not miss details about the situation.\nThe second approach to design is the Ethnographic approach. Mary showed a video introducing design company IDEO. The interesting take away for me was that again teams need to be diverse to see all details, that designers need not only find statistics on existing products but also go out to the customer using the product and listen to those \u0026ldquo;experts\u0026rdquo;. The need to work under time constraints, work in a very focused manner, build prototypes - not to deploy them but to learn from them, merge them and throw away what does not work early in the process.\nComing from a data driven, Hadoop-y background, the third approach was the one most familiar to me: The data based approach focuses on finding success metrics and optimising those. When going back to the \u0026ldquo;Mythical man month\u0026rdquo; example of IBM building the IBM PCjr\nthe team spending 2 years developing a product that was a horrible failure, a product that was judged very badly in common it magazines: What could they have changed to do better? Well the solution almost is in the question: If there are people writing articles to judge products, people that have criteria for judging product success - why not optimise for their criteria? Another example would be Toyota developing the Lexus: Early in the process they decided to optimise for a top score on each of the car magazine reviews. Turned out though hard it was, it was not impossible. And lead to huge success on the sales side.\nSo instead of being caught in our little development bubble maybe it\u0026rsquo;s time to think bigger - to think not only about what makes our project successful but to think about what makes business successful. And while at it - why not measure success or failure of a piece of software by how successful it is monetarily? This leads to a whole new way of thinking about development:\nInstead of a product roadmap we start thinking about a business model canvas: What metric gives us information on the business success of a new feature? How can we measure that? This can be as easy as click conversion rate. It\u0026rsquo;s closely tied to the way you make money.\nInstead of a product vision we turn to measuring a product to market fit.\nInstead of a release plan we start thinking about the minimal viable product: What is the least you can do to generate more revenue?\nInstead of an on-site customer we start having a on-site developer. This is about getting developers out of the building, get them to talk to your users and get this experience back in. Note: To some extend this is even possible by looking over the user\u0026rsquo;s shoulder by analysing your log files. Ever thought of taking a closer look at your search engines queries that return no results? Maybe instead of cursing the users that are too stupid to use your interface it\u0026rsquo;s time to re-think your interface design or add more features to your search engine.\nInstead of iterations we have a loop of building - measuring - learning about our product.\nInstead of an iteration backlog at the end of each loop we have to decide about whether to preserve the current development direction or pivot it a bit.\nInstead of a backlog of things to do we have a list of things to learn about our users.\nInstead of detailed tiny user stories we have hypothesis of how our product is used that are validated or falsified by actual data.\nInstead of continuous integration you have continuous deployment that is much more interesting.\nInstead of acceptance tests we may have split tests, A/B tests or similar.\nInstead of a definition of done including only development it suddenly also comprises validating the data - did we actually convert more users?\nInstead of costumer feedback we are having a cohort-based metric, we start watching our users to learn - they are rarely willing to give explicit feedback, but their actions are very telling.\nInstead of a product owner we have entrepreneurs that have the business as a whole in mind.\nDesign a solution\nGood industrial design is made up of ten principles: It\u0026rsquo;s innovative, makes the product useful, understandable, aesthetic, unobtrusive, honest (does not fool users), long lasting, thorough through, environmentally friendly - and most important of all there is as few design in there as possible.\nAs one example Mary mentioned the flight booking site Hipmunk.com that gives users the chance to sort flights by least agony. It optimises for the best flying experience - not the best buying experience.\nIn the end it\u0026rsquo;s all about building what the user needs - not selling what your build. It\u0026rsquo;s about watching your users and learning about their needs.\nImplementing design\nOne grave warning: Do not divorce design from development: Ford was a driver and racer himself, making him a great car designer. Wright flew the aircrafts he designed. Tom Edison thought through the whole process. Also in the tech industry the companies that are nowadays most successful were founded by people who are or at least were themselves very actively using the stuff they are building - think Bill Gates, Steve Wozniak and Steve Jobs, Sergey Brin and Larry Page. Takeaway is to do the job you are trying to automate, to have incremental development and delivery.\nIn the end it all boils down to how fast a learner you are: Not the most intelligent nor the strongest will survive. It\u0026rsquo;s the one that is fastest to adapt to change that will be successful. One commonly used analogy for that is the so-called OODA cycle coming from John Boyd: The tighter your cycle of observing, orienting, deciding and acting is, the likelier you are to be successful among your competitors.\nSpeaking of release cycles Mary gave a very nice graph of various release cycle lengths.\nIn her experience teams that have deployment cycles of six months and above are likely to spend one third of their time hardening their software. If you are down to a quarter it\u0026rsquo;s likely that the bottleneck is your business model. Maybe it is weird contracts or business partners expecting support for the exact version you shipped years ago. When you are down to a month per release you likely are running a software as a service model. As clarified later after a question from the audience for Mary this does include setups where customers basically buy a software including all future updates for a set period of time - including an automated roll-out. If you are down to daily or even just weekly releases you are likely running the product yourself. Mose likely then everyone is part of the team.\nAfter her talk Mary answered various questions from the audience: According to her experience Scrum defines the team way too narrow by focusing on development only - with Scrum we are possibly getting perfect at doing the wrong thing very well forgetting all about design and business.\nOne very general advise that leads to an interesting conclusion is to not waste your life, to not spend years of your life working on projects that are not needed by anyone. In the end this leads to realizing that failed projects are also caused by the developers not raising concerns and issues.\nMary is a huge proponent of trusting your team: Do not hand them detailed lists of user stories. You have hired great, senior people (or haven\u0026rsquo;t you??) - give them an idea what you want and let them figure out the details by talking to the right people.\nWhen asked for tools to support the lean startup process in a startup Mary emphasized the need for communication. Tools are likely to only slow you down: Get people to communicate, to sit at one table. Have a weekly or bi-weekly meeting to get everyone in sync. Otherwise let developers figure out their way to collaborate.\nThe talk was kindly hosted by Immobilien Scout, organised by agiliero with some tickets handed over to the Berlin Scrumtisch. Thanks guys. "},{"id":189,"href":"/talking-people-into-submitting-patches388/","title":"Talking people into submitting patches","section":"Inductive Bias","content":" Talking people into submitting patches # In November I am going to attend Apache Con NA. This year I decided to do a little experiment: I sumitted a talk on talking people into contributing to free software projects. The format of the talk is a bit unusual: Drawing from my - admittedly limited and very biased - experience explaining free software to others and talking people into contributing patches this talks tries to initiate a discussion on methods to get awesome developers to consider contributing their work back to free software projects.\nAs a precursor to the talk I have created a public Google docs document - it already contains the title of each slide I will use in my presentation. Of course the content does not get disclosed ahead of time.\nIf any of the readers of this blog post has experience with either explaining why they contribute, how to contribute, issues and questions new users have - please feel free to fill them into above document. I\u0026rsquo;ll try to integrate as much feedback as possible into my final slides.\n"},{"id":190,"href":"/machine-learning-problem-settings272/","title":"Machine learning problem settings","section":"Inductive Bias","content":" Machine learning problem settings # Together with Sebastian Schelter I held a Nokia sponsored (Thank you!) lecture on large scale data analysis and data mining during the past few months. After supervising a few successful university projects based on Apache Mahout the goal of this lecture was to introduce students to some of the basic concepts and problems encountered today in a world where huge datasets are generally available and are easy to process with Apache Hadoop. As such the course is targeted at an entry level audience - thorough treatment of the mathematical background of latest machine learning technology is left to the machine learning research groups in Potsdam, at TU Berlin and the neural information processing group at TU.\nSlides and exercises are available online via git. Please let me know if you want to re-use them in your lecture.\nThe very first problem that users of machine learning algorithms usually come across is mapping their application problem to one of the various machine learning problems. In 2010 Michael Brückner gave a lecture on Intelligent Data Analysis with Matlab (slides and videos in German) including a simple taxonomy of algorithms:\nAccording to the types of input data an algorithm can handle (either independent instances, also called examples, sequences or graphs of instances)\nthe type of training data available (e.g. instances with assigned nominal target attribute, no labels at all, a partial sorting of sets of instances)\nand the learning goal\nalgorithms can be nicely partitioned by the learning problem that they solve. Based on that very first step of identifying exactly what the problem setting is, deciding which algorithm to use becomes much easier. Based on that taxonomy I came up with the above graphic giving a first overview of which tasks can be solved with machine learning:\nBoxes in dark blue are what in general is called supervised learning, yellow unsupervised and light blue semi supervised - based on the amount of labeled training data available. Red boxes indicate settings with the goal of knowledge discovery. Green are any ranking problems.\n"},{"id":191,"href":"/apache-mahout-hackathon-berlin-271/","title":"Apache Mahout Hackathon Berlin","section":"Inductive Bias","content":" Apache Mahout Hackathon Berlin # Last year Sebastian Schelter from Berlin was added to the list of committers for Apache Mahout. With two committers in town the idea was born to meet some day, work on Mahout. So why not just announce that meeting publicly and invite others who might be interested in learning more about the framework? I got in touch with c-base - a hacker space in Berlin well suited to host a Hackathon - and quickly got their ok for the event.\nAs a result the first Apache Mahout Hackathon took place at c-base in Berlin last weekend. We had about eight attendees - arriving at varying times: I guess 11a.m. simply is way too early to get up for your average software developer on a Saturday. I got a few people surprised by the venue - especially those who were attending a Hackathon for the very first time and had expected c-base to be some IT company ;)\nWe started the day with a brief collection of ideas that everyone wanted to work on: Some needed help to use Mahout - topics included:\nHow to use Apache Mahout collaborative filtering with complex models.\nHow to use Apache Mahout via a web application?\nHow to use classification (mostly focussed on using Naive Bayes from within web applications).\nIs HBase a solution for scalable graph mining algorithms?\nIs there a frequent itemset algorithm that respects temporal changes in patterns?\nThose more into Mahout development proposed a slightly different set of topics:\nPLSI and Map/Reduce?\nBuild customisable sampling strategies for distributed recommendations.\nCome up with a more Java API friendly configuration scheme for Mahout clusterings.\nComplete the distributed SVD recommender.\nQuickly teams of two to three (and more) people formed. First several user side questions could be addressed by mixing more experienced Mahout developers with newbie users. Apart from Mahout specifics also more basic questions of getting involved even by simply contributing to the online documentation, answering questions on the mailing lists or just providing structured access to existing material that users generally have trouble finding. Another topic that is being overlooked all too when asking users to contribute to the project is the process of creating, submitting, applying and reviewing patches itself: Being deeply involved with free software projects dealing with patches, integration of issue tracker and svn with the project mailing lists all seems very obvious. However even this seemingly basic setup sometimes looks confusing and complex to regular users - that is very common but not limited to people who are just starting to work as software developers.\nThanks to Thilo Fromm for taking the group picture.\nIn the evening people finally started hacking more sophisticated tasks - working on the first project patches. On Sunday only the really hard core developers remained - leading to a rather focussed work on Mahout improvements which in the end led to first patches sent in from the Mahout Hackathon.\n"},{"id":192,"href":"/apache-dinner-berlin30/","title":"Apache Dinner Berlin","section":"Inductive Bias","content":" Apache Dinner Berlin # Title: Apache Dinner Berlin\nLocation: Good Morning Vietnam X-Berg\u0026lt;br /\u0026gt;Start Time: 19:00\nDate: 2011-02-28\nPlease contact Simon Willnauer if you would like to attend for further information.\n"},{"id":193,"href":"/note-to-self-svnignore-usage299/","title":"Note to self: svn:ignore usage","section":"Inductive Bias","content":" Note to self: svn:ignore usage # Putting the information here to make retrieving it a bit easier next time.\nWhen working with svn and some random IDE I\u0026rsquo;d really love to avoid checking in any files that are IDE specific (project configuration, classpath, etc.). The command to do that:\nsvn propedit svn:ignore $directory_to_edit\nAfter issuing this command you\u0026rsquo;ll be prompted to enter file patterns for files to ignore or the directory names.\nMore detailed information in the official documentation on svn:ignore.\n"},{"id":194,"href":"/berlin-scrumtisch-february-2011122/","title":"Berlin Scrumtisch - February 2011","section":"Inductive Bias","content":" Berlin Scrumtisch - February 2011 # The February Scrumtisch Berlin featured a talk by Lyssa Adkins, famously known for her publications on Coaching Agile teams. A mixture of fifty developers, scrum masters, coaches and product owner as well as one project manager followed Marion Eikmann\u0026rsquo;s invitation. Thanks for organising the event, as well as thank you to Hypoport for providing the venue.\nIn her one-hour presentation she mainly focussed on two core topics: On the roles agile coaches have to fullfill as well as on the skill set needed by agile coaches. Being an agile coach (as well as being a scrum master, really), entails four very basic roles:\nBeing a bulldozer - that is getting impediments out of the way. That can be as easy (or hard) as getting appropriate equipment for your developers or teaming up with the product owner to communicate with anyone trying to break the sprint.\nBeing a servant leader - for a coach that means to work for the team, to ask the right questions and enable the team, to listen during dailies - instead of asking for additional reports: Most tools are already within the Scrum toolbox. The hard task is to identify them and use them correctly and efficiently.\nBeing a shepherd - that may be as easy as getting people to attend the dailies, or as complex as communicating common values, a common goal.\nBeing the guard of quality and performance - as a coach that means making degrading quality visible - and letting the Scrum team take the decision on how to deal with it.\nHowever in reality each team is different, each sprint is different. So coaching really is similar to bing the river guide: Each trip is different. It is your task to make the team succeed and adapt to differing situations. To get to team to high performance - over and over again.\nWhen talking about coaching what people need is a very specific skill set:\nTo become a successful agile coach it helps to have coaching skills to be able to understand the client, to see their impediments and help the team become better by listening and asking powerful questions.\nBe a facilitator who organises sessions, meetings, conversations and may mediate in meetings.\nHave business domain expertise - that is to know process designs, figure out a companies strategy options.\nBe a great teacher to help people understand the basic concepts. That entails knowledge on course design, most likely design of exercises.\nHave mentoring skills to guide a team in the process.\nKnow about lean and agile processes and stay up to date.\nHave the technical skills on what makes development successful, know the extreme programming techniques to help team excel.\nHave transformational skills\nthat is be familiar with change management, transformation leadership, knowing how to change organisations.\nHowever being a coach remember that people are influenced not only by what you teach - but mostly by what you do. Most of what being a good coach means is invisible to the uninitiated outsider. It\u0026rsquo;s about living the process you teach, staying true to the principles you want others to implement.\nTo get there it helps to get inspired, to talk with others in local meetups (just as with any coding practice: Try to find a common forum to exchange ideas and get fresh input). It may help to keep a value journal - not only to track your accomplishments and be able to prove them to higher management, but also to track where to improve yourself.\nThe talk provided several interesting starting points for exploring further. However with an added exercise sixty minutes covered only the very basic ideas. Especially for the skill sets needed to successfully coach teams it would be very interesting to learn more on what books to read or what courses to attend to learn more on each topic. Ultimately not only agile coaches benefit from being great teachers, mentors or facilitators. "},{"id":195,"href":"/video-is-up-paolo-negri-on-scaling-by-one-order-of-magnitude421/","title":"Video is up: Paolo Negri on scaling by one order of magnitude","section":"Inductive Bias","content":" Video is up: Paolo Negri on scaling by one order of magnitude # "},{"id":196,"href":"/video-is-up-simon-willnauer-on-lucene-4-performance-improvements420/","title":"Video is up - Simon Willnauer on Lucene 4 Performance improvements","section":"Inductive Bias","content":" Video is up - Simon Willnauer on Lucene 4 Performance improvements # "},{"id":197,"href":"/video-is-up-josh-devins-on-apache-hadoop-at-nokia419/","title":"Video is up - Josh Devins on Apache Hadoop at Nokia","section":"Inductive Bias","content":" Video is up - Josh Devins on Apache Hadoop at Nokia # "},{"id":198,"href":"/call-for-presentations-berlin-buzzwords-one-more-week-to-go133/","title":"Call for Presentations Berlin Buzzwords - one more week to go","section":"Inductive Bias","content":" Call for Presentations Berlin Buzzwords - one more week to go # As a little reminder: the Call for presentations of Berlin Buzzwords will close next week on Tuesday, March 1st. Submissions on scalable search, data storage and analysis are all welcome. We are looking for presentations on the core technologies such as Apache Hadoop, CouchDB, Lucene, Redis, Voldemort but also talks on interesting use cases and system architectures.\nTickets are out for sale - don\u0026rsquo;t wait too long to get your early bird ticket.\nIn addition we are in the process of drafting some additional packages for those of you who would like to bring their non-tech spouse to the conference or need day care facilities for their children. If you are interested in either package please provide feedback on our blog.\n"},{"id":199,"href":"/teddy-in-amsterdam391/","title":"Teddy in Amsterdam","section":"Inductive Bias","content":" Teddy in Amsterdam # On his trip from SFO back to Europe teddy spent a few days in Amsterdam. He brought back the following pictures of bikes, cheese and the canals: Going to Amsterdam usually is pretty darn dangerous: I tend to return with some toys added to my collection of puzzles. Somehow I tend to be drawn to the shop called Gamekeeper selling them even when I do not remember the exact address or even just the street name: "},{"id":200,"href":"/apache-mahout-meetup-amsterdam81/","title":"Apache Mahout Meetup Amsterdam","section":"Inductive Bias","content":" Apache Mahout Meetup Amsterdam # Last week I was honoured to be invited as one of the two speakers on Apache Mahout at the Mahout meetup in Amsterdam at JTeams offices. After free beer, cola and pizza Frank Scholten gave an overview of Mahout's clustering capabilities. After a brief introduction to Mahout itself he went into a little more detail on how clustering works in general. After that with a selection of Seinfeld scripts he used a fun data set to guide the audience through the process of choosing the right data preparation steps, coming up with good training parameters and finally evaluating clustering quality.\nAfter that I gave a brief introduction to classification with Mahout - going into a little more detail when it comes to data preparation and quality evaluation. The audience seemed most interested in learning more on how data preparation works - after all that step cannot really be covered by Mahout itself (though we do have some support) but instead needs a lot of domain knowledge from the user side.\nJudging from the brief round of self introductions the meetup was well visited by an intesting mixture of people coming from JTeam, Hippo, the dutch police working on data analytics, developers working at RIPE and many more.\nIf you are interested in more data analysis, search and data storage - do not miss registration for Berlin Buzzwords on June 6/7th 2011.\n"},{"id":201,"href":"/fosdem-sunday-smaller-bits-and-pieces182/","title":"FOSDEM - Sunday - smaller bits and pieces","section":"Inductive Bias","content":" FOSDEM - Sunday - smaller bits and pieces # With WebODF the Office track featured a very interesting project that focusses on providing a means to open ODF documents in your favourite browser: Content and formatting are converted to a form that can easily be dealt with by using a combination of HTML and CSS. Advanced editing is then supported by using JavaScript.\nWith Open Stack the following talk focussed on an open cloud stack project that was started by NASA and Rackspace as both simultanously needed support for an open source, openly designed, developed cloud stack that strives for community inclusion. According to the speaker the goal is to be as ubiquitous a cloud project as Apache is for web servers - he probably was not quite aware of how close to even the foundation side of Apache that development model is.\nThe closing keynote dealt with the way kernel development takes place. There were a few very interesting pieces of information for contributors that are valid for any open source project really:\nOut of tree code is invisible to the kernel developers and users. As such the longer it remains out of tree code the harder it becomes to actually go out there and feel the wind.In contrast open code means giving up control: Maintainership means responsibility but it does not come with any power or control over the source code. Similarly opening code up as patch or separate project at Apache means giving up control - means working towards turning the project into a community that can live on its own.For kernel patches the general rule is to not break things and not go backward in quality: What is working for users today must be working with the next release as well. To be able to spot any compat issues it is necessary to take part on the wider disucssion lists - not only in your limited development community. Developers should focus on coming up with a problem solution instead of getting their original code into the project.Or in short: The kernel is no research project, as such it must not break existing applications. Visionary brilliance really is no excuse for poor implementation. Conspiracy theories such as \u0026quot;hey, developer x declined my patch only because it is out of scope for his employer's goals\u0026quot; are not going to get you anywhere. Such things do happen, but in general kernel developers first think of themselves as kernel developers - being employee somewhere only comes after that.\nKeep in mind that the community remembers past actions. In the end you need not convince business people or users but the developers themselves who might end up with the maintanance burden for your patch. To get your patch accepted it greatly helps to not express it in terms of the implementation needs only but to clearly formulate your requirements - independent of implementation. And as in any open source project, helping with cleanup (that is not only white space fixes, but real cleanup as in refactoring) does help build a positive attitude.\nWhy you should go for kernel development never the less? It's a whole lot of fun. It's a way to influence the kernel to support the features that you need. It's sort of like becoming part of an elite club - and which developer does not like the feeling of belonging to the elite changing the way the world looks tomorrow? In addition as with an substantial open source involvement being visible in the kernel community also most likely means being visible to your future employer.\n"},{"id":202,"href":"/fosdem-hbase-at-facebook-messaging180/","title":"FOSDEM - HBase at Facebook Messaging","section":"Inductive Bias","content":" FOSDEM - HBase at Facebook Messaging # Nicolas Spiegelberg gave an awesome introduction not only to the architecture that powers Facebook messaging but also to the design decisions behind their use of Apache HBase as a storage backend. Disclaimer: HBase is being used for message storage, for attachements with Haystack a different backend is used.\nThe reasons to go for HBase include its strong consistency model, support for auto failover, load balancing of shards, support for compression, atomic read-modify-write support and the inherent Map/Reduce support.\nWhen going from MySQL to HBase some technological problems had to be solved: Coming from MySQL basically all data was normalised - so in an ideal world, migration would have involved one large join to port all the data over to HBase. As this is not feasable in a production environment instead what was done was to load all data into an intermediary HBase table, join the data via Map/Reduce and import all into the target HBase instance. The whole setup was run in a dark launch - being fed with parallel life traffic for performance optimisation and measurement.\nThe goald was zero data loss in HBase - which meant using the Apache Hadoop append branch of HDFS. The re-designed the HBase master in the process to avoid having a single point of failure, backup masters are handled by zookeeper. Lots of bug fixes went back from Facebooks engineers to the HBase code base. In addition for stability reason rolling restarts were added for upgrades, performance improvements, consistency checks.\nThe Apache HBase community received lots of love from Facebook for their willingness to work together with the Facebook team on better stability and performance. Work on improvements was shared between teams in an amazing open and inclusive model to development.\nOne additional hint: FOSDEM videos of all talks including this one have been put online in the meantime.\n"},{"id":203,"href":"/fosdem-django179/","title":"FOSDEM - Django","section":"Inductive Bias","content":" FOSDEM - Django # The languages/ cloud computing track on Sunday started with the good, the bad and the ugly of Django's architecture. Without much ado the speaker started by giving a high level overview of the general package layout of Django - unfortunately not going into too much detail on the architecture itself.\nWhat he loves about Django are the model layer abstractions that really are no ORM only - instead both relational and non-relational databases can be supported easily. Abstractions in Django are made by task solved - there are multiple implementations available for caching, mailing, session handling etc. There is great geo support with options for defining geo objects, querying single points on a map for all their overlaying geo objects. Being a community of test driven people Django features awesome debugging and testing tools. To avoid cross side request forgery Django comes with built in protection mechanisms.\nThere is multi database support for building applications. Being a small core implementation features can be turned on and off as needed. In addition the framework comes with great documentation: No feature addition is accepted unless it comes with decent documentation - which fits nicely with the common perception that anything that is untested and undocumented does not exist.\nThe bad things about Django according to the speaker? Well, the old CSRF protection implementation that might lead to token leakage. Schema changes and migrations currently really are hard to handle. Though there is south to handle at least some of the migrations pain. The templating implementation could use some improvement as well - being designed to make inclusion of logic in the templates hard some use cases are just to clumsy to implement.\nAs for the ugly things: There is quite a bit of magic at work which generally leads to harder tracing of applications - that is about to get better. Too many parts of Django rely on unwieldy regular expressions. Anything that spans more than 4 lines on a screen probably is to be considered unmanageable and unchangeable. Authentication cannot really be customised - the information that is stored per user is hard coded and fixed.\nOver time what was learned: Refactoring cannot be avoided as requirements change. However being consistent in what you do makes it so much easier for users to pick up the framework. What helps with creating a great open source project: People that have the time to invest - never under estimate the time needed to really go from prototype to production ready.\n"},{"id":204,"href":"/fosdem-saturday181/","title":"FOSDEM - Saturday","section":"Inductive Bias","content":" FOSDEM - Saturday # Day one at FOSDEM started with a very interesting and timely keynote by Eben Moglen: Starting with the example of Egypt he voted for de-centralized distributed and thus harder to take over communication systems. In terms of tooling we are already almost there. Most use cases like micro blogging, social networking and real time communications can already be implemented in a distributed, fail safe way. So instead of going for convenience it is time to think about digital independence from very few central providers.\nI spent most of the morning in the data dev room. The schedule was packed with interesting presentations ranging from introductory overview talks on Hadoop to more in depth treatment of the machine learning framework Apache Mahout. With an analysis of the Wikileaks cables the schedule also included case studies on what use cases can be implemented by thourough data anlysis. The afternoon featured presentations on the background to more data analytics for better usability at Wikimedia as well as talks on buiding search applications.\nIn the lightning talks room a wide variety of projects was presented - in only ten minutes Pieter Hintjens explained the gist of using 0MQ for messaging. That talk included \u0026quot;Hintjens law of concurrency: e = m * c^2, where e is effort needed to implement and maintain, m is mass - that is the amount of code written and c is complexity.\nFor me the day ended with a very interesting presentation by Matthias Kirschner/FSFE on one of their campaigns: pdfreaders.org has the very narrow and well scoped goal of getting links to unfree software off of governmental web pages. Using a really intuitive example they were able to convince officials of linking to their vendor neutral list of pdf readers: \u0026quot;Just imagine a road in your city. At this road drivers will find a sign that tells them the road is well suited to be used by VW cars. Those cars can be obtained for test drive at the following address. Your government.\u0026quot; As unthinkable as such as sign may be that same text is included in nearly all governmental web pages linking to the acrobat reader.\nWhat made pdfreaders successful is the combined effort of volunteers, its very narrow and clear scope, it's scalability by nature: People were asked to submit \u0026quot;broken\u0026quot; web pages to a bug tracker, campaign participants would then go and send out paper letters to these institutions and mark the bugs fixed as soon as the links were changed. Letters were pre-written and well prepared. So all that was needed was money for toner, paper and stamps.\nOne final cute example of how that worked out can be seen at hamburg.de/adobe.\n"},{"id":205,"href":"/teddy-in-brussels395/","title":"Teddy in Brussels","section":"Inductive Bias","content":" Teddy in Brussels # Not much time here, except for a brief view of the atomium from our bed\u0026amp;breakfast place in Brussels and a few very beautiful churches:\n"},{"id":206,"href":"/oreilly-strata-day-one-afternoon-lectures301/","title":"O'Reilly Strata - day one afternoon lectures","section":"Inductive Bias","content":" O\u0026rsquo;Reilly Strata - day one afternoon lectures # Big data at startups - Info ChimpsAs a startup to get good people there is no other option then to grow your own: Offer the option to gain a lot of experience in return for a not so great wage. Start out with really great hires:\nPeople who have the \u0026quot;get shit done gene\u0026quot;: They discover new projects, are proud to contribute to team efforts, are confident in making changes to a code base probably not known before hand. To find these you should ask open ended questions in interviews.People who are passionate learners, that use the tools out there, use open code and are willing to be proven wrong.People who are generally fun to work with.Put these people on small, non-mission-critical initial projects - make them fail on parallel tasks (and tell them they will fail) to teach them to ask for help. What is really hard for new hires is learning to deal with git, ssh keys, command line stuff, what to do and when to ask for help, knowing what to do when something breaks.\nInfochimps uses Kanban for organisation: Each developer has a task he has chosen at any given point in time. He is responsible for getting that task done - which may well involved getting help from others. Being responsible for a complete features is one big performance boost once the feature truely goes online. Code review is being used for teachable moments - and in cases where something really goes wrong.\nDevelopment itself is organised to optimise for developers' joy - which usually means to take Java out of the loop.\nMachine learning at OrbitzThey use Hadoop mostly for log analysis. Also here the problem of fields or whole entries missing in the original log format was encountered. To be able to dynamically add new attributes and deal with growing data volumns they went from a data warehouse solution to Apache Hadoop. Hadoop is used for data preparation before training, for training recommender models, for cross validation setups. Hive has been added for ad-hoc queries usually issued by business users.\nData scaling patterns at LinkedInWhen scaling to growing data LinkedIn developers started gathering a few patterns that helped make dealing with data easier:\nWhen building applications constantly monitor your invariants: It can be so frustrating to run an hour long job just to find out at the very end that you made a mistake during data import.Have a QA cluster, have versioning on your releases to allow for easy rollback should anything go bad. Unit tests go without saying.Profile your jobs to avoid bottlenecks: Do not read from the distributed cache in a combiner - do not reuse code that was intended for a different component without thorough review.Dealing with real world data means dealing with irregular, dirty data: When generating pairs of users for connect recommendation, Obama caused problems as he is friends with seemingly every american. However the biggest bottleneck: IO during shuffling as every map talks to every reducer. As a rule of thumb, do most work on the map side and minimise data sent to reducers. This also applies to many of the machine learning M/R formulations. One idea for reducing shuffling load is to pre-filter on the map side with bloom filters.\nTo serve at scale:\nRun stuff multiple times. Iterate quickly to get fast feedback. Do AB testing to measure performance. Push out quickly for feedback. Try out what you would like to see.See also sna-projects.com/blog for more information.\n"},{"id":207,"href":"/oreilly-strata-day-two-keynotes302/","title":"O'Reilly Strata - Day two - keynotes","section":"Inductive Bias","content":" O\u0026rsquo;Reilly Strata - Day two - keynotes # Day two of Strata started with a very inspiring insight from the host itself that extended the vision discussed earlier in the tutorials: It's not at all about the tools, the current data analytics value lies in the data itself and in the conclusions and actions drawn from analysing it.\nBit.ly keynoteThe first key note was presented by bit.ly - for them there are four dimensions to data analytics:\nTimeliness: There must be realtime access, or at least streaming access to incoming data.Storage must provide the means to efficiently store, access, query and operate on data.Education as there is no clear path to becoming a data scientist today.Imagination to come up with new interesting ways to look at existing data.Storing shortened urls for bit.ly there really are three views on their data: The very personal intrinsic preferences expressed in your participation in the network. The neighborhood view taking into account your friends and accquaintances. Finally there is the global view that allows for drawing conclusion on a very large global scale - a way to find out what's happening world wide just by looking at log data.\nThomson Reuters In contrast to all digital bit.ly Thomson Reuters comes with a very different background - though acting on a global scale distributing news world wide there lots of manual intervention is still asked for to come up with high quality, clean, curated data. In addition their clients focus on very low latency to be able to act on new incoming news at the stock market.\nFor traditional media providers it is very important to bring news together with context and users: Knowing who users are and where they live may result in delivering better service with more focussed information. However he sees a huge gap between what is possible with today's web2.0 applications and what is still in common practice in large corporate environments: Social networking sites tend to gather data implicitly without clearly telling users what is collected and for which purpose. In corporate environments though it was (and still is) common practice to come up with general compliance rules that target protecting data privacy and insulating corporate networks from public ones. Focussing on cautious and explicit data mining might help these environments to benefit from cost savings and targeted information publishing to the corporate environment as well.\nMythology of big dataEach technology caries in itself the seeds for self destruction - same is true for Hadoop and friends: The code is about to start turning into commodity itself. As a result the real value lies in the data it processes and the knowledge about how to combine existing tools to solve 80% of your data analytics problems.\nThe myth really lies in the lonely hacker sitting in front of his laptop solving the world's data analysis problems. Instead analytics is all about communication and learning from those who stored and generated the data. Only they are able to tell more on business cases as well as the context of the data. Only domain knowledge can help solve real problems.\nIn the past data emerged from being the product, into being a by-product, to being an asset in the past decade. Nowadays it is turning into a substrate for developing better applications. There is no need for huge data sets for turning data into a basis for better applications. In the end it boils down to using data to re-vamp your organisation's decisions from being horse trading, gut-check based decisions to scientific, data backed informed decisions.\nAmazon - Werner VogelsFor amazon, big data means that storing, collecting, analyzing and processing the data are hard to do. Being able to do so currently is a competitive advantage. In contrast to BI where questions drove the way data was stored and collected today infrastructure is cheap enough to creatively come up with new analytics questions based on available data. Collecting data goes from a streaming model to daily imports even to batch imports - never under estimate the bandwidth of FedEx. There even is a FedEx import at Amazon.Never under estimate the need for increased storage capacity. Storage on AWS can be increased dynamically.When organizing data keep data quality and manual cleansing in mind - there is a mechanical turk offering for that at AWS.For Analysis Map Reduce currently is the obvious choice - AWS offeres elastic map reduce for that.The trend goes more and more to sharing analysis results via public APIs to enable customers down stream to reuse data and provide added value on top of it.Microsoft Azure data market placeMicrosoft used their keynote to announce the Azure Data Marketplace - a place to make data available for easy use and trading. To deal with data today you have to find it, license it from its original owner - which incurs overhead negotiating licensing terms. The goal of Microsoft is to provide a one click stop shop for data that provides a unified and discoverable interface to data. They work with providers to ensure cleanup and curation. In turn providers get a marketplace for trading data. It will be possible to visualize data before purchase to avoid buying what you do not know. There is a subscription model that allows for constant updates, has cleared licensing issues. There are consistant APIs to data that can be incorporated by solution partners to provide better integration and analysis support.\nAt the very end the Heritage health prize was announced - a 3 million data mining competition open for participation starting next April.\n"},{"id":208,"href":"/oreilly-strata-tutorial-data-analytics303/","title":"O'Reilly Strata - Tutorial data analytics","section":"Inductive Bias","content":" O\u0026rsquo;Reilly Strata - Tutorial data analytics # Acting based on dataIt comes as no surprise to hear that also in the data analytics world engineers are unwilling to share details of how their analysis works with higher management - with on the other side not much interest on learning how analytics really works. This culture leads to a sort of black art, witch craft attitude to data analytics that hinders most innovation.\nWhen starting to establish data analytics in your business there are a few steps to consider: Frist of all no matter how beautiful visualizations may look on the tool you just chose to work with and are considering to buy - keep in mind that shiny pebbles won't solve your problems. Instead focus on what kind of information you really want to extract and chose the tool that does that job best. Keep in mind that data never comes as clean as analysts would love it to be. Ask yourself how complete your data really is (Are all fields you are looking at filled for all relevant records?).Are those fields filled with accurate information (Ever asked yourself why everyone using your registration form seems to be working for a 1-100 engineers startup instead of one of the many other options down the list?) For how long will that data remain accurate?For how long will it be relevant for your business case?Even the cleanest data set can get you only so far: You need to be able to link your data back to actual transactions to be able to segment your customers and add value from data analytics.\nWhen introducing data analytics check whether people are actually willing to share their data. Check whether management is willing to act on potential results - that may be as easy as spending lots of money on data cleansing, or it may involve changing workflows to be able to provide better source data. As a result of data analytics there may be even more severe changes ahead of you: Are people willing to change the product based on pure data? Are they willing to adjust the marketing budget? ... job descriptions? ... development budget? How fast is the turnaround for these changes? When making changes yearly there is no value in having realtime analytics.\nIn the end it boils down to applying the OODA cycle: If you can be faster observing, orienting, deciding and acting than your competitor only then do you have a real business advantage.\nData analytics ethicsToday Apache Hadoop provides the means to give data analytics super powers to everyone: It brings together the use of commodity hardware with scaling to high data volumns. With great power there must come great great responsibility according to Stan Lee. In the realm of data science that involves solving problems that might be ethically at least questionable though technologically trivial:\nHelping others adjust their weapons to increase death rates.Making others turn into a monopoly.Predict the likelihood of cheap food making you so sick that you are able and willing to go to court against the provider as a result. On the other hand it can solve cases that are mutually sensible both for the provider and the customer: Predicting when visitors to a casino are about to become unhappy and willing to leave before the even know they are may give the casino employees a brief time window for counter actions (e.g. offering you a free meal).\nIn the end it boils down to avoiding to screw up other people's lifes. Deciding which action does least harm while achieving most benefit. Which treats people at least proportional if not equal, what serves the community as a whole - or more simply: What leads me to being the person I always wanted to be.\n"},{"id":209,"href":"/teddy-in-san-francisco403/","title":"Teddy in San Francisco","section":"Inductive Bias","content":" Teddy in San Francisco # Before attending O'Reilly Strata there were a few days left to adjust to the different time zone, meet up with friends and generally spend some days in the Greater San Francisco area. As was to be expected, those were way to few days. The weekend was a bit rainy, still packed with visiting China town right after the plane had landed and spending some time at ... Finally was taken out to Bucks - the restaurant among software engineers generally known for being the place where VC deals are being made.\nSunday was reserved for visiting some red wood trees - it's so great driving just a few minutes out of the city and arriving in an area that looks like being set up for a fairy tale movie. With all the mist and with sun coming out here and there the area looked even more bewitched. On Monday sun finally arrived in the bay - as a result a ferry trip to Sausolito seemed like the optimal thing to do. Unfortunately not enough time to rent a bike an to the \u0026quot;ride the bridge\u0026quot; tour - or get a kajak to go out into the bay. Maybe next time though.\nAfter returning back home, Teddy showed me some pieces of chocolate someone in the US made him adicted to - now it's not just the tasty swiss one but also the Berkley one I have to find a shop for in Berlin ;) "},{"id":210,"href":"/slides-of-yesterdays-apache-hadoop-get-together374/","title":"Slides of yesterday's Apache Hadoop Get Together","section":"Inductive Bias","content":" Slides of yesterday\u0026rsquo;s Apache Hadoop Get Together # This time with little less than 24 hours delay - the usual, by some impatiently expected, summary of the Apache Hadoop Get Together. The meetup took place at Zanox\u0026rsquo; event campus. The room was well filled with some fourty attendees from various companies, experience with Hadoop ranging from interested beginners to experienced users.\nSlides of all presentations:\nJosh Devins on Hadoop at Nokia\nSimon Willnauer on Lucene 4\nPaolo Negris on Scaling issues\nThe first presentation was given by Josh Devins from Nokia in Berlin. He is working closely with the OVI maps team. After giving a general overview of the cluster setup as well as some information on what machines they are running Hadoop on. Currently Hadoop is used mostly to process log data and aggregate information from it. For that task scribe is used for log collection, standard Ganglia and Nagios for monitoring and graphing. When starting to process and aggregate log data the main challenge is a mixture of transforming the logs into some slightly consistant format, cleaning logs from noisy data and in some cases initiating the storage of further information from various services. Nokia is a heavy - and happy - user of Pig though they are looking into Hive for making data accessible to business analysts who usually are more familiar with SQL like languages.\nAs an example - the results of a few simple jobs on analysing location based searches were shown: Looking at where in the greater Berlin area searches for \u0026ldquo;Ikea\u0026rdquo; were issued - at least Ikea Tempelhof and Spandau were easy to make out. On a more serious use case similar information could be used for automatically detecting traffic jams. Currently Nokia is only scratching the surface of all information that could possibly be extracted. So there is quite some interesting work ahead.\nIn the second presentation Simon Willnauer gave a deep dive introduction to the various stunning performance improvements of Lucene4 - the not yet released, not backwards compatible trunk version of Apache Lucene. For more flexible indexing column stride fields have been integrated. With the introduction of an automaton implementation fuzzy query performance could be improved significantly reducing complexity from n to log n. In addition Simon had a great surprise to share with the audience: He proudly announced that Ted Dunning (you know that guy who is active on nearly every Hadoop mailing list, shares a lot of in-depth theoretical knowledge that is backed by proven practical experience?) and Doug Cutting (founder of Lucene, Hadoop and many other Apache projects) are going to be keynote speakers at Berlin Buzzwords.\nIn the third presentation Paolo Negri shared some inside as to how Wooga\u0026rsquo;s Ruby on Rails/ MySQL based system was scaled. Disclaimer: Redis did play a major role when upping performance. Videos will be published as soon as they are processed - thanks again to Cloudera for supporting the event by sponsoring video taping.\n"},{"id":211,"href":"/cfp-berlin-buzzwords-2011-search-score-scale134/","title":"CFP - Berlin Buzzwords 2011 - search, score, scale","section":"Inductive Bias","content":" CFP - Berlin Buzzwords 2011 - search, score, scale # This is to announce the Berlin Buzzwords 2011. The second edition of the successful conference on scalable and open search, data processing and data storage in Germany,\ntaking place in Berlin.\nCall for Presentations Berlin Buzzwords\nhttp://berlinbuzzwords.de\nBerlin Buzzwords 2011 - Search, Store, Scale\n6/7 June 2011\nThe event will comprise presentations on scalable data processing. We invite you to submit talks on the topics:\nIR / Search - Lucene, Solr, katta or comparable solutions\nNoSQL - like CouchDB, MongoDB, Jackrabbit, HBase and others\nHadoop - Hadoop itself, MapReduce, Cascading or Pig and relatives\nClosely related topics not explicitly listed above are welcome. We are looking for presentations on the implementation of the systems themselves, real world applications and case studies. Important Dates (all dates in GMT +2) Submission deadline: March 1st 2011, 23:59 MEZ\nNotification of accepted speakers: March 22th, 2011, MEZ. Publication of final schedule: April 5th, 2011. Conference: June 6/7. 2011\nHigh quality, technical submissions are called for, ranging from principles to practice. We are looking for real world use cases, background on the architecture of specific projects and a deep dive into architectures built on top of e.g. Hadoop clusters. Proposals should be submitted at http://berlinbuzzwords.de/content/cfp-0 no later than March 1st, 2011. Acceptance notifications will be sent out soon after the submission deadline. Please include your name, bio and email, the title of the talk, a brief abstract in English language. Please indicate whether you want to give a lightning (10min), short (20min) or long (40min) presentation and indicate the level of experience with the topic your audience should have (e.g. whether your talk will be suitable for newbies or is targeted for experienced users.) If you\u0026rsquo;d like to pitch your brand new product in your talk, please let us know as well - there will be extra space for presenting new ideas, awesome products and great new projects.\nThe presentation format is short. We will be enforcing the schedule rigorously. If you are interested in sponsoring the event (e.g. we would be happy to provide videos after the event, free drinks for attendees as well as an after-show party), please contact us. Follow @berlinbuzzwords on Twitter for updates. News on the conference will be published on our website at http://berlinbuzzwords.de.\nProgram Chairs: Isabel Drost, Jan Lehnardt, and Simon Willnauer.\nSchedule and further updates on the event will be published on http://berlinbuzzwords.de Please re-distribute this CfP to people who might be interested.\nContact us at:\nnewthinking communications GmbH\nSchönhauser Allee 6/7\n10119 Berlin, Germany\nJulia Gemählich jge@newthinking.de\nIsabel Drost isabel@apache.org\n+49(0)30-9210 596\n"},{"id":212,"href":"/apache-mahout-in-amsterdam80/","title":"Apache Mahout in Amsterdam","section":"Inductive Bias","content":" Apache Mahout in Amsterdam # On February 7th there will be an Apache Mahout meetup in Amsterdam kindly organised by JTeam. There will be two presentations - one by myself on classification with Apache Mahout as well as a second one by Frank Scholten on clustering with Apache Mahout.\nTime: 18:00\nLocation: Frederiksplein 1, 1017XK Amsterdam, The Netherlands\nLooking forward to a few days in Amsterdam.\n"},{"id":213,"href":"/fosdem-ii-2011189/","title":"FOSDEM II 2011","section":"Inductive Bias","content":" FOSDEM II 2011 # It\u0026rsquo;s already sort of a nice little tradition for me to spend the first weekend in February in Brussels for FOSDEM. This year I am particulary happy that there will be a Data Analytics Dev Room at FOSDEM. A huge Thanks to @ogrisel and @nmaillot who have done most of the heavy lifting of getting the schedule in place.\nLooking forward to an interesting Cloud Track, to meeting Peter Hintjens who is going to give a talk on 0MQ, the DevOps presentation and lots of very interesting DevRooms. Looks like again it\u0026rsquo;s going to be tough to decide on which presentations to go to at any one time. "},{"id":214,"href":"/oreilly-strata-conference305/","title":"O'Reilly Strata Conference","section":"Inductive Bias","content":" O\u0026rsquo;Reilly Strata Conference # Title: O\u0026rsquo;Reilly Strata Conference\nLocation: Santa Clara\nLink out: Click here\u0026lt;br /\u0026gt;Description: Early next February O\u0026rsquo;Reilly is planning to put on a very interesting conference on the topic of data analysis and the business of generating value from raw digital data. I\u0026rsquo;m really glad to have received the acceptance notification for my presentation and travel sponsorship from the DICODE project. So see you in Santa Clara.\nStart Date: 2011-02-01\nEnd Date: 2011-02-03\nIf you are still unsure whether you should attend or not: Strata kindly handed out discount codes to speakers to share with their followers and readers. It saves you 25% of the registration cost - just use str11fsd during registration.\n"},{"id":215,"href":"/wifi-at-the-apache-hadoop-get-together433/","title":"WiFi at the Apache Hadoop Get Together","section":"Inductive Bias","content":" WiFi at the Apache Hadoop Get Together # Just a brief reminder: The next Apache Hadoop Get Together is scheduled to take place on Thursday, January 27th at 6p.m. at the Zanox Event Campus at Media Spree Berlin.\nWe have three very interesting talks, though thirty guests registered already, we still have a few free seats. Head over to the xing event page to register if you have not done so yet.\nIf you would like to have access to the local WiFi please let me know - I need to register your mail address for that two days before the event with the venue.\nA huge thanks to Zanox for providing the location for free, another huge thanks to Cloudera for sponsoring video taping of the event.\n"},{"id":216,"href":"/apache-hadoop-get-together-berlin-january-201157/","title":"Apache Hadoop Get Together Berlin - January 2011","section":"Inductive Bias","content":" Apache Hadoop Get Together Berlin - January 2011 # This is to announce the next Apache Hadoop Get Together sponsored by Cloudera and Zanox that will take place in the Zanox Event Campus in Berlin.\nWhen: January 27th 2011, 6p.m.\nWhere: zanox Event Campus (Please mark the changed event location.)\n\u0026lt;br /\u0026gt;Größere Kartenansicht\nAs always there will be slots of 30min each for talks on your Hadoop topic. After each talk there will be a lot time to discuss. We head over to a bar after the event for some beer and something to eat.\nTalks scheduled so far:\nSimon Willnauer: \u0026ldquo;Lucene 4 - Revisiting problems for speed\u0026rdquo;\nAbstract: This talk presents a brief case study of long standing problems in Lucene and how they have been approached to gain sizable performance improvements. Each of the presented problems will have brief introduction, implemented solution and resulting performance improvements. This talk might be interesting even for non-lucene folks. Josh Devins: \u0026ldquo;Title: Hadoop at Nokia\u0026rdquo;\nAbstract: In this talk, Josh will outline some of the ways in which Nokia is using Hadoop. We will start by having a quick look at the practical side of getting started with Hadoop and outline cluster hardware and configuration and management with tools like Puppet. Next we\u0026rsquo;ll dive head first into how Hadoop and its\u0026rsquo; ecosystem are being utilized on a daily basis to perform business analytics, drive machine learning and help build data-driven products. We will also touch on how we go about collecting metrics from dozens of applications distributed in multiple data centers around the world. An open Q\u0026amp;A session will follow.\nPaolo Negri: \u0026ldquo;The order of magnitude challenge: from 100K daily users to 1M \u0026ldquo;\nAbstract: \u0026ldquo;Social games backends share many aspects of normal web applications, but exasperate scaling problems, follow this talk to see how we evolved and brought a plain ruby on rails app to sustain 5000 reqs/sec, moved part of our data from sql to nosql to reach 5 millions queries per minute and see what we learned from this experience.\u0026quot;\nPlease do indicate on Upcoming or Xing if you are coming so we can more safely plan capacities.\nA big Thank You goes to zanox for providing the venue for free for our event as well as to Cloudera for supporting videos being taped of the presentations.\nLooking forward to seeing you in Berlin,\nIsabel\n"},{"id":217,"href":"/white-christmas432/","title":"White Christmas","section":"Inductive Bias","content":" White Christmas # Christmas brought \u0026ldquo;a little surprise\u0026rdquo; to Germany last night:\nThe result this morning: a few cm (as in about 20) of snow - that is a white christmas everyone had been looking forward to. Just one question: Where to put all that snow when digging out my car? ;)\nThanks to snow clearing services streets are all white but well drivable. Other than that lots of time for enjoying the white weather. I\u0026rsquo;ll probably use the time off to to go out with a sledge tomorrow.\n"},{"id":218,"href":"/apache-hadoop-trainings-by-cloudera-in-berlin39/","title":"Apache Hadoop - Trainings by Cloudera in Berlin","section":"Inductive Bias","content":" Apache Hadoop - Trainings by Cloudera in Berlin # Cloudera is offering trainings both for Administrators as well as for Developers early next year in Berlin. If your are getting started in using Apache Hadoop this might be a great option to get your developers and operations up to speed with the framework. If you are a regular of the local Apache Hadoop Get Together a discount code should have been sent to you by mail.\n"},{"id":219,"href":"/devoxx-day-one-java-performance-and-devops155/","title":"Devoxx – Day one – Java, Performance and Devops  ","section":"Inductive Bias","content":" Devoxx – Day one – Java, Performance and Devops # In his keynote Mark Reinhold provided some information on the very interesting features to be included in the Java 7 release. Generics will be easier to declare with the diamond operator. Nested try-finally constructs that are nowadays needed to safely close resources will no longer be necessary – their will be the option of implementing a Closeable interface supporting a method close() that get\u0026rsquo;s called whenever objects of that class\u0026rsquo;s type go out of scope. That way resources can be freed automatically. Though different in concept, it still reminds me a lot of the functionality typically provided by destructors in C++.\nThe support for lambda operators and direct method references that will greately help reducing clutter due to nested inner classes has been postponed for later Java releases. Though it took 4 years to come up with the Java 7 release new features are pretty much limited. However the current roadmap looks pretty much release date driven. The intention seems to be to get developers focussed on a limited set of reachable features to finally get the release out into the hands of users.\nThe speaker claimed Oracle to remain committed to Java development – first and foremost because of being a heavy Java user themselves. However also in order to generate revenue indirectly (through selling support and consulting for Java related products), directly (through Java support) and reducing internal development cost and Java friction.\nThough Oracle had a JVM implementation of its own (jRocket) development of HotSpot will be continued – mostly due to a larger number developers being familiar with HotSpot. However monitoring and diagnosis tooling that was superior at jRocket is supposed to be ported to HotSpot.\nIn the core Java session I also went to the talk on Java performance analysis by Joshua Bloch. He a good job bringing the topic of performance analysis on complex systems to software developers. In ancient times it was quite easy to estimate a piece of code\u0026rsquo;s static performance by static code analysis. Looking at the expression if (condition \u0026amp;\u0026amp; secondCondition) it is still commonly considered to be faster to use “\u0026amp;\u0026amp;” over “\u0026amp;”. However looking at current CPU architectures that make heavy use of instruction pipelines it heavily depends on their branch prediction heuristics whether this statement is still true. Dirtying the pipeline by using \u0026amp;\u0026amp; may well be more expensive than doing the extra evaluation. General message: The performance of your code in a real world system depends on the hardware it runs on, the operating system as well as the exact VM version used. Estimating performance based on static analysis only is no longer possible.\nHowever even when doing benchmarks one might well reach false conclusions. It is common knowledge that running a benchmark on a VM is required to be run multiple times – VM warmup phases are well known to developers, so the common performance pattern for on specific function usually looks like that:\nHowever even when repeating the test on the same machine multiple times, the values seen after warm-up may be skewed substantially. The only remedy to reaching false conclusions is to do several VM runs, average of the runs (and provide median etc. that are less susceptible to outliers) and provide error bars for each averaged run. When comparing two different implementations the only way to reliably tell which one is better than the other is to do statistical significance tests. Consider the diagram below. When leaving error bars out, the left implementation seems clearly better than the right. However when taking into account how widely skewed the performance numbers are and adding error bars to the entries, this is no longer the case: Both runs are no longer statistically significantly different. "},{"id":220,"href":"/apache-mahout-hackathon-berlin70/","title":"Apache Mahout Hackathon Berlin","section":"Inductive Bias","content":" Apache Mahout Hackathon Berlin # Early next year - on February 19th/20th to be more precise - the first Apache Mahout Hackathon is scheduled to take place at c-base. The Hackathon will take one weekend. There will be plenty of time to hack on your favourite Mahout issue, to get in touch with two of the Mahout committers and get your machine learning project off the ground.\nPlease contact isabel@apache.org if you are planning to attend this event or register with the xing event so we can plan for enough space for everyone. If you have not registered for the event there is now guarantee you will be admitted.\nIf you\u0026rsquo;d like to support the event: We are still looking for sponsors for drinks and pizza.\n"},{"id":221,"href":"/apache-mahout-podcast83/","title":"Apache Mahout Podcast","section":"Inductive Bias","content":" Apache Mahout Podcast # During Apache Con ATL Michael Coté interviewed Grant Ingersoll on Apache Mahout. The interview is available online as podcast. The interview covers the goals and current use cases of the project, goes into some detail on the reasons for initially starting it. If you are wondering what Mahout is all about, what you can do with it and which direction development is heading, the interview is a great option to find out more.\n"},{"id":222,"href":"/teddy-in-antwerp393/","title":"Teddy in Antwerp","section":"Inductive Bias","content":" Teddy in Antwerp # When at Devoxx Teddy went to the city taking a few pictures of the Grote Markt, the Haven as well as the main train station.\n"},{"id":223,"href":"/apache-lunch-devoxx62/","title":"Apache Lunch Devoxx ","section":"Inductive Bias","content":" Apache Lunch Devoxx # On Twitter I suggested to host an Apache dinner during Devoxx. Matthias Wesendorf of Apache MyFaces was so kind to take up the discussion carrying it over to the Apache community mailing-list. It quickly turned out that there was quite some interest with several members and committers attending Devoxx. We scheduled the meetup for Friday after the conference during lunch time.\nI pinged a few Apache related people I knew would attend the conference (being a speaker and a committer at some Apache project almost certainly resulted in getting a ping). Steven Noels kindly made a reservation at a restaurant close by and announced time and geo coordinates on party.apache.org. Although several speakers had left already that very same morning, we turned out to be eleven people – including Stephen Coleburn, Mathias Wessendorf, Steven Noels, Martijn Dashorst of the Apache Wicked project. Was great meeting all of you – and being able to put some faces to names :)\n"},{"id":224,"href":"/devoxx-day-three156/","title":"Devoxx – Day three ","section":"Inductive Bias","content":" Devoxx – Day three # The panel discussion on the future of Java was driven by visitor submitted and voted questions on the current state and future of Java. The general take-aways for me included the clear statement that the TCK will never be made available to the ASF. The promise of Oracle to continue supporting the Java community and remaining active in the JCP.\nThere was some discussion on whether coming Java versions should be backwards-incompatible. One advantage would be the removal of several Java puzzlers thus making it easier for Joe Java to write code in Java without knowing too much about potential inconsistencies. According to Joshua Bloch the language is no longer well suited to the average programmer who just simply wants to get his tasks done in a consistent and easy to use language: It has become too complicated over the course of the years and is in bitter need for simplification.\nHaving seen his presentation in Berlin at Buzzwords and silently following the project\u0026rsquo;s progress online I skipped parts of the elastic search presentation. Instead went to the presentation on the Ghost-^wBoilerplate Busters from project Lombok. It always stroke me as odd that in a typical Java project there is so much code that can be generated automatically by Eclipse – such as getters/setters, equals/hashcode, delecation of methods and more. I never really understood why it is possible to generate all that code from Eclipse but not during compile time. Project Lombok however comes to the rescue here. As a compile time dependency it provides several annotations that are automatically converted to the correct code on the fly. It includes support for getter/setter generation, handling of closable resources (even with the current stable version of java), generation of thread safe lazy initialisation of member variables, automatic implementation of the composition over inheritance pattern and much more.\nThe library can be used from within Eclipse, in maven, ant, ivy, on Google App Engine. One of the developers in charge for IntelliJ who was in the audience announced that the library will be supported by the next version of IntelliJ as well.\n"},{"id":225,"href":"/devoxx-day-2-hbase154/","title":"Devoxx – Day 2 HBase ","section":"Inductive Bias","content":" Devoxx – Day 2 HBase # Devoxx featured several interesting case studies of how HBase and Hadoop can be used to scale data analysis back ends as well as data serving front ends. Twitter\nDmitry Ryaboy from Twitter explained how to scale high load and large data systems using Cassandra. Looking at the sheer amount of tweets generated each day it becomes obvious that with a system like MySQL alone this site cannot be run. Twitter has released several of their internal tools under a free software license for others to re-use – some of them being rather straight forward, others more involved. At Twitter each Tweet is annotated by a user_id, a time stamp (ok if skewed by a few minutes) as well as a unique tweet_id. In order to come up with a solution for generating the latter one they built a library called snowflake. Though rather simple algorithm even works in a cross data-centre set-up: The first bits are composed of the current time stamp, the following bits encode the data-centre, after that there is room for a counter. The tweet_ids are globally ordered by time and distinct across data-centres without the need for global synchronisation.\nWith gizzard Twitter released a rather general sharding implementation that is used internally to run distributed versions of Lucene, MySQL as well as Redis (to be introduced for caching tweet timelines due to its explicit support for lists as data structures for values that are not available in memcached).\nFlockDB for large scale social graph storage and analysis. Rainbird for time series analysis, though with OpenTSDB there is something comparable available for HBase. Haplocheirus for message vector caching (currently based on memcached, soon to be migrated to Redis for its richer data structures). The queries available through the front-end are rather limited thus making it easy to provide pre-computed, optimised version in the back-end. As with the caching problem a tradeoff between hit rate on the pool of pre-computed items vs. storage cost can be made based on the observed query distribution.\nIn the back-end of Twitter various statistical and data mining analysis are run on top of Hadoop HBase To compute potentially interesting followers for users, to extract potentially interesting products etc.\nThe final take-home message here: Go from requirements to final solution. In the space of storage systems there is not such thing as a silver bullet. Instead you have to carefully evaluate features and properties of each solutions as your data and load increase.\nFacebook\nWhen implementing Facebook Messaging (a new feature that was announced this week) Facebook decided to go for HBase instead of Cassandra. The requirements of the feature included massive scale, long-tail write access to the database (which more or less ruled out MySQL and comparable solutions) and a need for strict ordering of messages (which ruled out any eventually consistent system. The decision was made to use HBase.\nA team of 15 developers (including operations and frontend) was working on the system for one year before it was finally released. The feature supports for integration of facebook messaging, IM, SMS and mail into one single system making it possible to group all messages by conversation no matter which device was used to send the message originally. That way each user\u0026rsquo;s inbox turns into a social inbox.\nAdobe\nCosmin Lehene presented four use cases of Hadoop at Adobe. The first one dealt with creating and evaluating profiles of the Adobe Media Player. Users would be associated with a vector giving more information on what types of genre the meda they consumed belonged to. These vectors would then be used to generate recommendations for additional content to view in order to increase consumption rate. Adobe built a clustering system that would interface Mahout\u0026rsquo;s canopy- and k-means implementations with their HBase backend for user grouping. Thanks Cosmin for including that information in your presentation!\nA second use case focussed on finding out more on the usage of flash on the internet. Using Google to search for flash content was no good as only the first 2000 results could be viewed thus resulting in a highly skewed sample. Instead they used a mixture of nutch and HBase for storage to retrieve the content. Analysis was done with respect to various features of flash movies, such as frame rates. The analysis revealed a large gap between the perceived typical usage and the actual usage of flash on the internet.\nThe third use case involves analysis of images and usage patterns on the Photoshop-in-a-browser edition of Photoshop.com. The forth use case dealt with scaling the infrastructure that powers businesscatalyst – a turn-key online business platform solution including analysis, campaigning and more. When purchased by Adobe the system was very successful business-wise. However the infrastructure was by no means able to put up with the load it had to accommodate. Changing to a back-end based on HBase led to better performance, faster report generation.\n"},{"id":226,"href":"/devoxx-day-two-hadoop-and-hbase158/","title":"Devoxx – Day two – Hadoop and HBase ","section":"Inductive Bias","content":" Devoxx – Day two – Hadoop and HBase # In his session on the current state of Hadoop Tom went into a little more detail not only on the features released in the latest release or on the roadmap for upcoming releases (including Kerberos based security, append support, warm standby namenode and others).\nHe also gave a very interesting view on the current Hadoop ecosystem. More and more projects are currently being created that either extend Hadoop or are built on top of Hadoop. Several of these are being run as projects at the Apache Software Foundation, however some are available outside of Apache only. Using graphviz he created a graph of projects depending on or extending Hadoop and from that provided a rough classification of these projects. As to be expected HDFS and Map/Reduce are part of the very basis of this ecosystem. Right next to them sits zookeeper, a distributed coordination and looking service.\nStorage systems extending the capabilities of HDFS include HBase that adds random read/write as well as realtime access to the otherwise batch-oriented distributed file-system. With PIG and Hive and Cascading three projects are making it easier to formulate complex queries for Hadoop. Among the three, PIG is mainly focussed on expressing data filtering and processing, with SQL support being added over time as well. Hive came from the need for SQL formulation on Hadoop clusters. Cascading goes a slightly different way, providing a Java API for easier query formulation. The new kid on the block sort of is Plume, a project initiated by Ted Dunning that has the goal of coming up with a Map/Reduce abstraction layer inspired by Google\u0026rsquo;s Flume Java publication.\nThere are several projects for data import into HDFS. Sqoop can be used for interfacing with RDMBS. Chukwa and Flume deals with feeding log data into the filesystem. For general co-ordination and workflow orchestration there is the release of Oozie, originally developed at Yahoo! as well as support for workflow definition in Cascading.\nWhen storing data in Hadoop it is a common requirement to find a compact, structured representation of the data to store. Though human readable, xml files are not very compact. However when using any binary format, schema evolution commonly is a problem: Adding, renaming or deleting fields in most cases causes the need to upgrade all code interacting with the data as well as re-formatting already stored data. With Thrift, Avro and Protocol Buffers there are three options available for storing data in a compact, structured binary format. All three projects come with support for schema evolution by providing users no only to deal with missing data but also by providing a means to map old to new fields and vice versa.\n"},{"id":227,"href":"/apache-mahout-meetup-in-santa-clara82/","title":"Apache Mahout Meetup in San Jose","section":"Inductive Bias","content":" Apache Mahout Meetup in San Jose # A few hours ago the Mahout Meetup at MapR Technologies in San Jose/CA ended. Two photos taken at the event leaked - happy to be able to publish them here.\nMore information on the discussions and more technical details to follow. Stay tuned.\n"},{"id":228,"href":"/devoxx-day-two-caching157/","title":"Devoxx – Day two – Caching ","section":"Inductive Bias","content":" Devoxx – Day two – Caching # Day two started with a really good talk on caching architectures by Greg Luck. He first motivated why caching works: Even with SSIDs being available now there is still a huge performance gap between RAM access times and having to go to disk. The issue is even worse in systems that are architected in a distributed way making frequent calls to remote systems. When sizing systems for typical load, what is oftentimes forgotten is that there is no such thing as typical load: Usually the load distribution observed over one day for a service used mainly in one time zone has the shape of an elephant – most queries are issued during lunch time (head of the elephant) with another but smaller peak during the afternoon. This pattern repeats when looking at the weekly distribution, repeats again when looking at the yearly distribution. When looking at the peak time of the year, at the peak day, at the peak time your lead may be increased by several orders of magnitude compared to average load.\nAlthough query volume may be high in most applications that reach out for caching, these queries usually exhibit a power law distribution. This means that there are just a few queries being issued very frequently, however many queries are pretty seldom. This pattern allows for high cache hit rates thus reducing load substantially even during very busy times.\nThe speaker went into some more detail concerning different architectures: Usually projects start with one cache located directly on the frontend server. When scaling horizontally and adding more and more frontends this leads to an ever increasing load on the database during one period of lifetime for one cached item. The first idea employed to remedy this setup is to link the different caches to each other increasing cache hit rates. Problem here are updates racing to the various caches when the same query is issued to the backend by more than one frontend. The usual next step is to go for a distributed remote cache such as memcache. Of course this has the draw-back of now having to do a network call for each cache access slowing down response times by several milliseconds. Another problem with distributed caching systems is a theorem well known to people building distributed NoSQL databases: CAP says that you can get only two of the three desired properties consistency, availability and partition-tolerance. Ehcache with a terracotta back end lets you configure where your priority lies.\n"},{"id":229,"href":"/devoxx-university-cassandra-hbase159/","title":"Devoxx – University – Cassandra, HBase  ","section":"Inductive Bias","content":" Devoxx – University – Cassandra, HBase # During the morning session FIXME Ellison gave an introduction to the distributed NoSQL database Cassandra. Being generally based on the Dynamo paper from Amazon the key-value store distributes key/value pairs according to a consistent hashing schema. Nodes can be added dynamically making the system well suited for elastic scaling. In contrast to Dynamo, Cassandra can be tuned for the required consistency level. The system is tuned for storing moderately sized key/value pairs. Large blobs of data should not be stored into it. A recent addition to Cassandra has been the integration with Hadoop\u0026rsquo;s Map/Reduce implementation for data processing. In addition Cassandra comes with a Thrift interface as well as higher level interfaces such as Hector.\nIn the afternoon Michael Stack and Jonathan Grey gave an overview of HBase covering basic installation, going into more detail concerning the APIs. Most interesting to me was the HBase architecture and optimisation details. The systems is inspired by Google\u0026rsquo;s BigTable paper. It uses Apache HDFS as storage back-end inheriting the failure resilience of HDFS. The system uses Zookeeper for co-ordination and meta-data storage. However fear not, Zookeeper comes packaged with HBase, so in case you have not yet setup your own Zookeeper installation, HBase will do that for you on installation.\nHBase is split into a master server for holding meta-data and region servers for storing the actual data. When storing data HBase optimises storage for data locality. This is done in two ways: On write the first copy usually goes to the local disk of the client, so even when storage exceeds the size of one block and remote copies get replicated to differing nodes, at least the first copy gets stored on one machine only. During a compaction phase that is scheduled regularly (usually every 24h, jitter time can be added to lower the load on the cluster) data is re-shuffled for optimal layout. When running Map/Reduce jobs against HBase this data locality can be easily exploited. HBase comes with its own input and output formats for Hadoop jobs.\nHBase comes not only with Map/Reduce integration, it also publishes a Thrift interface, a REST interface and can be queried from an HBase shell.\n"},{"id":230,"href":"/devoxx-university-mongodb-mahout160/","title":"Devoxx University – MongoDB, Mahout","section":"Inductive Bias","content":" Devoxx University – MongoDB, Mahout # The second tutorial was given by Roger Bodamer on MongoDB. It concentrates on being horizontally scalable by avoiding joins and complex, multi document transactions. It supports a new data model that allows for flexible, changeable \u0026ldquo;schemas\u0026rdquo;. The exact data layout is determined by the types of operations you expect for your application, by the access patterns (reading vs. writing data; types of updates and types of queries). Also don\u0026rsquo;t forget about indexing tables by columns to speed up frequently run queries. Scaling MongoDB is supported by replication in a master/slave setup quite as any traditional system. In a replica set of n nodes, any of these can be elected as the primary (taking writes). If that one goes down, new master election happens. For durability all writes are required to go to at least a majority of all nodes, if that does not happen, now guarantee is given as to the availability of the update in case of primary failure. Write sharding comes with MongoDB as well.\nJava support for Mongo is pretty standard - Raw Mongo driver comes in a Map\u0026lt;\u0026hellip;, \u0026hellip; \u0026gt; flavour. Morphia supports Pojo mapping, annotations etc. for MongoDB Java integration, Code generators for various other JVM languages are available as well. See also: http://blog.wordnik.com/12-months-with-mongodb My talk was scheduled for 30min in the afternoon. I went into some detail on what is necessary to build a news clustering system with Mahout and finished the presentation by a short overview of the other use cases that could be solved with the various algorithms. In the audience, nearly all had heard about Hadoop before – most likely in the introductory session that same morning. Same for Lucene. Solr was known to about half of the attendees. Mahout to just a few. Knowing that only very few attendees had any Machine Learning background I tried to provide a very high level overview of what can be done with the library, not going into too much mathematical details. There were quite a few interested questions after the presentation – both online and offline, including requests for examples on how to integrate the software with Solr. In addition connectors for instance to HBase as a data-source were interesting to people. Show-casing integration of Mahout, possibly even providing not only Java- but also REST interfaces might be one route to easier integration and faster adoption of Mahout.\n"},{"id":231,"href":"/frau-holle-frau-holle192/","title":"Frau Holle, Frau Holle","section":"Inductive Bias","content":" Frau Holle, Frau Holle # Winter arrived in Germany - below a few pictures I took last Thursday morning:\nSeems like this season winter is trying real hard to prove that I am wrong saying that there is no real winter in Berlin with just about a week snow on the streets ;) Guess its time to get the tires with spikes back on my bike.\n"},{"id":232,"href":"/devoxx-university-productive-programmer-hbase161/","title":"Devoxx University – Productive programmer, HBase","section":"Inductive Bias","content":" Devoxx University – Productive programmer, HBase # The first day at Devoxx featured several tutorials – most interesting to me was the pragramatic programmer. The speaker also is the author of the equally named book at O\u0026rsquo;Reilly. The book was the result of the observation that developers today are more and more IDE bound, no longer able to use the command line effectively. The result are developers that are unnecessarily slow when creating software. The goal was to bring usage patterns of productive software development to juniors how grew up in a GUI only environment. However half-way through the book, it became apparent that a book on command line wizardry only is barely interesting at all. So the focus was shifted and now includes more general productivity patterns.\nThe goal was to accelerate development – mostly by avoiding time consuming usage patterns (minimise mouse usage) and automation of repetitive tasks (computers are good at doing dull, repetitive tasks – that\u0026rsquo;s what they are made for.\nSecond goal was increasing focus. Two main ingredients to that are switching off anything that disturbs the development flow: No more pop-ups, not more mail notifications, no more flashing side windows. If you have ever had the effect of thinking “So late already?” when your colleagues were going out for lunch – then you know what is meant by being in the flow. It takes up to 20min to get into this mode – but just the fraction of a second to be thrown out. With developers being significantly more productive in this state it makes sense to reduce the risk of being thrown out.\nThird goal was about canonicality, fourth one on automation.\nDuring the morning I hopped on and off the Hadoop talk as well – the tutorial was great to get into the system, Tom White went into detail also explaining several of the most common advanced patterns. Of course not that much new stuff if you sort-of know the system already :)\n"},{"id":233,"href":"/devoxx-antwerp153/","title":"Devoxx Antwerp","section":"Inductive Bias","content":" Devoxx Antwerp # With 3000 attendees Devoxx is the largest Java Community conference world-wide. Each year in autumn it takes place in Antwerp/ Belgium, in recent years in the Metropolis cinema. The conference tickets were sold out long before doors were opened this year.\nThe focus of the presentations are mainly on enterprise Java featuring talks by famous Joshua Bloch, Mark Reihnhold and others on new features of the upcoming JDK release as well as intricacies of the Java programming language itself.\nThis year for the first time the scope was extended to include one whole track on NoSQL databases. The track was organised by Steven Noels. It featured fantastic presentations on HBase use cases, easily accessible introductions to the concepts and usage of Hadoop.\nTo me it was interesting to observe which talks people would go to. In contrast to many other conferences here the NoSQL/ cloud-computing presentations were less visited than I\u0026rsquo;d have expected. One reason might be the fact that especially on conference day two they had to compete with popular topics such as the Java puzzlers, Live Java posse and others. However when talking to other attendees their seemed to be a clear gap between the two communities caused probably by a mixture of\nthere being very different problems to be solved in the enterprise world vs. the free software, requirements and scalability driven NoSQL community. Although even comparably small companies (compared to the Googles and Yahoo!s of this world) in Germany are already facing scaling issues, these problems are not yet that pervasive in the Java community as a whole. To me this was rather important to learn, as coming from a Machine learning background, now working for a search provider and being involved with Mahout, Lucene and Hadoop scalability and a growth in data has always been one of the major drivers for any projects I have been working on so far.\nEven when faced with growing amounts of data in the regular enterprise world developers seem to be faced with the problem of not being able to freely select the technologies to be used for implementing a project. In contrast to startups and lean software teams there still seem to be quite a few teams that are not only given what to implement but also how to implement the software unnecessarily restricting the tools to use to solve a given problem.\nOne final factor that drives developers adopting NoSQL and cloud computing technologies is the observation for the need to optimise the system as a whole – to think outside the box of fixed APIs and module development units. To that end the DevOps movement was especially interesting to me as only by getting the knowledge largely hidden in operations teams into development and mixing that with the skill of software developers can lead to truly elastic and adaptable systems.\n"},{"id":234,"href":"/teddy-in-amsterdam-schiphol392/","title":"Teddy in Amsterdam Schiphol","section":"Inductive Bias","content":" Teddy in Amsterdam Schiphol # Caught Teddy just before the plane took off:\n"},{"id":235,"href":"/teddy-in-lisbon398/","title":"Teddy in Lisbon","section":"Inductive Bias","content":" Teddy in Lisbon # After Apache Con I spent a few days in Lisbon for Codebits. The conference is not developers-only. It is more of a mixture of hacking event, conference, exhibition. Though the location was not optimal for giving presentations (large exhibition hall with now a rather noisy presentation area) the whole event brought quite an interesting mixture of people together in one place in the capital of Portugal.\nI had been to Portugal earlier this year, however that was just for recreating and vacation. So this time around I was quite happy to get the chance of seeing some part of the local culture that otherwise I would probably never have gotten access to. Having some loose ties to the Berlin hackers community, to the free software people in Europe but also to pragmatic open source developers what was most astonishing to me was to see the comparably huge amount of systems running Microsoft Windows used by codebits attendees. Talking a bit with locals it seemed like using free software for development is not all that unusual in Portugal, however people tend to wait for problems getting fixed instead of getting involved and actively contributing back.\n"},{"id":236,"href":"/mahout-in-action-2276/","title":"Mahout in Action","section":"Inductive Bias","content":" Mahout in Action # Flying to Atlanta I finally had a few hours of time to finalize the review of the Mahout in Action MEAP edition. The book is intended for potential users of the Apache Mahout, a project focussing on implementing scalable algorithms for machine learning.\nDescribing machine learning algorithms and their application to practioners is a non-trivial task: Usually there is more than one algorithm available for seemingly identically problem settings. In addition each algorithm usually comes with multiple parameters for fine-tuning its behaviour to the problem setting at hand.\nSean Owen does an awesome job explaining the basic concepts behind building recommender systems in that book. In a very intuitive way he highlights the properties of each algorithm and its options. Based on one example setting taken from a real world problem (parents buying music Cds for their children based on more or less background information) he highlights the properties of each available recommender algorithm.\nThe second section of the book highlights available implementations for clustering documents, that is grouping documents by similarity – a problem that is very common when it comes to grouping texts into topics and detecting upcoming new topics in a stream of publications. Robin Anil and Ted Dunning make it very easy to understand what clustering is all about, explain how to use, configure and use the current implementations in Mahout in various practical settings.\nThe book looks very promising. It is well suited for engineers looking for an explanation of how to successfully use Mahout to solve real world problems. In contrast to existing publications it makes it easy to grasp the basic concepts event without wading through complicated computations. The book is specially targeted to Mahout users. However it does give important background information on the algorithms available that is needed to decide on exactly which implementation and which configuration to use. Looking forward to the last section on classification algorithms.\n"},{"id":237,"href":"/apache-con-wrap-up16/","title":"Apache Con – Wrap up","section":"Inductive Bias","content":" Apache Con – Wrap up # After one week of lots of interesting input the ASF\u0026rsquo;s user conference was over. With a focus on Apache software users quite a few talks are not too well suited for conference regulars but more or less targeted at newbies who want to know who too successfully apply the software. As a developer of Mahout with close ties to the Lucene and Hadoop community what is of course most interesting to me are stories of users putting the software into production.\nThe conference was well organised: The foundation features way more projects at the moment than can reasonably be covered in just one conference. As a result Apache Con only covers only a subset of what is being developed at Apache. As such more and more smaller community organised events are being run by individuals as well as corporations. Still Apache Con is a nice place to get in touch with other developers – and to get a glimpse of what is going on in project outsides ones own regular community.\n"},{"id":238,"href":"/christmas-scrumtisch138/","title":"Christmas Scrumtisch","section":"Inductive Bias","content":" Christmas Scrumtisch # Today the last Scrumtisch Berlin in 2010 took place in Friedrichshain. Thanks to Marion Eickmann and Andrea Tomasini for organising the Scrum user group regularly for the past years.\nThough no presentation had been scheduled ahead of time the Scrumtisch was well attended by over twenty people, mostly from companies based in Berlin who are either using Scrum already or are currently in a transition phase.\nWe went straight into collecting and voting on topics for discussion. In total we ended up having eight potential topics listed, including but not limited to\nScrum and non-feature teams, does it work - and if, how?\nTransitioning to Scrum - what are the stake holders that must be convinced first in a company?\nScrum with teams made of people buying into Scrum and those that don\u0026rsquo;t - does that work?\nCan Scrum be combined with people who want to telecommute?\nScrum and platform development - how does that get combined?\nScrum in systems engineering, embedded development - how to setup teams?\nAfter voting we had the two clear winners discussing Scrum in teams that don\u0026rsquo;t buy into the method completely as well as telecommuting with Scrum teams.\nScrum with broken teams\nThe situation described: The attendee proposing the topic has the problem of being Scrum master at a team that does not completely buy into Scrum. There are a few developers who like being self-organising, who love short feedback cycles. However there are a few others who would rather stick with their technological niche, get tasks assigned to them and avoid taking over tasks from others.\nDuring discussion we found out that in this company Scrum had been introduced as a grass-roots movement little over a year ago. The introduction of the method led to success clearly visible in the company. In turn the method was tried on a larger team as well. However at the moment the team is at a point where it is about to break apart: Into developers happy with change, flexible enough to adapt to shift in technology and a second half that would rather continue developing the old way.\nOne very important point was raised by one of the attendees: With Scrum getting introduced so fast, compared to the length in time the company had been living before, it may well be time to slow down a bit. To sit down with the team in a relaxed environment and find out more on how everyone is assessing the current situation. Find out more on what people like about the new approach, and about what should be changed and still needs improvement. In the end it\u0026rsquo;s not a process issue but a people problem - there is a need to get the team on-board.\nTeam-building activities might help as well - let the team experience what it means to be able to rely on each other. What does it mean to learn new things in short time, to co-operate to solve tasks so far untackled?\nIf team-members start ignoring the sprint backlog working on other tasks instead there is a question about whether there is enough trust in the product-owner\u0026rsquo;s decisions. On the other hand with pressure resting on the team\u0026rsquo;s shoulders there might be a need to stop the train, fix all open issues and continue only after the project is back in shape. However also this needs all team members working towards a common goal - with everyone willing to take up any open task.\nScrum and telecommuting\nBasically the question was whether it works at all (clear yes from the audience) and if, which best practices to use. To be more precise: Does Scrum still work if some of the team members work from home a few days a week but are in the office all other time. The risk of course lies in loosing information, in the team building common knowledge. And thus becoming less productive.\nThere are technical tools that can help the process: electronic scrum boards (such as Greenhopper for JIRA or Agilo) as well as tele-conferencing systems, wikis, social networking tools, screen sharing for easier pair programming. All tools used must entail less overhead then the provide in benefit to the team however. Communication will become more costly - however if and to what extend this translates to a loss in productivity varies greatly.\nThere must be a clear commitment from both sides - the telecommuter as well as the team on-site - to keep the remote person in the loop. Actually it is easier with teams that are completely remote. This experience is sort of familiar from any open source project: With people working in different time zones it comes naturally to take any decision on a mailing list. However with some people having the chance to communicate face-to-face suddenly decisions become way less transparent. At Apache we even go as far as telling people that any decision that is not taken on the mailing list, never really was taken at all. A good insight into how distributed teams at Apache work has been given earlier by Bertrand Delacrétaz.\nFor team building reasons it may make sense to start out with a co-located team and split off people interested in working from home later on. That way people have a chance to get to know each other face-to-face which makes later digital-only communication way easier.\nThanks again to Marion and Andrea for organising today\u0026rsquo;s Scrumtisch. If you are using Scrum and happen to be in Berlin - send an e-mail to Marion to let her know you are interested in the event, or simply join us at the published date.\n"},{"id":239,"href":"/teddy-in-atlanta394/","title":"Teddy in Atlanta","section":"Inductive Bias","content":" Teddy in Atlanta # While I was happily attending Apache Con US in Atlanta/GA my teddy had a closer look at the city: He first went to the centennial olympic park, took a picture of the world of coca-cola (wondering what strange kinds of museums there are in the US. After that he headed over to Midtown having a quiet time in the Piedmont park. And finally had a closer look at the private houses still decorated for Halloween. Seems like it was squirrel day that day: Met more than ten squirrels he told me.\nI found quite some impressive pictures of the arts museum on my camera after his trip out – as well as several images taken at the campus of the Georgia tech university. It\u0026rsquo;s amazing to see what facilities are available to students – especially compared to the equipment of German universities.\n"},{"id":240,"href":"/apache-con-last-day14/","title":"Apache Con – last day","section":"Inductive Bias","content":" Apache Con – last day # Day three of Apache Con started with interesting talks on Tomcat 7, including an introduction to the new features of that release. Those include better memory leak prevention and detection capabilities – the implementation of these capabilities have lead to the discovery of various leaks that appear under more or less weird circumstances in famous open source libraries and the JVM itself. But also better management and reporting facilities are part of the new release.\nAs I started the third day over at the Tomcat track, unfortunately I missed the Tika and Nutch presentations by Chris Mattman – so happy, that at least the slides were published online: $LINK. The development of nutch was especially interesting for me as that was the first Apache project I got involved with back in 2004. Nutch started out as a project with the goal of providing an open source alternative internet-scale search engine. Based on Lucene as a indexer kernel, it also providing crawling, content extraction and link analysis.\nFocussed on building an internet scale search engine the need for a distributed processing environment quickly became apparent. Initial implementations of a nutch distributed file system and a map reduce engine lead to the creation of the Apache Hadoop project.\nIn recent years it was comparably quiet around nutch. Besides Hadoop also content extraction was factored out of the project into Apache Tika. At the moment development is getting more momentum again. Future developments are supposed to be focussed on building an efficient crawling engine. As storage backend the project wants to leverage Apache HBase, for content extraction Tika is to be used, as indexing backend Solr. I loved the presentation by Geoffrey Young on how they used Solr to replace their old MySQL search based system for better performance and more features at Ticketmaster. Indexing documents representing music CDs presents some special challenges when it comes to domain modeling: There are bands with names like “!!!”. In addition users are very likely to misspell certain artists names. In contrast to large search providers like Google these businesses usually have neither human resources nor enough log data to provide comparable coverage e.g. when implementing spell-checking. A very promising and agile approach taking instead was to parse log files for most common failing queries and from that learn more about features needed by users: There were many queries including geo information coming from users looking for an event at one specific location. As a result geo information was added to the index leading to happier users.\n"},{"id":241,"href":"/apache-con-mahout-commons-and-lucene15/","title":"Apache Con – Mahout, commons and Lucene","section":"Inductive Bias","content":" Apache Con – Mahout, commons and Lucene # The second day the track interesting to me provided an overview of some of the Apache commons projects. So seemingly small in scope and light-weight in implementation and dependencies these projects provide vital features not yet well supported by the Sun JVM. There is a commons math implementation featuring a fair amount of algebraic, numeric and trigonometric functions (among others), the commons exec framework for executing processes externally to the JVM w/o running into the danger of creating dead-locks or wasting resources.\nAfter that the Mahout and Lucene presentations were up. Grant gave a great overview of various use-cases of machine learning in the wild, rightly claiming that anyone using the internet today makes use of some machine learning powered application each day – be it e-mail spam filtering, the Gmail priority inbox, recommendaed articles on news sites, recommended items to buy at shopping sites or targeted advertisements shown when browsing. The talk was concluded by a more detailed presentation of how to successfully combine the features of Mahout and Lucene/Solr to built next generation web services that integrate user feedback into their user experience.\n"},{"id":242,"href":"/apachecon-keynotes85/","title":"ApacheCon - Keynotes","section":"Inductive Bias","content":" ApacheCon - Keynotes # The first keynote was given by Dana Blankenhorn – a journalist and blogger regularly publishing tech articles with a clear focus on open source projects. Focussed on the evolution of open source projects with a special focus on Apache.\nComing from a research background the keynote given by Daniel Crichton from NASA was very interesting to me: According to the speaker scientists are facing challenges that are all to known to large and distributed corporations. Most areas in science is currently becoming more and more dependent on data intensive experiments. Examples include but are not limited to\nThe field of biology where huge numbers of experiments are needed to decipher the internal workings of proteins, or to be able to understand the fundamental concepts underlying data encoded in DNA.\nIn physics hadron collider experiments huge amounts of data are generated with each experiment. With facilities for running such experiments are expensive to build and the amount of data generated is for too large to be analysed by just one team groups of scientists are suddenly facing the issue of exchanging data with remote research groups. They suddenly run into the requirement of integrating their system to those of other groups. All of a sudden data formats and interfaces have to somehow be standardised.\nRunning space missions used to be limited to just a very small number of research institutions in a very tiny number of countries. However this is about to change as more countries are gaining the knowledge and facilities to run space missions. Again this leads to the need to be able to collaborate towards one common goal.\nNot only are software systems so far distinct and incompatible. Even data formats used usually are incompatible. The result are scientists spending most of their time re-formatting, converting and importing datasets before being able to get any real work done. At the moment research groups are not used to working collaboratively in distributed teams. Usually experiments are run on specially crafted, one-of software that cannot be easily re-used, that does not adhere to any standards and that is being re-written over and over again by every research group. Re-using existing libraries is oftentimes a huge cultural shift as researchers seemingly are afraid of external dependencies, afraid of giving up control over part of their system.\nOne step into the right direction was taken by NASA earlier this year: They released their decision making support system OODT under a free software license (namely the Apache Software License) and put the project under incubation at Apache. The project currently is about to graduate to its own top level Apache project. This step is especially remarkable as successfully going through the incubator also means to have established a healthy community that is not only diverse but also open to accepting incoming patches and changes to the software. This means to not only give up control over your external dependencies but also having the project run in a meritocratic, community driven model. For the contributing organisation, this boils down to no longer having total control over the future roadmap of the project. In return this usually leads to higher community participation, and higher adoption in the wild.\n"},{"id":243,"href":"/apache-con-hadoop-hbase-httpd13/","title":"Apache Con – Hadoop, HBase, Httpd","section":"Inductive Bias","content":" Apache Con – Hadoop, HBase, Httpd # The first Apache Con day featured several presentations on NoSQL databases (track sponsored by Day software), a Hadoop track as well as presentations on Httpd and an Open source business track. Since its inception Hadoop always was intended to be run in trusted environments firewalled from hostile users or even attackers. As such it never really supported any security features. This is about the change with the new Hadoop release including better Kerberos based security.\nWhen creating files in Hadoop a long awaited feature was append support. Basically up to now writing to Hadoop was a one-of job: Open a file, write your data, close it and be done. Re-opening and appending data was not possible. This situation is especially bad for HBase as its design relies on being able to append data to an existing file. There have been efforts for adding append support to HDFS earlier as well as an integration of such patches by third party vendors. However only with a current Hadoop version Append is officially supported by HDFS.\nA very interesting use case-wise of the Hadoop stack was presented by $name-here from $name. They are using a Hadoop cluster to provide a repository of code released under free software licenses. The business case is to enable corporations to check their source code against existing code and spot license infringements. This does not only include linking to free software under incompatible licenses but also developers copying pieces of free code, e.g. copying entire classes or functions into internal projects that originally were available only under copyleft licenses.\nThe speaker went into some detail explaining the initial problems they had run into: Obviously it\u0026rsquo;s no good idea to mix and match Hadoop and HBase versions freely. Instead it is best practice to use only versions claimed to be compatible by the developers. Another common mistake is to leave parameters of both projects at their defaults. The default parameters are supposed to be fool-proof. However they are optimised to work well for Hadoop newbies who want to try out the system on a single node cluster and in a distributed setting obviously need more attention. Other anti-patterns include storing only tiny little files in the cluster thus quickly running out of memory on the namenode (that stores all file information including block mappings in main memory for faster access).\nIn the NoSQL track Jonathan Grey from Facebook gave a very interesting overview on the current state of HBase. Turns out that Facebook would announce only a few days after that their internal use of HBase for the newly launched feature of Facebook messaging.\nHBase has adopted a release cycle including development/ production releases to get their systems into interested users\u0026rsquo; hands more quickly: Users willing to try out new experimental features can use the development releases of HBase. Those who are not should go for the stable releases.\nAfter focussing on improving performance in the past months the project is currently focussing on stability: Data loss is to be avoided by all means. Zookeeper is to be integrated more tightly for storing configuration information thus enabling live reconfiguration (at least to some extend). In addition also HBase is targeting to integrate stored procedures like behaviour: As explained in Googles Percolator paper $LINK batch oriented processing get\u0026rsquo;s you only so far. If data that gets added constantly it makes sense to give up on some of the throughput batch-based systems provide and instead optimise for shorter processing cycles by implementing event triggered processing.\nOn recommendation of one of neofonie\u0026rsquo;s sys-ops I visited some of the httpd talks: First Rich Bowen gave an overview of unusual tasks one can solve with httpd. The list included things like automatically re-writing http response content to match your application. There is even a spell checker for request URLs: Given marketing has given your flyer to the press with a typo in the url, chances are that the spellchecking module can fix these automatically for each request: Common mistakes covered are switched letters, numbers replaced by letters etc. The performance cost has to be paid only in case no hit could be found – so instead of returning a 404 right away the server first tries to find the document by taking into account common mis-spellings.\n"},{"id":244,"href":"/apache-con-hackathon-days12/","title":"Apache Con – Hackathon days","section":"Inductive Bias","content":" Apache Con – Hackathon days # This year on Halloween I left for a trip to Atlanta/GA. Apache Con US was supposed to take place there featuring two presentations on Apache Mahout – one by Grant Ingersoll explaining how to use Mahout to provide better search features in Solr, one by myself with a general introduction to what features Mahout provides, giving a bit more detailed information on how to use Mahout for classificaiton.\nI spent most of Monday in Sally Khudairi\u0026rsquo;s media training. In the morning session she explained the Ins and Outs of successfully marketing your open source project: One of the most important questions is to be able to provide a dense but still accessible explanation of what your project is all about and how it differentiates from other projects potentially in the same space. As a first exercise attendees would meet in pairs interviewing each other about their respective project. When summarising the information I had gotten, Sally quickly pointed out additional pieces of valuable information I had totally forgotten to ask about: First of all the full name of the interviewee, including the sur-name. Second the background of the person with respect to the project. It seemed all to natural that someone you meet at Apache Con in a Media Training almost certainly is either founder or core-committer to the project. Still it is interesting to know more on how long he has been contributing, whether he maybe even co-founded the project.\nAfter that first exercise we would go into detail on various publication formats. When releasing project information the first format that comes to mind are press releases. For software projects at the ASF these are created in a semi-standardised format containing\nBackground on the foundation itself.\nSome general background on the project.\nA few paragraphs on the news to be published on the project in an easily digestible format.\nContact information for more details.\nSome of these parts can be re-used across different publications and occasions. It does make sense to keep these building blocks as a set of boilerplates ready to use when needed.\nAfter lunch Michael Coté from redmonk visited us. Michael has a development background, currently he works as business analyst for redmonk. It is fairly simple to explain technical projects to fellow developers. To get some experience in explaining our project also to non-technical people Sally invited Michael to interview us. By the end of the interview Michael asked each whether they had any question for him. As understanding what machine learning can do for your average Joe programmer is not all to trivial I simply asked him for strategies for better explaining or show-casing our project. One option that came to his mind was to come up with one – or a few – example show cases where Mahout is applied to freely available datasets. Currently most data analysis systems are rather simple or based only on a very limited set of data. Showing on a few selected use cases what can be done with Mahout should be a good way to get quite some media attention for the project.\nDuring the remaining time of the afternoon I started working a short explanation of Mahout and our latest release. The text was reviewed by the Mahout community. The text was published by Sally on the blog of the Apache Software foundation. I also used it as a basis for an article on heise open that got published that same day.\nThe second day was reserved for a mixture of attending the Barcamp session and hacking away at the Hackathon. Ross had talked me into giving an overview of various Hadoop use cases as that was requested by one of the attendees. However it turned out the guy wasn\u0026rsquo;t really interested in specific use cases: The discussion quickly turned into the more fundamental question of how far the ASF should go in promoting its projects. Should there be a budget for case studies? Should there even be some marketing department. Well, clearly that is out of scope for the foundation. And in addition would run contrary to it being a neutral ground for vendors to collaborate towards common goals while still separately making money providing consulting services, selling case studies etc.\nDuring the Hackathon I was turned into a Mentor for Stanbol, a new project entering incubation just now. In addition I spent some time to finally catch up with the Mahout mailing list.\n"},{"id":245,"href":"/travelling414/","title":"Travelling","section":"Inductive Bias","content":" Travelling # Currently on my way back from a series of conferences in the past three weeks in the IC from Schiphol. After three weeks of conferences, lots of new input and lots of interesting projects I learned about it is finally time to head back and put the stuff I have learned to good use.\nView Travelling in a larger map\nAs seems normal with open source conferences I got far more input on interesting projects than I can expect to ever get applied in on a daily basis. Still it is always inspiring to meet with other developers in the same field – or even quite different fields and learn more on what projects they are working on, how they solve various problems.\nA huge Thank You goes to the DICODE EU research project for sponsoring the Apache Con and Devoxx trips, another Thanks to Sapo.pt for inviting me to Lisbon and covering travel expenses. A special thank you to the assistant at neofonie who made travel arrangements for Atlanta and Antwerp: It all worked without problems even up to me having a power outlet in the train that is finally taking me back.\n"},{"id":246,"href":"/apache-mahout-halloween-release65/","title":"Apache Mahout 0.4 release","section":"Inductive Bias","content":" Apache Mahout 0.4 release # On last Sunday the Apache Mahout project published the 0.4 release. Nearly every piece of the code has been refactored and improved since the last 0.3 release. The release was timed to happen exactly before Apache Con NA in Atlanta. As such it was published on October 31st - the Halloween release, sort-of.\nEspecially mentionable are the following improvements:\nModel refactoring and CLI changes to improve integration and consistency\nMap/Reduce job to compute the pairwise similarities of the rows of a matrix using a customizable similarity measure\nMap/Reduce job to compute the item-item-similarities for item-based collaborative filtering\nMore support for distributed operations on very large matrices\nEasier access to Mahout operations via the command line\nNew vector encoding framework for high speed vectorization without a pre-built dictionary\nAdditional elements of supervised model evaluation framework\nPromoted several pieces of old Colt framework to tested status (QR decomposition, in particular)\nCan now save random forests and use it to classify new data\nNew features and algorithms include:\nNew ClusterEvaluator and CDbwClusterEvaluator offer new ways to evaluate clustering effectiveness\nNew Spectral Clustering and MinHash Clustering (still experimental)\nNew VectorModelClassifier allows any set of clusters to be used for classification\nRecommenderJob has been evolved to a fully distributed item-based recommender\nDistributed Lanczos SVD implementation\nNew HMM based sequence classification from GSoC (currently as sequential version only and still experimental)\nSequential logistic regression training framework\nNew SGD classifier\nExperimental new type of NB classifier, and feature reduction options for existing one\nThere were many, many more small fixes, improvements, refactorings and cleanup. Go check out the new release, give the new features a try and report back to us on the user mailing list.\n"},{"id":247,"href":"/scrumtisch-november359/","title":"Scrumtisch November","section":"Inductive Bias","content":" Scrumtisch November # Title: Scrumtisch November\nLocation: Berlin\nLink out: Click here\u0026lt;br /\u0026gt;Description: Next Scrum meetup - Scrumtisch - is scheduled to take place end of November in Berlin FHain. See you there (that is at La Vecchia Trattoria) if you are interested in agile development. Please make sure to register with Marion first.\nStart Time: 18:30\nDate: 2010-11-29\n"},{"id":248,"href":"/apache-mahout-devoxx-tools-in-action-track67/","title":"Apache Mahout @ Devoxx Tools in Action Track","section":"Inductive Bias","content":" Apache Mahout @ Devoxx Tools in Action Track # This year\u0026rsquo;s Devoxx will feature several presentations coming from the Apache Hadoop ecosystem including Tom White on the basics of Hadoop: HDFS, MapReduce, Hive and Pig as well as Michael Stack on HBase.\n\u0026lt; br\u0026gt;\nIn addition there will be a brief Tools in Action presentation on Monday evening featuring Apache Mahout.\nPlease let me know if you are going to Devoxx - would be great to meet some more Apache people there, maybe have dinner at one of the conference days.\n"},{"id":249,"href":"/apache-mahout-lisbon-codebits68/","title":"Apache Mahout @ Lisbon Codebits","section":"Inductive Bias","content":" Apache Mahout @ Lisbon Codebits # Second week of November I\u0026rsquo;ll spend a few days in Lisbon - never would have thought that I\u0026rsquo;d return so quickly when I visited this beautiful city this summer during vacation. I\u0026rsquo;ll be there for Codebits - thanks to Sapo for inviting me to be there.\nBack in summer I learned only after I returned to Germany that there was someone form Portugal seeking to meet with other Apache people exactly when I was down there. I contacted the guy proposing to do an Apache Dinner to see how many other committers and friends could be reached. In addition Filipe asked me whether I could imagine flying down to Sapo to give a talk on Mahout as devs there would be interested in it. Well, I told him that if I got travel support, I\u0026rsquo;d be happy to be there. This 10min chat quickly turned into an invitation to a great conference in Lisbon. Looking forward to meet you there. (And looking forward to weather that compared to Germany is way warmer and more sunny right now. :) )\n"},{"id":250,"href":"/first-steps-with-git177/","title":"First steps with git","section":"Inductive Bias","content":" First steps with git # A few weeks ago I started to use git not only for tracking changes in my own private repository but also for Mahout development and for reviewing patches. My setup probably is a bit unusual, so I thought, I\u0026rsquo;d first describe that before diving deeper into the specifc steps.\nWorkflow to implement\nWith my development I wanted to follow Mahout trunk very closely, integrating and merging any changes as soon as I continue to work on the code. I wanted to be able to work with two different machines on the client side that are located at two distinct physical locations. I was fine with publishing any changes or intermediate progress online.\nThe tools used\nI setup a clone of the official Mahout git repository on github as a place the check changes into and as a place to publish my own changes.\nOn each machine used, I cloned this github repository. After that I added the official Mahout git repository as upstream repository to be able to fetch and merge in any upstream changes.\nCommand set\nAfter cloning the official Mahout repository into my own github account, the following set of commands was used on a single client machine to clone and setup the repository. See also the Github help on forking git repositories.\n#clone the github repository\ngit clone git@github.com:MaineC/mahout.git\n#add upstream to the local clone\ngit remote add upstream git://git.apache.org/mahout.git\nOne additional piece of configuration that helped make life easier was to setup a list of files and file patterns to be ignored by git.\nEach distinct changeset (be it code review, code style changes or steps towards own changes) would then be done in their own branches locally. To share them with other developers as well as make them accessible to my second machine I would use the following commands on the machine used for initial development:\n#create the branch\ngit branch MAHOUT-666\n#publish the branch on github\ngit push origin MAHOUT-666\nTo get all changes both from my first machine and from upstream into the second machine all that was needed was:\n#select correct local branch\ngit checkout trunk\n#get and merge changes from upstream\ngit fetch upstream\ngit merge upstream/trunk\n#get changes from github\ngit fetch origin\ngit merge origin/trunk\n#get branch from above\ngit checkout -b MAHOUT-666 origin/MAHOUT-666\nOf course pushing changes into an Apache repository is not possible. So I would still end up creating a patch, submit that to JIRA for review and in the end apply and commit that via svn. As soon as these changes finally made it into the official trunk all branches created earlier were rendered obsolete.\nWhat still makes me stick with git especially for reviewing patches and working on multiple changesets is it\u0026rsquo;s capability to quickly and completely locally create branches. This feature totally changed my so-far established workflow for keeping changesets separate:\nWith svn I would create a separate checkout of the original repository from a remote server, make my changes or even just apply a patch for review. To speed things up or be able to work offline I would keep one svn checkout clean, copy that to a different location and only there apply the patch.\nIn combination with using an IDE this workflow would result in me having to re-import each different checkout as a separate project. Even though both Idea and Eclipse are reasonably fast with importing and setting up projects it would still cost some time.\nWith git all I do is one clone. After that I can locally create branches w/o contacting the server again. I usually keep trunk clean from any local changes - patches are applied to separate branches for review. Same happens to any code modifications. That way all work can happen when disconnected from the version control server.\nWhen combined with IntelliJ Idea fun becomes even greater: The IDE regularly scans the filesystem for updated files. So after each git checkout I\u0026rsquo;ll find the IDE automatically adjust to the changed source code - that way avoiding project re-creation. Same is of course possible with Eclipse - it just involves one additional click on the Refresh button.\nFor me git helped speed up my work processes and supported use cases that otherwise would have involved sending patches to and fro between separate mailboxes. That way work with patches and changeset seemed way more natural and better supported by the version control system itself. In addition it of course is a great relief to be able to checkin, diff, log, checkout etc. even when disconnected from the network - which for me still is one of the biggest advantages of any distributed version control system. Update\nLance Norskog recently pointed out one more step that is helpful:\nYou didn\u0026rsquo;t mention how to purge your project branch out of the github fork. From http://help.github.com/remotes/: Deleting a remote branch or tag\nThis command is a bit arcane at first glance… git push REMOTENAME :BRANCHNAME. If you look at the advanced push syntax above it should make a bit more sense. You are literally telling git “push nothing into BRANCHNAME on REMOTENAME”. And, you also have to delete the branch locally also.\n"},{"id":251,"href":"/scientific-debugging337/","title":"Scientific debugging","section":"Inductive Bias","content":" Scientific debugging # Quite some years ago I ready Why programs fail - a systematic guide to debugging - a book written on the art of debugging programs written by Andreas Zeller (Prof. at University Saarbrücken, researcher working on Mining Software Archives, Automated Debugging, Mutation Testing and Mining Models and one of the authors of famous Data Display Debugger).\nOne aspect that I found particularly intriguing about the book was up to then known to me only from the book Zen and the Art of Motorcycle Maintenance: Scientific debugging as a way to find bugs in a piece of software in a structured way with methods usually known from the scientific method.\nWhen working together with software developers one theme that often occured to me as very strange is other developers - especially Junior level developers\u0026rsquo; - ways of debugging programs: When faced with a problem they would usually come up with some hypothesis as to what the problem could be. Without verifying the correctness of that hypothesis they would jump directly to implementing a fix for the perceived problem. Not too seldom this led to either half baked solutions somewhat similar to the Tweaking the Code behaviour on \u0026ldquo;The daily WTF\u0026rdquo;, or - even worse - to no solution at all after days of implementation time were gone.\nFor the rest of this posting I\u0026rsquo;ll make a distinction between the terms\ndefect for a programming error that might cause unintended behaviour.\nfailure for an observed mis-behaviour of a program. Failures are caused by specific defects.\nFor all but trivial bugs (for your favourite changing and vague definition of \u0026ldquo;trivial\u0026rdquo;) I try to apply a more structured approach to identifying causes for failures. Identify a failure. Reproduce the failure locally whenever possible. To get most out of the process write some automated test that reproduces the failure.\nForm a hypothesis. Usually this is done sub-consciously by most developers. This time around we explicitly write down what we think what the underlying defect is.\nDevise an experiment. The goal here is to show experimentally that your hunch about the defect was correct. This may involve debug statements, more extensive logging, using your favourite debugger with a break point set exactly at the position where you think the defect lies. However the experiment could also involve using wireshark or tcpdump if you are debugging even simple distributed systems. On the other hand for extremely trivial defects the experiment could just be to fix the defect and see the test run through successfully.\nObserve results.\nReach a conclusion. Interpret the result of your experiment. If they reject your hypothesis move on to your next potential cause for the failure. If they don\u0026rsquo;t you can go on to either devise more experiments in support of your hypothesis if the last one didn\u0026rsquo;t convince you (or your boss) or fix the defect just found. In any case you should tidy up any remains of your experiment before moving on in most cases.\nWhen taking a closer look at the steps involved it\u0026rsquo;s actually pretty straight forward. This makes this method so easy to use while still being most effective. When combined with automated testing it even helps when squashing bugs in code that is not written by the person fixing it. One way to make the strategy even stronger is to support the process by manually writing down a protocol of the debugging session with pen and paper. Not only does this help avoid checking the same hypothesis over and over again. It\u0026rsquo;s also a way to quickly note down all hypothesis\u0026rsquo;: In the process of debugging the person doing the analysis might be faster thinking of new possible causes than he can actually check. Noting them down helps keeping one\u0026rsquo;s mind free and open as well as remembering all possible options.\n"},{"id":252,"href":"/cfp-data-analysis-dev-room-at-fosdem-2011135/","title":"CfP: Data Analysis Dev Room at Fosdem 2011","section":"Inductive Bias","content":" CfP: Data Analysis Dev Room at Fosdem 2011 # Call for Presentations: Data Analysis Dev Room, FOSDEM\nhttp://fosdem.org\n5 February 2011\n1pm to 7pm\nBrussels, Belgium\nThis is to announce the Data Analysis DevRoom co-located with FOSDEM. The first Meetup on analysing and learning from data, taking place in Brussels, Belgium.\nImportant Dates (all dates in GMT +2):\nSubmission deadline: 2010-12-17\nNotification of accepted speakers: 2010-12-20\nPublication of final schedule: 2011-01-10\nMeetup: 2011-02-05\nData analysis is an increasingly popular topic in the hacker community. This trend is illustrated by declarations such as:\n\"I keep saying the sexy job in the next ten years will be statisticians. People think I’m joking, but who would’ve guessed that computer engineers would’ve been the sexy job of the 1990s? The ability to take data—to be able to understand it, to process it, to extract value from it, to visualize it, to communicate it—that’s going to be a hugely important skill in the next decades, not only at the professional level but even at the educational level for elementary school kids, for high school kids, for college kids. Because now we really do have essentially free and ubiquitous data. So the complimentary scarce factor is the ability to understand that data and extract value from it.\"\n-- Hal Varian, Google’s chief economist\nTopics\nThe event will comprise presentations on scalable data processing. We invite you to submit talks on the topics:\nInformation retrieval / Search\nLarge Scale data processing\nMachine Learning\nText Mining\nComputer vision\nLinked Open Data\nSample list of related open source / data projects (not exhaustive) :\nhttp://lucene.apache.org\nhttp://hadoop.apache.org (including MapReduce, Pig, Hive, ...)\nhttp://www.r-project.org/\nhttp://scipy.org\nhttp://mahout.apache.org\nhttp://opennlp.sour ceforge.net\nhttp://nltk.org\nhttp://opencv.willowgarage.com\nhttp://mloss.org \u0026 http://mldata.org\nhttp://dbpedia.org \u0026 http://freebase.com\nClosely related topics not explicitly listed above are welcome.\nHigh quality, technical submissions are called for, ranging from principles to practice.\nWe are looking for presentations on the implementation of the systems themselves, real world applications and case studies.\nSubmissions should be based on free software solutions.\nSubmission\nProposals should be submitted at fosdem.datadevroom@gmail.com no later than 2010-12-17. Acceptance notifications will be sent out on 2010-12-20.\nPlease include your name, bio and email, the title of the talk, a brief abstract in English language. Please indicate the level of experience with the topic your audience should have (e.g. whether your talk will be suitable for newbies or is targeted for experienced users.)\nThe presentation format is short: 30 minutes including questions. We will be enforcing the schedule rigorously.\nSponsoring\nIf you are interested in sponsoring the event (e.g. we would be happy to provide videos after the event, free drinks for attendees as well as an after-show party), please contact us. Note: \"DataDevRoom sponsors\" will not be endorsed as \"FOSDEM sponsors\" and hence not listed in the sponsors section on the fosdem.org website.\nAnnouncements\nFollow @DataDevRoom on twitter for updates. News on the conference will be published on our website at http://fosdem.org.\nProgram Chairs:\nOlivier Grisel - @ogrisel\nIsabel Drost - @MaineC\nNicolas Maillot - @nmaillot\nPlease re-distribute this CFP to people who might be interested. "},{"id":253,"href":"/video-max-heimel-on-sequence-tagging-w-apache-mahout425/","title":"Video: Max Heimel on sequence tagging w/ Apache Mahout","section":"Inductive Bias","content":" Video: Max Heimel on sequence tagging w/ Apache Mahout # Some time ago Max Heimel from TU Berlin gave presentation of the new HMM support in the Mahout 0.4 release at the Apache Hadoop Get Together in Berlin:\nMahout Max Heimel from Isabel Drost on Vimeo.\nThanks to JTeam for sponsoring video taping, thanks to newthinking for providing the location and thanks to Martin Schmidt from newthinking for producing the video.\n"},{"id":254,"href":"/machine-learning-gossip-meeting-berlin271/","title":"Machine Learning Gossip Meeting Berlin","section":"Inductive Bias","content":" Machine Learning Gossip Meeting Berlin # This evening the first Machine Learning Gossip meeting is scheduled to take place at 9p.m. at Victoriabar: Professionals working in research advancing machine learning algorithms and industry projects putting machine learning algorithms to practical use meet for some drinks, food and hopefully lots of interesting discussions.\nIf successful the meeting is supposed to take place on a regular schedule. Ask Michael Brückner for the date and location of the next meetup.\n"},{"id":255,"href":"/part-4-constant-evaluation-and-improvement-finding-sources-for-feedback324/","title":"Part 4: Constant evaluation and improvement: Finding sources for feedback.","section":"Inductive Bias","content":" Part 4: Constant evaluation and improvement: Finding sources for feedback. # In recent years demand for shorter feedback cycles especially in software development has increased. Agile development, lean management and even Scrum are all for short feedback cycles: Coming from the dark ages when software projects would last for months or even years before any results could be delivered to customers we are transforming development into a process that integrates the customer in the design and evolution of his own product. Developers have learned that planning ahead for years does not work: It\u0026rsquo;s not only customers changing their mind so fast, it\u0026rsquo;s requirements changing quickly. The only achievement from months-long cycles is getting input on your work later, being able to hide deficiencies longer.\nHowever not only for planning and management does it make sense to install fast feedback loops. A few days ago I finished reading the book \u0026ldquo;Apprenticeship patterns\u0026rdquo;. A book that gives an overview of various patterns that help improve software development skills.\nOne major theme was about getting fast feedback constantly. On the (agile) development side, automated tests (unit and integration) and continuous integration systems are technical aids that can help. Pair programming and code review take the idea of fast feedback one step further by having humans give you feedback on what cannot possibly be evaluated automatically.\nThere is just one minor glitch: With really short feedback loops any mistake you make gets revealed instantly. This is not particularly special to agile development. Another area with these kinds of fast feedback loops are projects developing free software - see also the last paragraph in Bertrand\u0026rsquo;s blog on This is how we work at Apache.\nThere are developers who have a hard time living with exposing their work to a wider audience quickly. However it has the huge benefit of revealing any errors as early as possible - ideally at a point in time where fixing them is still cheap. Examples for options spotting mistakes early in various environments are listed below:\nOpen source development: Mistakes are revealed during code review of patches (or checkins).\nScrum: Speeding up error spotting can be implemented by closely integrating the product owner during each sprint. As soon as a feature is done - according to the developer team - it gets reviewed by the product owner this way reducing risk of features getting rejected during the sprint review.\nIn the team: Get each change set either mailed to all developers allowing for fast review of incoming code.\nThese are all good ways for feedback, however what about non-coding areas? Are there ways to build fast feedback into tasks that do not involve coding? I\u0026rsquo;ll pick just one example to highlight some ways that facilitate fast feedback in a non-coding environment.\nFrom time to time even hard-core coders have to meet face-to-face to discuss new designs, learn more about customers\u0026rsquo; requirements. This may involve going to conferences, giving talks, organising internal workshops, public meetups or even conferences.\nUsually people doing the organisation are too busy to \u0026ldquo;watch what happens\u0026rdquo;: They already drown in tasks to do and have no time (and are in no good position) to objectively evaluate the conference\u0026rsquo;s quality.\nStill there are ways to build feedback loops even into this kind of setup. Most of them have to do with communication:\nAsk people to give you feedback in an official feedback form: Don\u0026rsquo;t expect more than a fraction of the actual attendees to answer that form. Still it can be a source for honest feedback when done correctly. Include a few open questions, don\u0026rsquo;t ask people to rate each and every task - they\u0026rsquo;ll never find the time to do that anyway. Read the free-form comments - usually they provide way more insight than any rating question anyway.\nTalk to people, ask for proposals on what you should do differently next time.\nWatch what people are telling on the net - however keep in mind that those statements usually are a bit biased showing only the extreme ends of the spectrum.\nThe same applies to people giving presentations: Talk to people from the audience after your presentation is over. If your talk was video-taped, you are in a really great situation, as now you can judge for yourself what should be improved and where there are glitches in your arguments.\nAccording to my experience people very rarely give explicit feedback - except when being really disappointed or extremely positively surprised. However when asked for precise feedback on specific aspects people are usually more than willing to share their experience, tell you where to improve and what to change. Usually it turns out to be a good idea to actively seek out people for evaluation of your projects to get better at what you do, to encourage peers to tell you what you do wrong or even where you could get slightly better.\n"},{"id":256,"href":"/video-sebastian-schelter-on-recommendation-w-apache-mahout427/","title":"Video: Sebastian Schelter on Recommendation w/ Apache Mahout","section":"Inductive Bias","content":" Video: Sebastian Schelter on Recommendation w/ Apache Mahout # A few weeks ago we had the autumn edition of the Apache Hadoop Get Together in newthinking store in Berlin. I am glad to announce the first video online:\nMahout Sebastian Schelter from Isabel Drost on Vimeo.\nThanks to JTeam for sponsoring video taping, thanks to newthinking for providing the location and thanks to Martin Schmidt from newthinking for producing the video.\nStay tuned for the second video to be published next week.\n"},{"id":257,"href":"/apache-mahout-at-apache-con-na69/","title":"Apache Mahout at Apache Con NA","section":"Inductive Bias","content":" Apache Mahout at Apache Con NA # The upcoming Apache Con NA to take place in Atlanta will feature several tracks relevant to users of Apache Mahout, Lucene and Hadoop: There will be a full track on Hadoop as well as one on NoSQL on Wednesday featuring talks on the framework itself, Pig and Hive as well as presentations from users on special use cases and on their way of getting the system to production.\nThe track on Mahout, Lucene and friends starts on Thursday afternoon, followed by another series of Lucene presentations on Friday.\nAlso don\u0026rsquo;t miss the track on the community and business tracks for a glimpse behind the scenes. Of course there will also be tracks on well-known Apache Tomcat, httpd, OSGi and many, many more.\nLooking forward to meeting you in Atlanta!\n"},{"id":258,"href":"/salsa-steps334/","title":"Salsa steps","section":"Inductive Bias","content":" Salsa steps # If you always wanted to start learning Salsa and never really had the time to do so: Today a new beginners\u0026rsquo; course starts at Salsa Con Corazon in Berlin Schöneberg. The course is still open for registration teaching the very basics in four sessions - one per week.\n"},{"id":259,"href":"/slides-available373/","title":"Slides available","section":"Inductive Bias","content":" Slides available # Yesterday evening the Autumn Hadoop Get Together took place in Berlin. The meetup this time focussed mainly on latest developments at Apache Mahout. The meetup was kindly sponsored by JTeam, providing video taping of the presentations as well as for free drinks. Thanks a lot for that.\nAfter the meetup the group went over to Cafe Aufsturz for drinks, food and lots of interesting discussions - I left them there as I still have to get rid of a persistent cold. Hope you guys had fun!\nThe two speakers were so kind to provide the slides:\nIntroduction\nMax Heimel: “Hidden Markov Models for Apache Mahout”\nSebastian Schelter: \u0026ldquo;Distributed Itembased Collaborative Filtering with Apache Mahout”\nVideos of the talks will be posted very soon - so stay tuned.\n"},{"id":260,"href":"/reminder-apache-hadoop-get-together-berlin-today332/","title":"Reminder: Apache Hadoop Get Together Berlin Today","section":"Inductive Bias","content":" Reminder: Apache Hadoop Get Together Berlin Today # Just a brief reminder: The Apache Hadoop Get Together Berlin is supposed to take place today in newthinking store, Tucholskystr. 48 at 5p.m.\nThe meeting features two talks on Apache Mahout: Committer Sebastian Schelter will explain how to scale recommender systems with Mahout. Contributor Max Heimel is going to give an introduction to the sequence labeling facilities available in Mahout.\nAs usual the group will move over to Cafe Aufsturz after the meetup is over.\nA big Thanks goes to JTeam for sponsoring video taping as well as to newthinking store for providing the venue for free.\n"},{"id":261,"href":"/a-get-together-checklist4/","title":"A Get Together Checklist","section":"Inductive Bias","content":" A Get Together Checklist # Still on the list of potentially interesting books: The Checklist Manifesto - explaining why checklists can still be valuable - especially for complex problems and tasks.\nThough not very complex, I chose to come up with a checklist for running a Hadoop Get Together in Berlin as an exercise. I\u0026rsquo;m trying to stick with advise provided by the Checklist for Checklists.\nParties involved\nFind two to three speakers two months in advance.\nFind a sponsor for the videos.\nGathering information\nDouble check time and date with all speakers and newthinking store.\nGet name, title, abstract from the speakers.\nGet logo and exact conditions from sponsor.\nSpreading the word\nPut together an announcement text including thanks to video and venue sponsors.\nPublish the event on Upcoming.\nPublish the event on Xing.\nAugment the announcement text by the Xing event and Upcoming links.\nSend a newsletter to the Meetup Xing group.\nSend the text to the Get Together mailing list, and if appropriate to the Hadoop, HBase, katta, Lucene, Solr and Mahout mailing lists.\nOn event day send a reminder to the Get Together mailing list\nCreate meetup intro slides including thanks for the sponsors, schedule, announcements of future events.\nDuring the meetup\nMention newthinking bar during introduction.\nSelf-introduction of all participants.\nGet mail addresses of future mailing list subscribers.\nKeep presentations at 30 to 40 minutes.\nGet speakers\u0026rsquo; slides.\nAfter the event\nPublish talks\u0026rsquo; slides.\nPublish links to videos.\nThe more meetups you have run the larger the chance of the main organiser getting sick the day the meetup takes place. To avoid having to re-schedule the event make sure there are people that are capable and willing to take over moderation.\n"},{"id":262,"href":"/apache-dinner-october-today-in-potsdam36/","title":"Apache Dinner October Today in Potsdam","section":"Inductive Bias","content":" Apache Dinner October Today in Potsdam # The Apache Dinner Berlin will take place today. As always, everybody is invited even if you didn\u0026rsquo;t participate in the poll ( http://doodle.com/8bi2456enwe6z2g6). This time around the dinner was organised by Daniel Naber. Thanks!\nWhen: Monday, 2010-10-04, 19:30\nWhere: Lindencafe, Rudolf-Breitscheid-Str. 47/48, 14482 Potsdam-Babelsberg, This is directly next to the S-Bahn (S7, Babelsberg).\nLooking forward to meeting you there!\n"},{"id":263,"href":"/are-devs-contribution-to-oss-happier107/","title":"Are devs contributing to OSS happier?","section":"Inductive Bias","content":" Are devs contributing to OSS happier? # When talking to fellow developers or meeting with students it happens from time to time that I get the question of why on earth I spent my freetime working on an open source project? Why do I spend weekends at developers\u0026rsquo; conferences like FOSDEM? Why do spent afternoons organising meetups? Why is it that I am reviewing and writing code after work for free?\nUsually I point people to a post by Shalin explaining some of his reasons to contribute to open source. The post quite nicely summarises most reasons that match well with why I contribute back.\nOn the Apache Community mailing list Grant Ingersoll asked the question about whether devs who work on or use open source are happier in their employment.\nIn his response Mike posted a link to a video on what motivates people that adds another piece of information to the question of why work on open source software can be perceived as very rewarding though no money is involved: With people doing cognitively challenging tasks, motivation via payment can get you only so far. There are other motivational factors that might play an equal if not larger role in getting people to perform well on their day-to-day work:\nAutonomy: If people are supposed to be engaged with their project they need time and freedom to chose how to solve their tasks. Many large engineering driven companies like Google or Atlassian have gone even further by introducing the concept of giving people a day a week to work on what they want how they want provided they share their results. These so-called 20% projects have shown to have high potential of turning into new, creative project ideas but also even into bugs or problems getting fixed.\nMastery: Great developers strive to get better at what they do - simply because realizing that you actually learn something and get better at what you do can be very satisfying. One way of achieving that goal is to work together with peers on common projects. The larger the pool of peers to draw from, the higher the probability of you finding mentors to help you out and to point out mistakes you make.\nThere is one more factor why working on open source increases your coding level that should not be underestimated. Grant Ingersoll nicely described it in the thread mentioned above: \u0026ldquo;I was just talking with a friend yesterday, and fellow committer, who said he is a much better programmer since contributing. Of course, it makes sense. If your underwear is on display for all to see, you sure better make sure it is clean!\u0026quot;\nPurpose: People like to work on projects for a purpose. Be it to make all information accessible to the world or to turn earth into a better place by making cheap calls available to everyone. As a counter example deploying some software only for the purpose of selling a license and not make life of your client better by recommending the best solution to help solve his problem may not be half as satisfying.\nThere is quite some documentation out there on what drives people who contribute to open source projects. The video shared by Mike nicely summarizes some of the motivations of people that are independent of open source work but are closely related to it.\n"},{"id":264,"href":"/apprenticeship-patterns-oreilly106/","title":"Apprenticeship patterns (O'Reilly)","section":"Inductive Bias","content":" Apprenticeship patterns (O\u0026rsquo;Reilly) # A few days ago I finished reading the book \u0026ldquo;Apprenticeship Patterns\u0026rdquo; - Guidance for the Aspiring Software Craftsman, by\nDave Hoover, Adewale Oshineye. The book is addressed to readers who have the goal of becoming great software devleopers.\nOne naive question one could ask is why there is a need for such a book at all? Students are trained in computer science at university, then enter some IT departement and simply learn from their peers. So how is software development any different than other professions? Turns out there are a few problems with that approach: At university students usually don\u0026rsquo;t get the slightest idea of what professional software development looks like. After four years of study they still have a long way to go before writing great software. When entering your average IT shop these juniors usually are put on some sort of customer project with tight deadlines. However learning implies making mistakes, it implies having time to try different routes to find the best one. Lucky are those very few who join a team that has a way for integrating and training junior developers. Last but not least at least in Germany tech carrier paths are still rare: As soon as developers excel they are offered a promotion - which usually leads straight into management before they even had a chance to become masters in their profession.\nSo what can people do who love writing software and want to become masters in their profession? The book provides various patterns, grouped by task: Emptying the cup deals with setting up an attitude that enables learning: To be able to learn new skills the trainee first has to face his ignorance and realise that what he knows already is just a tiny little fraction of what differenciates the master from the junior.\nIn the second chapter \u0026ldquo;Walking the long road\u0026rdquo; the book deals with the problem of deciding whether to stick with software development or to go into management. Both paths provide their own hurdles and rewards - in the end the developer himself has to decide which one to go. Deciding for a technical carrier however might involve identifying new kinds of rewards: Instead of being promoted to senior super duper manager, this may involve benefits like getting a 20% project, setting up a company internal user group, getting support for presenting ones projects at conferences. The chapter also deals with motivational side of software development: Let\u0026rsquo;s face it, professional development usually is way different from what we\u0026rsquo;d do if we had unlimited time. It may involve deadlines that cannot be met, it may invovle customers that are hard to communicate with. One might even have to deal with unmovtivated colleagues who have lower quality standards and no intention to learn more than what is needed to accomplish the task at hand. So there is the problem of staying motivated even if times get rough. Getting in touch with other developers - external and internal - here can be a great help: Attending user groups (or organising one), being part of an open source project, meeting regularly with other developers in one\u0026rsquo;s general geografical area all may help to remember the fun things about developing software.\nThe third group of patterns has been put under the headline \u0026ldquo;Accurate self-assessment\u0026rdquo; - as people get better and better it get ever harder to remember that there are techniques out there one does not yet know. Being the best in a team means that there is not more room to learn in that environment. It\u0026rsquo;s time to find another group to get in touch with others again: To be the worst in a team means there is a lot of room for learning, finding mentors helps with getting more information on which areas to explore next. Especially helpful is working on a common project with others - doing pair programming can help even with picking up just minor optimisations in their work environment.\nThe fourth chapter \u0026ldquo;Perpetual learning\u0026rdquo; deals with finding opportunities to learn new technologies - either in a toy project that in contrast to professional work is allowed to break and can be used to try and test new techniques and learn new languages. Other sources for learning are the source code itself, tech publications on magazines, books (both new and classic), blogs and mailing lists. Reflecting on what you learned helps remember it later - on option to reflect may involve writing up little summaries of what you read and keeping them in a place where you can easily retrieve them (for me this blog has turned into such a resource - yeah, I guess writing this book summary is part of the exercise, even was a proposal in the book itself). Last but not least one of the best resources for reflection and continued learning is to share knowledge - though you may feel there are others out there way better then you are, you are the one who just went though all the initial loops that no master remembers anymore. You can explain concepts in easy to understand words. Sharing and teaching means quickly finding gaps in your own knowledge and fixing them as you go forward. Last but not least it is important to create feedback loops: It does not help to learn after three years of coding that what you did does not match a customers expectations. As an apprentice you need faster feedback: On a technical level this may involve automated tests, code analysis and continuous integration. On a personal level it involves finding people to review your code. It means discussing your ideas with peers.\nThe last chapter on \u0026ldquo;Constructing your curriculum\u0026rdquo; finally dealt with the task of finding a way to remain up to date, e.g. by following re-known developers\u0026rsquo; blogs. But also studying the classic literature - there are various books in computer science and software development that have been written back in the 60s and 70s but are still highly relevant.\nThe book does not give you a recipe to turn from junior to master in the shortest possible time. However it successfully identifies situations many a software developer has encountered in his professional life that made him quesion his current path. It provides ideas on what to do to improve one\u0026rsquo;s skills even if the current IT industry may not be best equipped with tools for training people.\nMy conclusion from the book was that most important is getting in touch with other developers, exchanging ideas and working on common projects. Open source get several mentions in the book, but also for me has turned out to be a great source for getting feedback, help and input from the best developers I\u0026rsquo;ve met so far.\nIn addition meeting people who are working on similar projects face-to-face provides a lot of important feedback as well as new ideas to try out. Talking with someone over a cup of coffee for two hours sometimes can be more productive than discussing for days over e-mail. Hacking on a common project, maybe even in the same location, usually is the most productive way not only to solve problems but also to pick up new skills.\n"},{"id":265,"href":"/apache-dinner-wrap-up38/","title":"Apache dinner wrap-up","section":"Inductive Bias","content":" Apache dinner wrap-up # Today three Lucene committers, two Mahout committers (one of them also being committer of Lucene: Hi Karl, was great having you here - see you at the next Hadoop Get Together, or maybe some day for lunch.), several users of Lucene and Hadoop together with their family (including a very cute, unbelievably quiet three weeks old baby) met at Jamerica - a restaurant offering American as well as Jamaican food in Schöneberg.\nDaniel promised to organise the next dinner - looks like in October we meet somewhere close to his place in Potsdam. If you are one of the attendees please do feel invited to organise one of these evenings. It\u0026rsquo;s really simple: Setup a doodle with some proposed dates, send a mail to our Apache Dinner Berlin mailing list with the link included. After that we simply vote on the dates, the date with most votes wins. It\u0026rsquo;s then up to you to book a table in your favourite restaurant, send the address and time to the mailing list and that\u0026rsquo;s about it. Don\u0026rsquo;t be shy standing up\nthis meeting really is intended to be community driven, getting Apache people, friends, relatives at one table. Please also invite any friends that you know are in town - there is not problem adjusting the schedule to visitors.\nFor those of you who are currently not in Berlin: There will be a Apache Dinner Paris very soon. Would be great if you could let people know if you want to attend - makes booking a table way easier. "},{"id":266,"href":"/apache-dinner-this-evening37/","title":"Apache Dinner this evening","section":"Inductive Bias","content":" Apache Dinner this evening # This evening the September Apache Dinner takes place in Jamerica Schöneberg. I have booked a table for ten to fifteen people - we\u0026rsquo;ll see whether that is sufficient this time :)\nLooking forward to see you there at 7p.m.\n"},{"id":267,"href":"/apache-hadoop-get-together-berlin-october-201058/","title":"Apache Hadoop Get Together Berlin - October 2010","section":"Inductive Bias","content":" Apache Hadoop Get Together Berlin - October 2010 # This is to announce the next Apache Hadoop Get Together sponsored by JTeam that will take place in newthinking store in Berlin.\nWhen: October 7th, 5p.m.\nWhere: Newthinking store Berlin, Tucholksystr. 48\nAs always there will be slots of 30min each for talks on your Hadoop topic. After each talk there will be a lot time to discuss. You can order drinks directly at the bar in the newthinking store. If you like, you can order pizza. We will go to Cafe Aufsturz after the event for some beer and something to eat.\nTalks scheduled so far:\nMax Heimel: \u0026ldquo;Hidden Markov Models for Apache Mahout\u0026rdquo;\nAbstract: In this talk I will present and discuss an implementation of a powerful statistical tool called Hidden Markov Models for the Apache Mahout project. Hidden Markov models allow to mathematically deduce the structure of an underlying - and unobservable - process based on the structure of the produced data. Hidden Markov Models are thus frequently applied in pattern recognition to deduce structures that are not directly observable. Examples for applications of Hidden Markov Models include the recognition of syllables in speech recordings, handwritten letter recognition and part-of-speech tagging.\nSebastian Schelter: Distributed Itembased Collaborative Filtering with Apache Mahout\u0026quot;\nAbstract: Recommendation Mining helps users find items they like. A very popular way to implement this is by using Collaborative Filtering. This talk will give an introduction to an approach called Itembased Collaborative Filtering and explain Mahout\u0026rsquo;s Map/Reduce based implementation of it.\n\u0026lt; br /\u0026gt;View Larger Map\nPlease do indicate on Upcoming if you are coming so we can more safely plan capacities.\nJTeam is looking for Java developers and search enthusiasts. Check out their jobs page for more info!\nAs always a big Thank You goes to newthinking store for providing the venue for free for our event.\nLooking forward to seeing you in Berlin as well,\nIsabel\n"},{"id":268,"href":"/part-3-a-polite-way-to-say-no-and-why-there-are-times-when-it-doesnot-work323/","title":"Part 3: A polite way to say no - and why there are times when it does not work. ","section":"Inductive Bias","content":" Part 3: A polite way to say no - and why there are times when it does not work. # After having shared my thoughts on how to improve focus and how to track tasks eating up time this post will explain how to keep time invested at a more or less constant level. The goal of this exercise is to keep obligations at a reasonable level - be it at work or during ones spare time.\nIn recent time I have collected a small set of techniques to reduce what gets to my desk - I don\u0026rsquo;t claim this list to be exhaustive. However some of it did help me organise conference and still have a life besides that. Sharing and delegating tasks\nSharing and delegating are actually two different ways of integrating other people: Sharing for me means working together on a topic. That could be as easy as setting up a CMS or it could be more involved as in publishing articles on Lucene in some magazine. The advantage is that both of you can contribute to the task, possible even learn from each other: When I was doing the article series on Lucene together with Uwe it also was a great learning experience for me to have someone take the time to explain to me - well, not only to me - what flexible indexing, local search and numeric range queries are really all about, as in technically implemented. So it was not only an enormous time-saver for me, as the alternative would have been me reading through documentation, code and mailing lists to get up to date. But it also gave me the unique opportunity to learn from the very developers of these features about how they work and how they are meant to be used.\nThe disadvantage of sharing is that part of the work still remains on your desk. That\u0026rsquo;s where delegation helps: Take the task, find someone who is capable and willing to solve it and give it to them. There are two problems here: First you have to trust people to actually work on the task. Second you probably cannot avoid checking back from time to time to see if there is progress, if there are any impediments etc. So it means less work than with sharing. But there is more risk in not getting your results and more work to be done for co-ordination. However it is a very powerful technique if applied correctly to scale what can be achieved: Telling people what you need help with and letting them take over some of that work does scale way better than micro-managing people or even trying to be part of every piece of a project. It means giving up some of your control, in return you can turn to other potentially more involved tasks. Note to self: Need to build up more trust in that area.\nBoth concepts however are not actually about saying no but about being able to say yes even if you already have just very few time left.\nPrioritisation\nPrioritising tasks can be done on a scale from zero to any arbitrarily large number. Obviously it helps with deciding whom to say no to: It\u0026rsquo;s going to be those projects rated very low. That is those you could easily do without That\u0026rsquo;s the simplest case as it is easiest to explain. The strategy I usually use is to be honest with people: If there are conflicting conferences, it\u0026rsquo;s easy to reject invitations. If some publication does not pay for you, it\u0026rsquo;s easiest to be open and honest with people and tell them. Usually they will understand.\nA second reason for a rating of zero is that the task is one of those \u0026ldquo;Does not belong on my desk\u0026rdquo; tasks. My advice for those would be to get rid of them as quickly as possible: They draw away your energy without giving back any value. This issue plays nicely with the \u0026ldquo;patches welcome\u0026rdquo; theme from open source: People working on open source projects are most successful if they are driven by their own needs. So if you want something implemented, either implement and submit it yourself - or find someone you can pay to do so. People will not work for you. You can jump up and down, complain on the mailing lists - but if the feature you would like to see is something that no-one else in the existing community needs, it won\u0026rsquo;t get done until someone needs it.\nIntroduce barriers\nA nice way of rejecting favours that works at least sometimes is to raise the barrier. The example here would be getting an invitation to give an introductory talk for a closed audience. So what I tried was to raise the bar by asking for funding for travel and accommodation.\nKeep in mind though that there is the risk that the one inviting you actually accepts your conditions - no matter how high you think you have set them. Especially the example given above has the problem of being too low a bar in most cases. So be prepared to have to keep your promise. As a result the conditions you set really should lead to the task turning into something that is fun to do. Cut off early\nImaging you have committed to some task. Later on you realise you won\u0026rsquo;t actually make it: You have no time, priorities have changed, the task is too involved or any other reason you could potentially imaging.\nThe important way to reduce the load on your desk is to communicate this issue as early as possible. It\u0026rsquo;s clear that people will be more disappointed the later they learn that something they probably depend on won\u0026rsquo;t arrive in time or will never happen: They\u0026rsquo;ll never be extremely happy, however the sooner they learn the more time they have on their part to react. And actually, most people don\u0026rsquo;t react that disappointed at all, simply because they may have counted some risk into the equation when giving you the task - which is not to say you should lower the reliability of your commitments, simply because no-one is expecting you to meet your goals anyway. However usually the amount of trouble expected is way higher than what actually happens. Second note to self: Don\u0026rsquo;t forget about this option.\nPatches welcome\nAt least in open source: If it\u0026rsquo;s nothing that helps make your world better - there are other people out there to help out. Patches being welcome may seem obvious. However in some areas it really is not: If someone asks the project member to be present at some conference, he may himself not consider himself capable of representing the project or even just making an impact by talking to people about it. That is the point where to encourage people that any input is welcome - not only code, but also documentation, communication and marketing work.\nOf course as with any Pattern there are boundaries when not to apply it or when applying it would mean too much effort or loss. If that is the case and you have committed and cannot step back, than you should think about what could be a great reward if you went through the tasks: What would it take to make you happily comply and still gain energy through what you are doing? Basically it isn\u0026rsquo;t about doing what you like but about loving what you do (L. Tolstoi).\nThere is also valuable advice on managing ones energy from the Apache Software Foundation that is specially targeted at new committers. If you have not done so yet take the time to read it.\n"},{"id":269,"href":"/part-2-tracking-tasks-or-where-the-hack-did-my-time-go-to-last-week322/","title":"Part 2: Tracking tasks, or - Where the hack did my time go to last week?","section":"Inductive Bias","content":" Part 2: Tracking tasks, or - Where the hack did my time go to last week? # After summarising some strategies for not loosing track of tasks, meetings and conferences in the last post, this one is going to focus on the retrospect on achievements. If at some point in time you have asked yourself \u0026ldquo;Where the hack did time go to?\u0026rdquo; - maybe after two busy weeks of work this article might have a few techniques for you.\nUsually when that happens to me it\u0026rsquo;s either a sign that I\u0026rsquo;ve been on vacation (where that is totally fine) or that too many different, sometimes small but numerous tasks have sneaked into my schedule.\nTogether with Thilo I have found a few techniques helpful in dealing with these kind of problems. The goals in applying them (at least for me) have been:\nConfigure the planned work load to a manageable amount.\nMake transparent and trackable (to oneself and others) which and how many tasks have been finished.\nTrack over time any changes in number of tasks accomplished per time slot.\nAfter hearing about Scrum and its way of planning tasks I thought about using it not only for software development but for task planning in general. Scrum comprises some techniques that help achieving the goals described above:\nIn Scrum, development is split into sprints: Iterations of focussed software development that are confined to a fixed length. Each sprint is filled up with tasks. The number of tasks put into one sprint is defined by the so-called velocity of the team.\nTasks are ordered by priority by the product owner. Priority here is influenced by factors like risk (riskier tasks should be attacked earlier than safe ones), ROI (those tasks that promise to increase ROI most should of course be done and launched first) and a few more. After priorisation, tasks are estimated in order - that way those tasks most important to the product owner are guaranteed to have an estimated complexity defined even if there was not enough time to estimate all items.\nComplexity here does not mean \u0026ldquo;amount of time to implement a feature\u0026rdquo; - it\u0026rsquo;s more like how much time do we need, how much communication overhead is involved, how complex is the feature. A workable way to come up with reasonably sensible numbers is to chose one base item, assing complexity of one to it and estimate all coming items in terms of \u0026ldquo;is as complex as the base item\u0026rdquo;, \u0026ldquo;has double the complexity\u0026rdquo; - and so on - according to the fibonacci series. Fibonacci is well suited for that task as do not increase linearly - similarly humans are better at estimating small things (be it distances or complexities). As soon as items get too big, estimation also tends to be way off the real number.\nTo come up with a reasonable estimate of what can be done in any week, I usually just look back to past weeks and use that as an estimate. That technique is close enough to the real number to be a working approach.\nTo track what got done during the past week, we use a whiteboard as Scrum Board. Putting tasks into the known categories of todo, checked out and done. That way when resetting the board after one week and adding tasks for the following week it is pretty obvious which actions ate up most of the time. The amount of work that goes onto the board is restricted to not be larger than what got accomplished during the past week.\nSo what goes onto the whiteboard? Basically anything that we cannot track as working hours: The Hadoop Get Together can be found just next to doing the laundry. Writing and sending out the long deferred e-mail is put right next to going out for dinner with potential sponsors for free software courses at university.\nNow that weekly time tracking is set-up - is there a way to also come up with a nice longer term measure? Turns out, there are actually three:\nFirst and most obviously the whiteboard itself provides an easy measure: By tracking weekly velocity and plotting that against time it is easy to identify weeks with more or less freetime. As a second source of information a quick look into ones calendar quickly shows how many meetings and conferences one attended over the course of a year. Last but not least it helps to track talks given on a separate webpage.\nIt helps to look back from time to time: To evaluate the benefit of the respective activities, to not loose track of the tasks accomplished, to prioritise and maybe re-order stuff on the ToDo list. Would be great if you\u0026rsquo;d share some of your techniques of tracking and tracing time and tasks - either in the comments or as a separate blog post.\n"},{"id":270,"href":"/berlin-scrumtisch-open-discussion123/","title":"Berlin Scrumtisch - open discussion","section":"Inductive Bias","content":" Berlin Scrumtisch - open discussion # This evening the Berlin Scrumtisch took place in Friedrichshain. More than thirty participants followed Marion\u0026rsquo;s invitation for discussions on Scrum, wine and pizza at Vecchia Trattoria.\nAs there were several new participants, Felix started out with a very brief summary of the very core concepts of Scrum itself: Most important to know is the basic assumption of Scrum, that is planning ahead of time in a very detailed way is impossible. Defining goals and letting those who do the acutal work take the decisions on how to reach that goal is way easier and more promising. The whole process relies on fast feedback loops enabling developers and business people to run experiments on how to improve their work in a controlled environment.\nScrum comes with three roles: The development team responsible for delivering quality software, the product owner responsible for defining development goals that maximise return on investment and the scrum master as the moderator and facilitator who takes care that the roles and rituals are not broken.\nScrum comes with three plus one rituals: The daily standup (about 15min) used by the development team to get everyone up to date on a daily basis on everyone\u0026rsquo;s status, the Scrum Review and the Scrum Planning. In addition very important each sprint includes a retrospective that serves the purpose of improving the scrum team\u0026rsquo;s processes.\nScrum comes with three artifacts: The sorted product backlog of all user stories, the sprint backlog and the burndown chart showing the team\u0026rsquo;s progress.\nHowever Scrum is just a framework - it tells you more on the goals, but less on exactly how to reach them. It should serve as a basis to adapt one\u0026rsquo;s processes to the project\u0026rsquo;s needs.\nIn the usual meetup planning phase we collected potential topics for discussions and ranked them by voting on them in the end. The topics proposed were:\nApplications of Scrum for non-software-development projects.\nHow to convince teams of Scrum?\nAwareness of the definition of done.\nHow to integrate testers in a team, extended by discussing the values of cross-functional teams.\nHow to be a tech PO.\nAdding agile/XP to Scrum.\nHow to keep the team on focus.\nDecision making in self organising teams.\nBonus HR in Scrum.\nThe topics rated highest were on raising the awareness for the definition of done and on decision making in self organising teams.\nDefinition of done\nTo introduce the topic, Felix repeated the goal of Scrum teams to deliver potentially quality - ahem - shipable software. ;) The problem of the guest and his co-workers was described as follows: Teams have definitions that are inconsistant not only across teams but also within teams.\nUltimately the goal of the definition of done is to enable teams to produce shipable software. One option to make the team aware of the need for better quality software might be to make them feel the pain their releases cause. It does not help to dictate a company-wide definition of done: It\u0026rsquo;s up to the team to define it. However to learn more on what shipable means the team must be allowed to make mistakes. They will fail - but learn from that failure as soon as they feel and see what gets influenced by their mistakes. As a resulst, they will refine their definition of done.\nAs the person to make happy in Scrum iterations is the PO, this could mean that the PO simply does not accept features, after all he is the one to define what shipable means. One factor that is a pre-condition for teams to be able to learn is to keep them stable. Learning needs time - teams need to be allowed to evolve. If yesterday\u0026rsquo;s team does not feel the pain their mistakes caused just because the team does not exist anymore or has been reconfigured - how would people be able to learn at all?\nDecision making in self organising teams\nThe person proposing this question has the problem that in his teams some developers turn into leaders dictating the way software gets implemented. Other team members rarely join into that discussion and close to never take decisions. The result are endless discussions w/o real results.\nThe first idea that came up was for the Scrum Master to act as moderator. Marion came up with the proposal to use well known mediation techniques. She promised to share the links - would be great to have them published on the Scrumtisch Berlin blog as well. Thilo mentioned there are courses on mediation and moderation that can help him play that role. As for long discussions: Felix mentioned a few typical patterns (or anti-patterns) that tend to lead to developers discussing endlessly:\nFear: fear of punishment for taking the wrong decision usually leads developers to avoid decisions altogether. Fix for that would be establishing and open culture that allows for failure and that enables people to learn from failure.\nStriving for the 100% solution: Developers are not used to incremental thinking and try to solve all problems at once. Fix would be to teach them they get time for refactoring and are thus not punished for adhering to the YAGNI principle.\nPersonal conflicts in teams can lead to the described situation as well and can only be fixed by double-checking the team configuration, potentially changing it.\nThere is a very good book by Cohen on \u0026ldquo;Succeeding with agile\u0026rdquo; that has a whole chapter on what makes a good Scrum Master. Checking these properties against your chosen Scrum master might help as well.\nWhen discussing this topic we soon discovered one problem with the team configuration as-is: Scrum masters used to be system architects or senior software developers - that is, highly respected, influencial people. Maybe simply re-configuring teams might help already. Thanks to Marion for organising the evening - and thanks to all attendees for your questions and input on discussion topics. Looking forward to the next edition of the Scrumtisch.\nDisclaimer: I usually just take notes on an old-fashioned paper-notebook, typing stuff into the blog after the meeting is over. Only reason I do it the same evening is the goal of keeping the list of draft postings as short as possible.\n"},{"id":271,"href":"/apache-dinner-dus33/","title":"Apache Dinner DUS","section":"Inductive Bias","content":" Apache Dinner DUS # the evening after FrOSCon - that is on August 22nd 2010 at 7:30p.m. CEST - a combined \u0026ldquo;FSFE Fellowship meetup/ Apache dinner*\u0026rdquo; takes place in Tigges in Düsseldorf (Brunnenstraße 1, at Bilker S-Bahnhof). Given it doesn\u0026rsquo;t rain, we\u0026rsquo;ll be sitting outside.\nWould be great to meet you there for tasty food, interesting discussions on Apache in general, as well as projects like Lucene, Hadoop or Tomcat in particular. Anyone interested in either the FSFE or Apache is welcome to join us.\nOne personal request: Somehow, Rainer (Kersten, FSFE) talked me into preparing a talk on what the ASF is all about - would be really great to have more people around share their experience.\nSee you in Düsseldorf\n"},{"id":272,"href":"/scrumtisch-august-berlin356/","title":"Scrumtisch August Berlin","section":"Inductive Bias","content":" Scrumtisch August Berlin # Just seen it - the next Scrumtisch Berlin has been scheduled for 25th August 2010 at 18:30 Uhr. So far, no official talk has been scheduled, so please expect two topics on Scrum and its application to be selected for discussion according to Marion\u0026rsquo;s agile topic selection algorithm.\nPlease talk to Marion Eickmann if you would like to attend the next meetup.\n"},{"id":273,"href":"/some-statistics379/","title":"Some statistics","section":"Inductive Bias","content":" Some statistics # Various research projects focus on learning more on how open source communities work:\nWhat makes people commit themselves to such projects?\nHow much involvement from various companies is there?\nDo people contribute during working hours or in their spare time?\nWho are the most active contributors in terms of individuals and in terms of companies?\nWhen asked to fill out surveys, especially in cases where that happens for the n-th time with n being larger than say 5, software developers usually are not very likely to fill out these questionairs. However knowing some of the processes of open source software development it soon becomes obvious there are way more extensive sources for information - albeit not trivial to evaluate and prone to at least some error.\nFree software tends to be developed \u0026ldquo;in the open\u0026rdquo;: Project members with various backgrounds get together to collaborate on a common subject, exchanging ideas, designs and thoughts digitally. Nearly every project with more then two members at least has mailing list archives and some sort of commit log to some version control system. Usually people also have bugtrackers that one can use as a source for information.\nIf we take the ASF as an example, there is a nice application to create various statistics from svn logs:\nThe caveats of this analysis are pretty obvious: Commit times are set according to the local of the server, however that may be far off compared to the actual timezone the developer lives in. Even when knowing each developer\u0026rsquo;s timezone there is still some in-accuracy in the estimations as people might cross timezone bounderies when going off for vacation. Still the data available from that source should already provide some input as to when people are contributing, how many projects they work on, how much effort in general goes into each project etc.\nTurning the analysis the other way around and looking at mailing list contributions, one might ask whether a company indeed is involved in free software development. One trivial, naive first shot could be to simply look for mailinglist postings that originate from some corporate mail address. Again the raw numbers displayed below have to be normalised. This time company size and fraction of developers vs. non-developers in a company has to be taken into consideration when comparing graphs and numbers to each other.\nYet another caveat are mailinglists that are not archived in the mail archiving service that one may have choosen as the basis for comparison. In addition people may contribute from their employer\u0026rsquo;s machines but not use the corporate mail address (me personally I am one of these outliers, using the apache.org address for anything at some ASF project).\n101tec\nJTeam\nTarent\nKippdata\nLucid Imagination\nDay\nHP\nIBM\nYahoo!\nNokia\nOracle\nSun\nEasily visible even from that trivial 5min analysis however is general trending of involvement in free software projects. In addition those projects are displayed prominently projects that employees are working with and contributing to most actively - it comes as no surprise that for Yahoo! that is Hadoop. In addition if graphs go back in time far enough, one might even see the timeframe of when a company changed its open source strategy (or was founded (see the graph of Lucid), or got acquired (see Sun\u0026rsquo;s graph), or acquired a company with a different stategy (see Oracle\u0026rsquo;s graph) ;) ).\nSort of general advise might be to first use the data that is already out there as a starting point - in contrast to other closed communities free software developers tend to generate a lot of it. And usually it is freely available online. However when doing so, know your data well and be cautious to draw premature conclusions: The effect you are seeing may well be caused by some external factor.\n"},{"id":274,"href":"/nosql-summer-berlin-this-evening294/","title":"NoSQL summer Berlin - this evening","section":"Inductive Bias","content":" NoSQL summer Berlin - this evening # This evening at Volkspark Friedrichshain, Café Schoenbrunn the next NoSQL summer Berlin (organised by Tim Lossen) is meeting to discuss the paper on Amazon\u0026rsquo;s Dynamo \u0026ldquo;Dynamo: Amazon\u0026rsquo;s Highly Available Key-value Store\u0026rdquo;. The group is planning to meet at 19:30 for some beer and discussions on the publication.\n"},{"id":275,"href":"/apache-dinner-august-berlin-recap29/","title":"Apache Dinner August Berlin recap","section":"Inductive Bias","content":" Apache Dinner August Berlin recap # This evening yet another Apache Dinner took place in Berlin (this time Schöneberg), location booked by Simon Willnauer. As it was announced less then a week ago (see post below) we were expecting no more then some 7 people \u0026hellip; we ended up being a group of 15 attendees: There was Michi Busch from Twitter together with Tanja, Uwe Schindler from Bremen joined us. With Matt and Josh some of our local Hadoop users from Nokia joined our group. We had Sebastian Schelter from Mahout. In addition there were the usual suspects, that is Jan Lehnardt, Simon Willnauer and Torsten Curdt.\nIndian food at Yogi Haus was great and very tasty - though we should introduce a sharing algorithm for the various dishes next time around. Speaking of next time: If you would like to be part of the dinner, subscribe to our Apache Dinner mailing list. Best way to make the location suit your needs is to simply send out the next proposal yourself.\nAs usual the Lucene guys are the last to leave: Currently they are on their way to X-Berg for further drinks, some food and lots of fun. Looking forward to the pictures you promised, Simon ;)\nUpdate: Images added. Thanks for forwarding them.\n"},{"id":276,"href":"/apache-dinner-berlin-august-201131/","title":"Apache Dinner Berlin - August 2010","section":"Inductive Bias","content":" Apache Dinner Berlin - August 2010 # Simon (Willnauer) just sent around the following e-mail. If you have some time left next Monday evening, come join us in Yogihaus: For tasty Indian food, geeky discussions and a generally beautiful evening.\nUnlike the other dinner mails this one is not a poll, it\u0026rsquo;s an announcement. Some Apache folks are in town next monday (9th of August) so we decided to have a Apache Dinner with a short term notice. If you plan to come please shoot me a quick heads-up and I count you in!\nWe will meet at http://www.restaurant-yogihaus-berlin.de/ on Monday 9th of August at 7:30 pm. I will reserve a table for about 14 people (the average size of the last two meetings while gstein and his gang wasn\u0026rsquo;t counted :D)\nLooking forward to meet you there on Monday!!\nSimon\nLooking forward to seeing you on Monday evening next week. Please do not forget to give Simon a quick heads-up if you are coming: Would be nice if our estimated number of guests would at least be close to the real number this time (instead of somewhere at 50% ;) ).\nIn the unlikely event that you can\u0026rsquo;t make it next Monday, please subscribe to our Apache Dinner Mailinglist to recieve further announcements. If you are not living in Berlin but are still interested in dropping in from time to time: Don\u0026rsquo;t worry we do take into account that schedules of people travelling here are tight and organise meetings accordingly.\nUpdate: Corrected year - must have mixed that up with another conference\u0026rsquo;s kick off meeting that takes place today\u0026hellip;\n"},{"id":277,"href":"/part-1-travelling-minds321/","title":"Part 1: Travelling minds","section":"Inductive Bias","content":" Part 1: Travelling minds # In the last post I promised to share some more information on techniques I came across and found useful under an increasing work load. Instead of taking a close look at my professional calendar I decided to use my private one as an example - first because spare time is even more precious then working hours, simply because there is so few of it and secondly because I am free to publicly scrutinize not only the methods for keeping it in good shape but also the entries in it.\nI am planning to split the article in four pieces as follows as keeping all information in one article would lead to a text longer then I could possibly expect to be read from beginning to end: Part 1: Traveling minds - how to stay focussed in an always-on community.\nPart 2: Tracking tasks, or: Where the hack did my time go to last week?\nPart 3: A polite way to say no - and why there are times when it doesn\u0026rsquo;t work.\nPart 4: Constant evaluation and improvement: Finding sources for feedback.\nPart 5: A final word on vacation.\nSeveral years ago, I had no problem with tasks like going out reading a book for hours, working on code for hours, answering mails only from time to time, thinking about one particular problem for days. As the number of projects and tasks grew, these tasks became increasingly hard to accomplish: Writing code, my mind would wander off to the mailing list; when reviewing patches my mind would start actually thinking about that one implementation that was still lingering on my hard disk.\nThere are a few techniques for getting back to that state of thinking about just one thing at a time. One article I found very insightful was an essay by Paul Graham. He gave a pretty good analysis of thoughts that can bind your attention and draw them away from what should actually be the thing you are thinking about. According to his analysis a pretty reliable way to discover ideas that steal your attention is to observe what thoughts your mind wanders to when you are taking a shower (I would add cycling to work here, basically anything that lets your mind free to dream and think): If it is not in line with what you would like to think about, it might be a good time to think about the need to change.\nThere are a few ways to force your mind to stay \u0026ldquo;on-topic\u0026rdquo;. Some very easy ones are explained in a recent blog post on attention span (Thanks to Thilo for the link): Organising your virtual desktops such that applications are sorted according to tasks (one for communication, one for coding project x, another one for working on project y) helps to switch off distraction that would otherwise hide in plain sight. Who wants to work on code if TweetDeck is blinking at you next to your editor? In contrast to the original author I would not go so far to switch off multiple monitors: Its great to have your editor, some terminals, documentation in the browser open all at the same time in one workspace. However I do try to keep everything that has do with communication separate from coding etc.\nTrain to work for longer and longer periods of time on one task and one task only: The world does not fall apart, if people have to wait for an answer to your mail for longer than 30min - at least they\u0026rsquo;ll get used to it. You do not need to take your phone to meetings: If anything is starting to melt down there will be people who know where you are and who will drag you out of the meeting room in no time. Anything else can well wait for another 60min.\nWhen working with tabbed browsing: Don\u0026rsquo;t open more tabs then you can easily scan. You won\u0026rsquo;t read those interesting blog post you found four weeks ago anyway. In modern browsers it is possible to detach tabs. That way you can follow the first hint of keeping even the web pages sorted on desktops according to activity: You do not need your time tracking application next to your editor. Having only documentation and testing application open there does help.\nKeep your environment friendly and supportive. Who has ever shared an office (or a lecture at university back when I was a student) with me knows that close to my desk the probability of finding sweets, cookies, drinks and snacks approaches one. Being hungry when trying to fix a bug does not help, believe me.\nOne additional trick that helps staying just focussed enough for debugging complex problems is to make use of systematic debugging by Andreas Zeller (also explained in Zen and the Art of Motorcycle Maintenance). The trick is to explicitly track you thoughts on paper: Write down your hypothesis of what causes the problem. Then identify an experiment to test the hypothesis - you should know how to use your debugger, when to use print statements, which unit tests to write and when to simply take a very close look at the code and potentially make it simpler for that. Only when your experiment confirms that you have found the cause of the problem you really have identified what you need to fix.\nThere are a few other techniques for getting things off of your head that are just there to distract you: If you ever have read the book \u0026ldquo;Getting things done\u0026rdquo; or seen the Inbox zero presentations you may already have an idea of what I am hinting at.\nBy now I have a calendar application that works like a charm: It reminds me of meetings ahead of time, it warns me in case of conflicts, it accepts notes, it has an amazing life span of one year and is always available (provided I do not forget it at home): - got mine here ;) That\u0026rsquo;s for organising meetings, going to conferences, getting articles done in time and not forgetting about family birthdays.\nFor week to week planning we tend to use Scrum including a scrum board. However that is not only for planning as anyone using Scrum may have expected already.\nFor my inbox the rule is to filter any mailing list into its own folder. Second rule is to keep the number of messages in my inbox to something that fits into a window with less than 15 lines: Anything I need for further reference (conference instructions, contacts, addresses that did not yet go into my little blue book, phone numbers not yet stored in my mobile phone) goes into its own folder. Anything that needs a reply is not allowed to stay in the inbox for longer than half a week. For larger projects mail gets sorted into their own project folders. Anything else simply goes to an archive: There are search indexes available, even Linux supports desktop search, search is even integrated in most mail clients. Oh and did I mention that I managed to search for one specific mail for an hour just recently, though it was filed into its own perfectly logical folder - simply because I had forgotten which folder it was?\nTo get rid of things I have to do \u0026ldquo;some time in the near future but not now\u0026rdquo; I keep a list in my notebook - just so my mind knows the note is there for me to review and it knows I don\u0026rsquo;t forget about it. So to some extend my notebook is my personal swap space. One thing I learnt at Google was to not use loose paper for these kinds of notes - a bound book is way better in that it keeps all notes in one place. In addition you do not get into danger of throwing notes away too early or mis-place them.\nThe only thing missing is a real product backlog that keeps track of larger things to do and projects to accomplish - something like \u0026ldquo;I really do need to find a weekend to drive these \u0026gt;250km north to the eastbaltic sea (Thanks to Astro for pointing out the typo to me - hey, that means there is at least one guy who actually did read that blog post from beginning to end - wow!) and relax\u0026rdquo; :)\n"},{"id":278,"href":"/series-getting-things-done367/","title":"Series: Getting things done","section":"Inductive Bias","content":" Series: Getting things done # Probably not too unusual for people working on free software mostly (though no longer exclusively) in their spare time, the number of items that appear in my private calendar have increased steadily in the past months and years:\nEvery three months I am organising the Apache Hadoop Get Together in Berlin.\nI have been asked (and accepted the offer) to publish articles on Hadoop and Lucene in magazines.\nThere are various conferences I attend - either as speaker or simply as participant: FOSDEM, Froscon, Apache Con NA, Devoxx, Chemnitzer Linuxtag - to name just a few.\nFor Berlin Buzzwords I did get quite a bit of time for organisation, still some issues leaked over to what others would call free time.\nI am mentoring one of Mahout\u0026rsquo;s GSoC students which is a lot of fun.\nAt least I try to spend as much time as possible on the Mahout mailing lists keeping up with what is developed and discussed there.\nThere are various techniques to cope with increased work load and still find enough time to relax. Some of them involve simply remembering what to do at the right time, some involve prioritization, others deal with measuring and planning what to do. In this tiny series I\u0026rsquo;ll explain the techniques I employ - or at least try to - in the hope of getting your feedback, and comments on how to improve the system. After all, the most important task is to constantly improve ones own processes.\n"},{"id":279,"href":"/apache-hadoop-in-debian-squeeze61/","title":"Apache Hadoop in Debian Squeeze","section":"Inductive Bias","content":" Apache Hadoop in Debian Squeeze # After using Mandrake for quite a while (still blaming my boyfriend Thilo for infecting not only my computer but also myself first with that system, then with the more general idea of Free Software\nbut that\u0026rsquo;s another story.) after finishing my master\u0026rsquo;s thesis I started using GNU Debian Linux (back then in the version code-named Woody). Since I always had a GNU Debian on my private box as my main operating system - even installed it on my MacBook following the steps in the Debian Wiki.\nAs I am also an Apache Mahout committer, closely related to the Apache Hadoop project, I always found it kind of sad that there were no Hadoop packages in the official Debian repositories. I tried multiple times to find some time to get into Debian packaging myself, I learned what \u0026ldquo;debian/rules\u0026rdquo; is all about and discovered some of the intricacies of packaging Java based software. However I have to admit that I never was able to find enough time to really finish that task.\nA few weeks before this year\u0026rsquo;s FOSDEM I learned on the Apache Hadoop as well as on the Debian Java lists that a guy called Thomas Koch was working on solving bug 535861 - ITP to package Hadoop. We met at FOSDEM where I tried to raise some attention in the audience for Thomas\u0026rsquo; plans (back then he was in need for help with a few last missing pieces). In addition I invited him for Berlin Buzzwords to get in touch with other Hadoop developers and users for further input.\nI am really happy that by now Hadoop has made it into the official Debian package repositories - as soon as Debian Squeeze\u0026lt;/a (currently at testing) gets released, installing Hadoop on your Debian box will be as easy as issuing apt-get install [Hadoop component you need]: Debian package search. If you want to speed up the process of Squeeze being released as stable version: Help fixing the remaining bugs in that distribution. There are various Debian Bug Squashing Parties being organised around the world. Next one in Berlin is on next Monday, the one for Munich is running this weekend. Just got the information that Fefe posted in his blog a link to the Mozilla bug bounty: The packages are based on the upstream Apache Hadoop distribution, being comparably new they are intended for development machines at the moment. If you are using Debian and want to work with Hadoop - this is a great opportunity to help making the packages more stable by simply using them and reporting your experiences back to the Debian community.\nIn addition Debian now also provides packages for Zookeeper as well as HBase - though the HBase version is not yet production ready as the HDFS-append patch is still missing.\nTo follow the general state and progress of these packages feel free to follow the packages pages for Hadoop, HBase, Zookeeper respectively.\nThomas currently plans to work more closely with upstream e.g. to tidy up the chaos in the start-up scripts and other minor glitches. So watch out for further improvements.\nIn addition I just saw another interesting ITP in the Debian bugtracker: Wishlist: katta. I am sure there are quite a few others as well.\n"},{"id":280,"href":"/apache-lunch-in-portugal63/","title":"Apache Lunch in Portugal","section":"Inductive Bias","content":" Apache Lunch in Portugal # Just read on the Apache community mailing list that inspired by our Apache Dinner Berlin people in Porto are organising an Apache Lunch event. As with the dinner here in Berlin, anyone who is interested in Apache is welcome to join - no need to be a committer or even ASF member ;) If you are living close to Porto, or always wanted to visit the city - after all it\u0026rsquo;s a very beautiful place, there is a beach close by, there are many tasty restaurants - don\u0026rsquo;t hesitate to get in touch with the organisers:\nMy xmpp is: fdmanana@gmail.com. Feel free to add me.\nPeople interested in coming, let us known your availability during the 2\nfirst weeks of August.\nSo, if you are interested in Apache head over to Filipe - I\u0026rsquo;d love to be there, however my summer vacation ended one week ago. Wish you guys a lot of fun.\n"},{"id":281,"href":"/teddy-in-portugal401/","title":"Teddy in Portugal","section":"Inductive Bias","content":" Teddy in Portugal # During the past two weeks my teddy was on vacation. As destination he chose to fly to Portugal. One day was reserved for a visit to Lisboa, the capital city of the country. He also took a few really nice pictures there:\nOn his return, he was no longer alone. Seems like he found a cute little portugese girl friend:\n\u0026lt;img src=\u0026quot;/teddyGirlFriend.png\u0026quot; width=\u0026ldquo;400\u0026rdquo; /\u0026gt;\nIn addition he brought the following image. However he promised that he was not in California, but explained that the bridge actually does exist in Lisboa, being constructed by the same company according to the same blue prints that already were used for Golden Gate bridge:\n"},{"id":282,"href":"/scrum-prepare-your-meetings339/","title":"Scrum - prepare your meetings","section":"Inductive Bias","content":" Scrum - prepare your meetings # \u0026ldquo;But Scrum involves so many meetings - we already have meetings like all day and don\u0026rsquo;t get to coding anything.\u0026rdquo; - \u0026ldquo;However we do need transparency and communication to build great software.\u0026rdquo; - \u0026ldquo;Actually scheduling and re-prioritising items during scrum planning takes so much time.\u0026rdquo; Does that sound familiar to you?\nWhat if you could fit Sprint Review and Planning I for a team of three people doing three week sprints into one hour? Impossible? Well, not quite so. As always there is a very easy trick: Be prepared.\nBefore the review re-visit all items you have accomplished. Get the application into a state that makes it easy to demonstrate all new features to your product owner. If you are a team working on internal projects with many internal clients - get one of them to test our the new features early on (as in way before review) and get their input onto the table.\nGet a list of all items up on a whiteboard. Then with the PO work you way through these items, either demonstrating them or referring to what the \u0026ldquo;real client\u0026rdquo; said about the feature. After that compute the velocity of your past sprint.\nThen go over to the list of still to be done items. (You did estimate them in separate estimation meetings already, didn\u0026rsquo;t you? You also got the prioritised by your PO beforehand, didn\u0026rsquo;t you?) According to the computed velocity simply check out what you can do in the upcoming sprint.\nAfter that team and PO are done. Guess what: Working with pen, whiteboard and paper did speed up our process considerably. After all, these are meetings of at least four people: Don\u0026rsquo;t want to waste working hours of four people by not using the fastest and most flexible planning method.\nSo - who\u0026rsquo;s responsible for getting all this information up? And - if using a digital planning tool, who is supposed to get it back into the tool? Trivial: It\u0026rsquo;s the scrum master\u0026rsquo;s job to provide all help and tools to the team to make it a high performing team. It\u0026rsquo;s way better to spend 2 extra hours preparing and \u0026ldquo;tidying up\u0026rdquo; the sprint planning than have four people spend 2 hours (total 6 vs. 8 hours) in a sub-optimal meeting.\nTogether with estimation meetings for me this means that each sprint one to two days go into organisational work. Still this is very low overhead compared to what the team is able to accomplish in that time.\n"},{"id":283,"href":"/using-scrum-for-software-development417/","title":"Using Scrum for software development","section":"Inductive Bias","content":" Using Scrum for software development # A few months ago I entered a new team of until then two software developers. Being so small and with a rather busy product owner, until then people had followed the rituals of Scrum only loosely. When starting to work on a new component the three of us decided to change a few thing several weeks ago:\nWe would setup infrastructure to be somewhat similar to what we knew from Apache projects: All issues to be accomplished were to be tracked in our issue tracker. We would have a dev list that mirrors whatever goes to JIRA, a commit list to mirror whatever goes into svn and a user list.\nWe would try to follow Scrum rituals more closely: Dailies were re-introduced, we setup estimation meetings and got a cooking timer to stick with 60min per meeting, review and planning was supposed to be prepared and done with paper and pens on a whiteboard. After each planning we would have a planning II as well as a retrospective to improve our processes.\nAfter the first weeks of following these ideas we did notice several improvements that are going to be described in upcoming blog posts.\nLets start with the first (trivial) change we made: Introducing a commit mailing list. With developers sitting all together in one room, communicating openly and regularly this may seem like a huge overkill. However there are a few changes that brought to how we develop:\nPeople would suddenly start to think about what goes into svn: Being very obviously publicly readable, commit messages became much more readable, explaining what\u0026rsquo;s going on. Changes would be checked-in only if they belong to one logical change set. In return others would review what was checked in sort of automatically\nspotting problems or questionable code and configuration very early. Changes that got done were made transparent to other team members very easily.\nOf course the information is available anyway. However to be honest - I never really closely check each change set that got committed after issuing an svn up. Getting the stuff pushed to my inbox hugely improved the situation while still keeping a faster development cycle than when working with a review-before-commit model. This is especially important for cases where only smaller adjustments are being made. Where larger refactoring steps were necessary we would still get the code reviewed by our colleagues before checking it in. "},{"id":284,"href":"/scalability335/","title":"Scalability","section":"Inductive Bias","content":" Scalability # For Berlin Buzzwords we concentrated pretty heavily on scalable systems and architectures: We had talks on Hadoop for scaling data analysis; HBase, Cassandra and Hypertable for scaling data storage; Lucene and Solr for scaling search.\nA recurring pattern was people telling success stories involving project that either involve large amounts of data or growing user numbers. Of course the whole topic of scalability is extremely interesting for ambitious developers: Who would not be happy to solve internet-scale problems, have petabytes of data at his fingertips or tell others that their \u0026ldquo;other computer is a data center\u0026rdquo;.\nThere are however two aspects of scalability that people tend to forget pretty easily: First of, if you are designing a new system from scratch that implements a so far unknown business case - your problem most likely is not scalability. It\u0026rsquo;s way more likely that you have to solve marketing tasks, just getting people to use your cool new application. Only after observing what users actually do and use you have the slightest chance of spotting the real bottlenecks and optimising with clear goals in mind (e.g. reduce database load for user interaction x by 40%).\nThe second issue people tend to forget about scalability is that the term is about scaling systems - some developers easily mix that up with high performance. The goal is not to be able to deal with high work load, but to build a system that can deal with increasing (or decreasing) work load. Ultimately this means that not only your technology must be scalable: Any architecture can only scale to a certain load. The organisation building the system must be willing to continuously monitor the application they built - and be willing to re-visit architectural decisions if the environment changes. Jan Lehnardt had a very interesting point in his talk on CouchDB: When talking about scalability, people usually look into the upper right corner of the application benchmark. However to be truely scalable one should also look into the lower left corner: Being scalable should not only mean to be able to scale systems up - but also to be able to scale them down. In the case of CouchDB this means that not only large installations at BBC are possible - but running the application on mobile devices should be possible without problems as well. It\u0026rsquo;s an interesting point in the current \u0026ldquo;high scalability\u0026rdquo; hype.\n"},{"id":285,"href":"/bye-bye-germany131/","title":"Bye, bye Germany","section":"Inductive Bias","content":" Bye, bye Germany # \u0026hellip; for the next two weeks: I\u0026rsquo;ll be on vacation with strict internet interdiction. Will be a tourist exploring beaches and maybe a few hiking tracks in the next few days, so don\u0026rsquo;t expect to read anything here apart from what was scheduled already ;)\n"},{"id":286,"href":"/teaching-free-software-development390/","title":"Teaching Free Software Development","section":"Inductive Bias","content":" Teaching Free Software Development # In Summer last year I was invited to give a presentation on Apache Mahout at TU Berlin. After the talk was over some of the research group members asked me to design and give a course on scalable machine learning with open source software during the winter semester.\nThe project attracted four to five students - not very many - but then again it is a course people can take voluntarily. During the first semester participants were asked to integrate Mahout to build a system that crawls web pages, assigns them to clusters and makes the content searchable with Lucene. The intention was to get students to publish any patches they have to make at Mahout. In addition the code behind the system was supposed to be published after the project was over.\nThis setup turned out to be sub-optimal: The participants never grew confident enough to publish not only their ideas and design on the mailinglist but also send in the access data to the SCM system that hosted the project source code.\nSome similar setup was run at HPI Potsdam by Christoph Böhm: He let students implement various information retrieval and machine learning algorithms on top of Apache Hadoop. After the course was over he tried to motivate students to publish their code at Apache Mahout. So far I have seen no submissions.\nBeing aware of these problems next time I setup the course for the summer semester at TU I chose a slightly different model: Having only four students who do not have enough free cycles to work on the project full time I set the goal to implement an HMM - including tests, example and documentation. Being roughly aligned with GSoC I asked students to publish their timeline in JIRA. As soon as coding started I urged them to publish even incremental progress and ask the community for feedback. Now we do have an open JIRA issue with a patch attached to it. People also got some code review feedback already. Having Berlin Buzzwords in town while the course was still running I used my chance to get students in touch with other Mahout developers. Looks like at least one of them is planning to stay with the project for a little longer. For me it would be a great success if at least one student could be turned into a longer term contributor to the project.\nSo far it looks like applying the general principle of releasing code early and often helps people do integrate into some project. My own lesson learned from those experiences however is to urge students early on to get in touch and release their code: It was not particularly easy to get them to send e-mails to public mailing lists. However if they had done this just once, feedback usually was very positive - and surprised by how friendly and helpful in the free software community generally are.\n"},{"id":287,"href":"/service-as-in-real-good-customer-service368/","title":"Service as in real good customer service","section":"Inductive Bias","content":" Service as in real good customer service # First thing to do after getting up: Go to the kitchen and switch on the coffee machine. However, one random Sunday morning that caused the fuse for exactly this kitchen to go off. After fixing that we turned on the coffee machine again - trying to finally get a first cup. All worked well until having a closer look at what the machine produced as coffee: It was cold!\nWe initially got the machine from Giuseppetti - a vendor in Berlin. Though it was to late to get it fixed on warranty we still took the thing over to his shop the following week. What happened than was amazing to see for us:\nThe mechanics unscrewed the machine, started examining it immediately. Knowing we had bought it there, the owner gave both of us a cup of coffee. Long before I had finished mine the fixed machine was brought back to us - a fuse in it had gone off as well.\nSo after less than half an hour we got our working coffee machine back w/o being charged for the repair. Next morning was way better than the previous Sunday morning.\n"},{"id":288,"href":"/linus-torwalds-on-the-linux-kernel-community264/","title":"Linus Torwalds on the Linux kernel community","section":"Inductive Bias","content":" Linus Torwalds on the Linux kernel community # A few days ago, Linus send a very interesting mail on why he considers C the programming language that is most suitable for the Linux kernel. Despite the language specific arguments, the text contains quite a few insights on how the Linux kernel community works and communicates that might be interesting to non-kernel-hackers as well:\nPeople working for free still doesn\u0026rsquo;t mean that it\u0026rsquo;s fine\nto make the work take more effort - people still work for\nother compensation, and not feeling excessively\nfrustrated about the tools (including language) and getting\nproductive work done is a big issue.\nWhen attending Open Source conferences or contributing to free software projects I have made the exact same observation multiple times: Developers may not get money out of contributing to a free software project (though often they may do) there are other rewards that keep them working on any particular project: Learning from excellent peers usually is one reason. Being able to work on a topic you like at any time that suits you may be another one. Developing free software is largely different than your usual professional work: People work voluntarily, putting as much effort into the code as is needed for them to be satisfied with the end-result. There may not be any deadlines fixed by contracts, still developers honour release cycles with the goal of providing a reliable product to their end users.\nIn the end it all boils down to being passionate about what you work on. To be involved in any open source project takes a huge amount of energy - but usually you get more in return than you are even able to invest. However if passion is a pre requisite to working on any free software this also means it is extremely hard to pay developers to work on any free software project: You just cannot buy passion or love for money.\nBut the thing is, \u0026ldquo;lines of code\u0026rdquo; isn\u0026rsquo;t even remotely close\nto being a measure of productivity, or even the gating\nissue. The gating issue in any large project is pretty much\nall about (a) getting the top people and (b) communication.\nCan only quote that - totally agree with the analysis. This applies to any software project - free or proprietary. So if you own a software development business: What is your strategy for getting the top people and facilitating communication in your company? What is your measure of productivity?\n"},{"id":289,"href":"/velocity-update418/","title":"Velocity update","section":"Inductive Bias","content":" Velocity update # After Berlin Buzzwords is over now, I thought it might be time to update a formerly published velocity graph. If you have been following this blog during the past few months you may know already, that we are using Scrum @ Home for several months already. I even published results of a Scrum Nokia test earlier this year.\nToday I created another one of these nice velocity charts. What is interesting about this graph are two things. First of all: There are two spikes in the velocity graph. As our scrum board is used only for things done during our spare time, these spikes roughly correspond to German vacation days (Christmas and Easter).\nMore interestingly the work load shortely before Berlin Buzzwords happend was not much higher than usual. Weekend was totally free of any work - except for meeting with friends, going to town and attending the Barcamp. A sure sign of fantastic preparation and organisation with that few tasks left to do. Julia, thanks for helping with scheduling tasks such that no panicky late-night action was needed before, during or after the conference. "},{"id":290,"href":"/my-highly-subjective-berlin-buzzwords-recap290/","title":"My highly subjective Berlin Buzzwords recap","section":"Inductive Bias","content":" My highly subjective Berlin Buzzwords recap # Last November I innocently asked Grant what it would take to make him to give a talk in Berlin. The only requirement he told me was that I\u0026rsquo;d have to pay for his flight. About eight months later we had Berlin Buzzwords - a conference all around the topics scalability, data storage and search. With Simon Willnauer, Uwe Schindler, Michael Busch, Robert Muir, Grant Ingersoll, Andrzej Bialecki and many others we had quite a few Lucene people in town.\nFrom the NoSQL community, Peter Neubauer, Rusty Klophaus, Jan Lehnardt, Mathias Meyer, Eric Evans and many others made sure people got their fair share of NoSQL knowledge. With Aaron Kimball, Jay Booth, Doug Judd and Steve Loughran we had several Hadoop and related people at the conference. The conference also featured two talks on Apache Mahout: An overview from Frank Scholten as well as a more in-depth talk by Sean Owen. It\u0026rsquo;s great to see the project grow - not only in terms of development community but also in terms of requests from professional Mahout users.\nIn addition we had a keynote by Pieter Hintjens that concentrated on messaging in general and 0MQ in particular - a scalability topic otherwise highly underrepresented at Berlin Buzzwords.\nWe got well over 300 attendees that filled Berlin Kosmos - a former cinema. Attendees were a good mixture of Apache and non-Apache people, developers and users. People used the breaks and bar tours after the event to get in touch, exchange ideas. It\u0026rsquo;s always good to see developers discuss design issues and architectural challenges.\nMonday evening was reserved for local people taking out the speakers and interested attendees for Bar Tours to Friedrichshain. Those from Berlin took Berlin Buzzwords people to their favourite restaurants and bars - or to what they considered to be \u0026ldquo;typical Berlin\u0026rdquo;. Some spent evenings later that week drinking beer or Berliner Weisse.\nThe tour for keynote speakers Grant Ingersoll, Pieter Hintjens and friends was organised by Julia and myself. We went over to Kreuzberg - some went to famous Burgermeister for Burgers, the other half went to a nearby Indian restaurant. After that we spent the evening in Club der Visionäre - a club next to the water. Me personally I left at about midnight - several people of the Lucene community moved to the well known Fette Ecke later on.\nWhen asking the audience about repeating the conference next year, all hands went up immediately. Beside lots of praise for the organisation, from the feedback form we put up we got some good ideas on how to improve the conference next year. I\u0026rsquo;d love to have you guys back here in 2011 - and I\u0026rsquo;d love to get even more attendees in. Was great fun having you here. Thanks for 5 great days:\nFive instead of two days, because:\nKeynote speakers got a special treatment - that is a personal city guide for the weekend before Buzzwords. We had the official conference start on Sunday with a Barcamp. We had another Apache dinner on Wednesday with those Apache people that live in Berlin. In addition the Aaron and Sarah joined us as they were still in town for the Apache Hadoop trainings. Also Greg Stein had pizza and beer with us - he was in town for the svn conference at the end of the week.\nThanks to all who helped turn this conference into a success: Julia Gemählich for conference management, Ulf and Wetter for WiFi setup, Nils for travel management, Simon and Jan for support ranking talks and reaching out to your communities, all speakers for fantastic talks, those taking pictures of the conference and sharing them on Flickr for showing those who stayed at home how great the conference was, peoplezapping for the videos that will soon be available online, all sponsors for supporting the conference, all attendees for their participation. I\u0026rsquo;d love to have all of you (and many more) back in Berlin next year. An informal call for presentations has been set up already - submit now and be the one to set the trend instead of just following the Buzzwords!\nFor those who do not want to wait for another year: We will have another Apache Hadoop Get Together in September 2010 - watch this space for more information. If you\u0026rsquo;d like to give a talk their and present your Hadoop/ Solr/ Lucene etc. system - please get in touch with me.\n"},{"id":291,"href":"/apache-dinner-june-201035/","title":"Apache Dinner June 2010","section":"Inductive Bias","content":" Apache Dinner June 2010 # After Berlin Buzzwords was over yesterday - and as there is an svn conference in the city that starts tomorrow, we thought we could easily put together a smallish Apache Dinner for tonight. So Torsten mailed a few people, booked some space at Heinz Minki in Kreuzberg. We announced it at the end of the conference and invited people to join us.\nSo after a beautiful day out I spent the evening with a bunch of Apache related guys and girls having drinks and great pizza: We had several svn committers, Greg Stein met us there - he arrived today for the svn conference. Of couse the usual suspects were there as well: Simon Willnauer and his wife, Torsten Curdt, Erik Abele, Thomas Wöhlke, Valerie Hajdik. In addition we had guests from the Apache Lucene and Apache Hadoop communities: Sarah Sproehnle and Aaron Kimball as well as Uwe Schindler joined us.\nThanks to Torsten for putting the meetup together. See you next time in July. If you are interested in joining our meetups: Subscribe to our mailing list.\n"},{"id":292,"href":"/apache-dinner-after-berlin-buzzwords28/","title":"Apache Dinner after Berlin Buzzwords","section":"Inductive Bias","content":" Apache Dinner after Berlin Buzzwords # Staying in town after Berlin Buzzwords? Interested in meeting with the Apache folks here? Torsten Curdt kindly organises an Apache Dinner on Wednesday evening after Berlin Buzzwords. If you would like to participate, contact Torsten for details on when and where it will take place.\nThough named Apache dinner, there is no need to be Apache committer or even member to participate: Being generally interested in Apache projects and in meeting the guys behind the project is totally sufficient.\n"},{"id":293,"href":"/apache-dinner-may-201027/","title":"Apache Dinner - May 2010","section":"Inductive Bias","content":" Apache Dinner - May 2010 # This evening a bunch of Apache committers and friends gathered in Berlin Kreuzberg at \u0026ldquo;Goodmorning Vietnam\u0026rdquo; for tasty food, nice drinks - or put another way, for a very nice evening. Simon had booked the table - we were expecting no more than eight people. However, as with any user group these meetup tends to grow. Shortly after the appointed time we had to move to another table to fit everyone around. See below for a quick shot taken while eating (Thanks to Eric for taking the picture):\nThere were people from Lucene, from SVN, Cocoon, CouchDB, HttpComponents and various other projects. Even one potential future Mahout committer :) Counting attendees quickly I guess we were about fifteen people.\nLooking forward to the next meetup that will be scheduled to take place shortely after Buzzwords. Please talk to Torsten Curdt if you want to get notified or simply subscribe to our mailing list.\n"},{"id":294,"href":"/scaling-user-groups336/","title":"Scaling user groups","section":"Inductive Bias","content":" Scaling user groups # A few hours ago, Jan Lehnardt posted a link on How to organise a nerd conference - joking that this is how we planned Berlin Buzzwords. Well, it is not exactly that easy - however the comic actually is not so far from the truth either:\nAbout two years ago, after having started Apache Mahout together with Grant Ingersoll, Karl Wettin and others, several Apache Hadoop user groups, meetups and get togethers started to pop up all around the world. The one closest to me was the Hadoop user group UK. Back in 2008 I was pretty envious to all these user groups - being so distributed, there was no way I could ever attend all of them, though talks were certainly interesting. So the naive thought of a back then naive free software developer was: Let\u0026rsquo;s have that in Berlin. To have initial talks I called Stefan Groschupf. His answer was very positive: Oh yeah, let\u0026rsquo;s do this. I am in Germany for another two weeks, so it should be at about that timeframe. We agreed that if no-one showed up, we could still have some pizza together and share insights from our projects.\nFor the venue I knew from regular meetups of the Free Software Foundation Europe - read FSFE - that newthinking store was available for free for meetups for devs of free software. On I went, calling Martin from the store, booked the room. After that some mails went to the usual suspects, mailing lists and such. At the first meetup two years ago, more than 15 attendees - with two more people who had prepared slides. Pizzas obviously had to wait a little.\nIf you are wondering what that looked like back then - Thanks to Martin for taking the image back then and putting it online.\nWe (as in all attendees) decided to repeat the exercise three months later*, talks for the next time were proposed during that first session. Noone objected to having it in Berlin again - everyone knew this was the only way to avoid having to do the organization next time.\nThe meetup grew steadily in size, talks started being proposed three to six months in advance. I ended up creating not only a mailing list for the meetup but also a blog so I could publish news on Jan\u0026rsquo;s CouchDB talk and Lars George\u0026rsquo;s HBase talk back then. We got video sponsoring from Cloudera (Thanks Christophe), StudiVZ (Thanks Nils), and Nokia (Thanks Matt). Late last year I did the first European NoSQL meetup together with Jan Lehnardt - 80 attendees, lots of potential for more, the newthinking store obviously a bit too small for that :)\nIf you are wondering what NoSQL and Hadoop meetups looked like last time:\nDuring that meetup the idea was born for a larger NoSQL conference in Berlin in 2010. First ideas were tossed around together with Jan and Simon Willnauer during Apache Con US in Oakland. The topic Hadoop got added there. In January 2010 finally Lucene was added to the mix. We contacted newthinking for support - got a very warm welcome.\nNow - two years after the first Apache Hadoop Get Together Berlin we are proud to host Berlin Buzzwords - focussed on NoSQL, Apache Hadoop and search as in Apache Lucene.The conference is co-organised by newthinking communications, Simon Willnauer, Jan Lehnardt and myself. A big thanks to neofonie for supporting me by making it possible that I could do most of the organisation during my regular working hours.\nThe speaker lineup looks fantastic. Registration is going very well - exceeding expectations (did I mention that registration is still open, group and student tickets still available?). I am really looking forward to an amazing conference on 7th and 8th of June. We will have a NoSQL barcamp in newthinking store Sunday evening before the conference. Keynote speaker packages have been sent out and were well received. Hotel rooms for speakers are booked. We are about to pull together the last loose ends in the coming days. Happy to have so many guys (and a few girls) interested in scalability topics here in town at the beginning of June. Looking forward to seeing you in Berlin.\n* The second meetup turned out to be the first and so far only one that took place w/o the organiser - I broke my leg on my way to newthinking by getting hit by a BMW X5\u0026hellip; sigh Note for other meetup organizers: Always have a backup moderator - in may case that was my neofonie manager Holger Düwiger who happened to attend that meetup for the first time back then.\n"},{"id":295,"href":"/going-to-berlin-buzzwords213/","title":"Going to Berlin Buzzwords","section":"Inductive Bias","content":" Going to Berlin Buzzwords # Meet me in two weeks at Berlin Buzzwords. As you may have noticed, together with Simon Willnauer, Jan Lehnardt and newthinking communications I am organising Berlin Buzzwords - a conference on scalable search, data analysis and storage.\n\u0026lt;img src=\u0026ldquo; http://berlinbuzzwords.de/sites/berlinbuzzwords.de/files/buzzwords_banner.jpg\" alt=\u0026ldquo;I\u0026rsquo;m going to Berlin Buzzwords\nthe conference on searching, processing and storing data.\u0026rdquo;/\u0026gt;\nThere are a few regular tickets left, so don\u0026rsquo;t wait too long to register. If you want to bring your friends, check out our group tickets with up to 50% discount. If you are a student bring your student ID and register for 100,- Euro. "},{"id":296,"href":"/solving-puzzles377/","title":"Solving puzzles","section":"Inductive Bias","content":" Solving puzzles # Like most software developers I like tasks that involve solving more or less complex problems analytically. Most developers I know love puzzles - either those that involve dis-entangling metal rings, or those involving putting wooden pieces back into order, or even solving Rubik\u0026rsquo;s cube:\nWorking on the schedule for Berlin Buzzwords, I noticed that coming up with a good schedule actually has a lot more in common with solving puzzles that one is usually aware of: First of all talks on similar or related topics should not take place at the same time. Presentations should be grouped according to common topics so attendees don\u0026rsquo;t have to switch room after each and every talk. In addition some speakers have a tight schedule themselves and can only be at the conference for a day.\nIt gets even more interesting if after having put up a draft of the initial schedule you start filling the gaps, publishing those talks that were confirmed later than others or could be accepted only after freeing a spot somewhere else.\nI spent the past few day re-arranging the Berlin Buzzwords schedule a bit. I added Christophe\u0026rsquo;s talk on Apache Hadoop from an industry perspective. After adding it, I had 45min left in the NoSQL track - on the other hand there was a speaker from the Lucene community that we very much liked to have in. So off I went, sorting and shifting around until finally the Lucene talk ended up in the Lucene track and a Hadoop talk that was formerly there ended up in the Apache Hadoop track, resulting in one NoSQL talk from the Apache Hadoop track moving over to the NoSQL track\u0026hellip;\nTo cut a long story short: The schedule is final now - unless changes need to be made last minute.\n"},{"id":297,"href":"/tierpark-berlin410/","title":"Tierpark Berlin","section":"Inductive Bias","content":" Tierpark Berlin # I love taking fotos, like being outdoors and like animals. Living in a large city, it is not exactly easy to get in touch with donkeys or sheep. A very simple way to combine all three preferences here is to visit Tierpark Berlin. Being larger than your average zoo, most bawns are rather roomy with lots space outside and inside.\nLittle more than one year ago I received a great birthday present from Thilo: It is possible to purchase a one year ticket for that park. Since we\u0026rsquo;ve been there pretty often. In spring when most animal babies are born, in summer to escape heat in town, in winter when all paths are white of snow.\nOne attraction we usually do not want to miss are the pelicans (Thanks to p_h_o_t_o_m_i_c who took the picture below):\n\u0026lt;img src=\u0026quot;/tintenpatrone.jpg\u0026quot; alt=\u0026ldquo;Tinte\u0026rdquo; /\u0026gt;\nIf you are wondering \u0026ldquo;What\u0026rsquo;s that thing hanging \u0026lsquo;round her neck?\u0026rdquo; Whenever I have some spare time left, I usually take my camera with me. It\u0026rsquo;s not particularly new, not even a digital one. My parents already used it during vacation before I was born. It\u0026rsquo;s a Praktika Nova 1 - capable of taking breathtakingly beautiful images:\n\u0026lt;img src=\u0026quot;/praktika.jpg\u0026quot; alt=\u0026ldquo;praktika\u0026rdquo; /\u0026gt;\nThere\u0026rsquo;s just one catch: The camera is not self focussing, nor does it come with an internal exposure meter. Instead the one I have is to be used separately before taking the picture.\nHowever even after reading just a tiny little bit about f-numbers, exposure times initial pictures I took four years ago were astonishingly beautiful. Since I regularly tend to go out just for taking pictures.\n"},{"id":298,"href":"/getting-a-ubuntu-laptop-setup-for-my-mum210/","title":"Getting a Ubuntu Laptop setup for my Mum","section":"Inductive Bias","content":" Getting a Ubuntu Laptop setup for my Mum # With DSL contracts getting ever cheaper in recent years in Germany – even outside larger cities – my mom decided to get a faster internet connection (compared to the former 56k modem) including a telephone landline flatrate.\nAs sitting in the garden while surfing the internet is way cooler than only having a dedicated computer in an office we decided to get a notebook while at it. As both Thilo and myself are very familiar with Linux, the plan was to get a Linux-compatible netbook, install Ubuntu on it, get wireless up and running, pre-configure the necessary applications and hand it over after a short usage introduction.\nWell – first idea: Mom is living close to Chemnitz, so we drove to the Media Markt in Chemnitz Center. They had a nice, not too small and not too large Acer netbook. Only question that was open: Does that thing perform well with Linux? Easily solved: We had a bootable USB stick with the latest Ubuntu version with us. We asked one of the shop assistants for permission to boot Linux from the netbook – telling him that we wanted to buy the notebook, only making sure everything works fine. Answer: “No, sorry, that is not possible. There could be a virus on that stick.” Knowing from my favourite Mac shop in Berlin that there are hardware suppliers that allow testing their products, we went out of Media Markt – disappointed, but with the plan to repeat the experiment at various other suppliers in Berlin.\nMonday afternoon the following week Thilo went to a MediMax in Berlin. Experience was way different: The assistant was most helpful, offering various machines to try out – unfortunately none of them had an Intel graphics card – that is, none could be run with a free graphics driver.\nEnd of the same week we went to Media Markt in Steglitz: Asking the assistant there for permission to boot linux from our USB stick actually made him happy. As the machine not only matched our target specifications but was even cheaper than the one in Chemnitz and did work well with Ubuntu we finally bought the notebook (Acer Timeline 3810T). Yeah: Finally not only a working machine (with 8 hours of battery time) but also a shop that cares about its custormers.\nFor two weeks now mom is now happy user of the Ubuntu netbook edition – step by step learning how to write e-mails, chat and use the internet. As usual first thing we tried out was searching for vacation destinations, but also for at least my name. The latter searches seemed to be most interesting – at least at Google, YouTube, flickr \u0026hellip; ;)\n"},{"id":299,"href":"/berlin-buzzwords-2010-scalability-conference-june-7th-8th-in-berlin115/","title":"Berlin Buzzwords 2010 - Scalability conference June 7th/ 8th in Berlin","section":"Inductive Bias","content":" Berlin Buzzwords 2010 - Scalability conference June 7th/ 8th in Berlin # The Berlin Buzzwords schedule was published a few days ago. There are tracks specific to the three tags search, store and scale. We have a fantastic mixture of developers and users of open source software projects that make scaling data processing today possible.\nThere is Steve Loughran, Aaron Kimball and Stefan Groschupf from the Apache Hadoop community. We have Grant Ingersoll, Robert Muir and the \u0026ldquo;Generics Policeman\u0026rdquo; Uwe Schindler from the Lucene community.\nFor those interested in NoSQL databases there is Mathias Stearn from MongoDB, Jan Lehnardt from CouchDB and Eric Evans, the guy who coined the term NoSQL one year ago.\nThe schedule has been published online. Visit the webpage and register for the conference - looking forward to seeing you in Berlin this summer!\nRegular tickets are available online. In addition we offer student tickets: If you have a valid student ID, you are eligible for one of these tickets. Each costs 100,- Euro. We also have a special group ticket available: If you buy four tickets or more you are eligible for a discount of 25%, when purchasing 10 tickets or more the discount is 50%. Learn more at http://berlinbuzzwords.de/content/tickets\nOne day before the conference we are having a Berlin Buzzwords Barcamp in town. In addition, directly after the conference, Cloudera will be hosting Apache Hadoop trainings - registration is separate from Berlin Buzzwords.\nSo just in case you need a good excuse for a long term trip to Berlin: You can spend the weekend in town, attend the Barcamp on Sunday evening, visit Berlin Buzzwords on Monday and Tuesday. The rest of the week could be used to take part in Apache Hadoop trainings. Finally you have one weekend left for a city tour.\nThanks to Jan Lehnardt, Simon Willnauer and newthinking communications for co-organising the event.\n"},{"id":300,"href":"/apache-dinner-april-201026/","title":"Apache Dinner - April 2010","section":"Inductive Bias","content":" Apache Dinner - April 2010 # Today, the April Apache Dinner took place in Berlin. We met at Sadhu - an Indian restaurant in Berlin X-Berg. We were six people: Lars Trieloff from Day Software, Simon Willnauer and Vera from Lucene, Torsten Curdt - currently Freelancer and Daniel Naber from Lucene as well.\nWith great food, nice discussions and a first glimpse on the submissions for Berlin Buzzwords it quickly got later and later :)\nIf you are Apache Committer/Member yourself or are simply interested in learning more about this crazy bunch of software developers meeting each month in a different restaurant, please contact Torsten for more information on the meetup - and to be included on the next meetup schedule. As we are by far more than ten people interested in delicious food and meeting other \u0026ldquo;indians in Berlin\u0026rdquo;, I created a Dinner mailing list - please join, if you plan to take part in the dinner in the near future.\n"},{"id":301,"href":"/apache-dinner-berlin-this-evening32/","title":"Apache Dinner Berlin - next Monday","section":"Inductive Bias","content":" Apache Dinner Berlin - next Monday # Next Monday at 7p.m. the April Apache Dinner Berlin is scheduled to take place. Thanks to Torsten Curdt for organising the dinner - as in: Contacting people, finding a suitable data, booking the location etc.\nLooking forward to another nice evening at an awesome restaurant with tasty indian food. If you\u0026rsquo;d like to join us, please contact Torsten to be included in the next announcement mail. The dinner is not restricted to Apache people from Berlin: Anyone who is fine joining a group of geeks and free software hackers for dinner is invited to join us. If you are Apache developer planning to visit Berlin in the coming months, please contact Torsten so we can try to schedule the respective dinner to match your calendar.\n"},{"id":302,"href":"/berlin-buzzwords-end-of-cfp-drawing-closer113/","title":"Berlin Buzzwords - End of CfP drawing closer","section":"Inductive Bias","content":" Berlin Buzzwords - End of CfP drawing closer # One week to go for submitting a talk on your favourite NoSQL topic, your favourite search application or your most interesting data analysis task: The call for presentations for Berlin Buzzwords ends on April 17th, that is Sunday next week.\nShortly after the last talk was submitted we will start announcing speakers - final list of speakers is to be expected by the start of May, final schedule will be published shortly after that.\n"},{"id":303,"href":"/berlin-buzzwords-early-bird-registration112/","title":"Berlin Buzzwords - Early bird registration","section":"Inductive Bias","content":" Berlin Buzzwords - Early bird registration # I would like to invite everyone interested in data storage, analysis and search to join us for two days on June 7/8th in Berlin for Berlin Buzzwords - an in-depth, technical, developer-focused conference located in the heart of Europe. Presentations will range from beginner friendly introductions on the hot data analysis topics up to in-depth technical presentations of scalable architectures.\nOur intention is to bring together users and developers of data storage, analysis and search projects. Meet members of the development team working on projects you use. Get in touch with other developers you may know only from mailing list discussions. Exchange ideas with those using your software and get their feedback while having a drink in one of Berlin\u0026rsquo;s many bars.\nEarly bird registration has been extended until April 17th - so don\u0026rsquo;t wait too long.\nIf you would like to submit a talk yourself: Conference submission is open for little more than one week. More details are available online in the call for presentations:\nLooking forward to meeting you in the beautiful, vibrant city of Berlin this summer for a conference packed with high profile speakers, awesome talks and lots of interesting discussions.\n"},{"id":304,"href":"/working-on-mahout-as-part-of-your-studies-at-tu-berlin437/","title":"Working on Mahout as part of your studies at TU Berlin","section":"Inductive Bias","content":" Working on Mahout as part of your studies at TU Berlin # Did you ever wonder, who those weird people working on free software projects are? Did you ever ask yourself how these developers organise their work, how they collaborate, which values are important to them? Did you ever think about participating in a free software project yourself but never really had time to do so because your studies were just too time-consuming?\nWell, if you are a student of one of the Berlin universities, there is a project at the research group DIMA at TU Berlin that might be of interest to you: With Hot Topics in Information Management the second edition of last year\u0026rsquo;s course focussed on building systems with Apache Mahout.\nThis term the course will concentrate on extending Mahout. During the first week, students are given a set of possible project ideas to choose from. Of course you are invited to add your own ideas as well. You will need to come up with a rough plan of material to read, modules to implement and a timeframe for each module.\nYou are asked to not only implement your choosen extension but to thouroughly (unit-/integration-) test it, to document it, to provide examples of its usage and finally to work together with the community on contributing your implementation back to the project.\nDuring the course you are free to re-use resources built up for last year\u0026rsquo;s course - both hardware as well as installed software and available data.\nThe course starts next week on Tuesday - registration closes in a few days, so make sure you signed up if you are interested in working on Mahout during your regular project time and get credits for that.\n"},{"id":305,"href":"/gsoc-one-day-to-go-for-your-application218/","title":"GSoC - one day to go for your application","section":"Inductive Bias","content":" GSoC - one day to go for your application # If you are a student interested in participating in Google Summer of Code: Registration closes tomorrow (as in \u0026ldquo;April 9, 19:00 UTC\u0026rdquo;). You hopefully published and discussed your proposal at your favourite project already so you have a clear plan of where to go and which milestones to achieve in summer.\nIf you are interested in Apache Mahout: Yes, as last years, we are again looking for students willing to work on awesome student projects this summer. Several core Mahout developers have signed up as mentors for GSoC. With Robin one of our former GSoC students now has turned into a mentor: It\u0026rsquo;s always amazing to watch students stick with the project and continue contributing valuable input.\nSo in case you would love to learn more on machine learning, train your software development skills and work with great people on your favourite problem, do not forget to submit your project proposal until tomorrow.\n"},{"id":306,"href":"/coaching-self-organising-teams143/","title":"Coaching self-organising teams","section":"Inductive Bias","content":" Coaching self-organising teams # Today, the Scrumtisch organised by Marion Eickmann from Agile 42 met in Berlin Friedrichshain. Though no talk was scheduled for this evening the room was packed with guests from various companies and backgrounds interested in participating in discussions on Scrum.\nAs usual we started collecting topics (timeboxed to five minutes). The list was rather short, however it contained several interesting pieces:\n(6) Management buy-in\n(6+) CSP - Certified Scrum Professional - what changes compared to the practitioner?\n(4) Roles of Management in Scrum - how do they change?\n(13) Coaching self-organising teams - team buy in.\nTeam buy-in\nAs prioritised by the participants the first topic discussed was on coaching self organising teams - with a heavy focus on team buy-in. The problem described dealt with transforming water fall teams that are used to receiving their work items into self organising teams that voluntarily accept responsibility for the whole project instead of just their own little work package.\nThe definition of self organising here really is about teams, that have no (and need no) dedicated team leader. On the contrary: leadership is automatically transferred to the person who - based on his skills and experiences - is best suited for the user story that is being implemented at any given time.\nThe problem the question targets is about teams, that really are not self organising, where developers do not take responsibility for the whole project, but just for their little pieces: They have their gardens - with fences around that protect them from others but also protect themselves from looking into other pieces of the project. Even worse - they tend to take these fences with them as soon as work items change.\nSeveral ways to mitigate the problem were discussed:\nTeams should work in a collaborative environment, should have clear tasks and priorities, whould get some pressure from the outside to get things done.\nSome teams need to learn what working in a team - together - really means. It may sound trivial, but what about solving problems together: Spending one day climbing hills?\nCommittments should not happen on tasks (which by definition are well defined and small) but rather on Features/ user stories. Task breakdown should happen after the committment.\nThere are patterns to break user stories that are too large into multiple user stories. (Marion: Would be great, if I could add a link here ;) )\nTeams need to be coached - not only the scrum master should get education, but the complete team. There are people interested in management that tend to read up on the topic after working hours - however these are rather rare\u0026hellip;\nTeams must be empowered - they must be responsible for the whole project and for the user stories they commit to. In return they must get what the need to get their tasks done.\nNewly formed teams inexperienced with Scrum have to get the chance to make mistakes - to fail - and to learn from hat.\nA great way to explain Scrum really is as a two-fold process: First half is about getting a product done, reviewing quality by the end of each sprint during the review. Second half is about improving the process to get the product done. Meeting to review the process quality is called retrospective.\nManagement buy-in\nThe second topic discussed was on the role of management in scrum - and how to convince management of Scrum. To some extend, Scrum means loosing power and control for management. Instead of micro-manageing people it\u0026rsquo;s suddenly about communicating your vision and scope. To get there, it helps to view lean management as the result of a long transformation:\nFirst there is hierarchical management - with the manager at the top and employees underneath.\nSecond there is shared management - with the manager sitting between his employees enabling communication.\nThird there is collaborative management - here the manager really is part of the team.\nFourth comes empowering management - this time the manager is only responsible for defining goals.\nLast but not least there is lean management - where managers are merely coordinating and communicating the vision of a project.\nTo establish a more agile management form, there are several tasks to keep in mind: First and foremost, do talk to each other. Explain your manager what you are doing and why you are working in pairs, for instance. Being a manager, do not be afraid to ask questions - understanding what your developers do, helps you trust their work. Scrum is established, however there needs to be a clear communication of what managers loose - and what they win instead.\nScaling can only be done via delegation\nhowever people need to learn how to delegate tasks. In technology we are used to learning new stuff every few years. In management this improvement cycle is not yet very common. However especially in IT it should be. Being able to sell Scrum to customers is yet another problem: You need good marketing to sell Scrum to your customers. \u0026ldquo;Money for nothing change for free\u0026rdquo; is a nice to read on formulating agile contracts. Keep in mind, that the only way to really win all benefits is by doing all of Scrum - cherry picking may work to some extend, however you won\u0026rsquo;t get the full benefit from it. In most cases it works worse than traditionally managed projects.\nAfter two very interesting and lively discussions moderated by Andrea Tomasini we finally had pizza, pasta and drinks - taking some of the topics offline.\nLooking forward to seeing you in F-Hain for the next Scrumtisch in April. "},{"id":307,"href":"/some-pictures378/","title":"Some pictures","section":"Inductive Bias","content":" Some pictures # Uwe and Simon were so kind to take some pictures of the last Hadoop Get Together in Berlin:\n\u0026lt;img src=\u0026quot;/hadoop_march_1.JPG\u0026quot; alt=\u0026ldquo;Image Hadoop Get Together Berlin\u0026rdquo; /\u0026gt;\n\u0026lt;img src=\u0026quot;/hadoop_march_5.JPG\u0026quot; alt=\u0026ldquo;Image Hadoop Get Together Berlin\u0026rdquo; /\u0026gt;\nThanks for the pictures.\n"},{"id":308,"href":"/bob-schulze-on-tips-and-patterns-with-hbase126/","title":"Bob Schulze on Tips and patterns with HBase","section":"Inductive Bias","content":" Bob Schulze on Tips and patterns with HBase # At the last Hadoop Get Together in Berlin Bob Schulze from eCircle in Munich gave a presentation on “Tips and patterns with HBase”. The talk has been video recorded. The result is now available online:\nHBase Bob Schulze from Isabel Drost on Vimeo.\nFeel free to share and distribute the video. Thanks to Bob for an awesome talk on eCircle’s usage of HBase - and on providing some background information on how HBase was applied to solve your problems.\nAnother thanks to Nokia for sponsoring the video taping - and to newthinking for providing the location for free.\nLooking forward to Berlin Buzzwords in June. Early registration is open already. Several great talk proposals have been submitted already. If you are a Hadoop Get Together visitor (or even speaker) and would like to have a community ticket, please contact me.\n"},{"id":309,"href":"/video-up-dragan-milosevic-on-product-search-and-reporting-with-hadoop164/","title":"Dragan Milosevic on Product Search and Reporting with Hadoop","section":"Inductive Bias","content":" Dragan Milosevic on Product Search and Reporting with Hadoop # At the last Hadoop Get Together in Berlin Dragan Milosevic from zanox in Berlin gave a presentation on \u0026ldquo;Product Search and Reporting powered by Hadoop\u0026rdquo;. The talk has been video recorded. The result is now available online:\n\u0026lt;param name=\u0026ldquo;allowscriptaccess\u0026rdquo; value=\u0026ldquo;always\u0026rdquo; /\u0026gt;Hadoop Dragan Milosevic from Isabel Drost on Vimeo.\nFeel free to share and distribute the video. Thanks to Dragan for a fantastic talk on Zanox\u0026rsquo; usage of Hadoop - and on providing some background information on why and how you introduced Hadoop into your systems.\nAnother thanks to Nokia for sponsoring the video taping - and to newthinking for providing the location for free.\nOne more video to go. It will be available early next week.\n"},{"id":310,"href":"/apache-mahout-03-released64/","title":"Apache Mahout 0.3 released","section":"Inductive Bias","content":" Apache Mahout 0.3 released # This week, Apache Mahout 0.3 was released. First of all thanks to all committers and contributors who made that possible: Thanks for all your hard work on making the code even faster and integrating even more algorithms.\nTo the highlights:\nNew: math and collections modules based on the high performance Colt library Faster Frequent Pattern Growth(FPGrowth) using FP-bonsai pruning\nParallel Dirichlet process clustering (model-based clustering algorithm)\nParallel co-occurrence based recommender\nParallel text document to vector conversion using LLR based ngram generation\nParallel Lanczos SVD(Singular Value Decomposition) solver\nShell scripts for easier running of algorithms, utilities and examples\n\u0026hellip; and much much more: code cleanup, many bug fixes and performance improvements. Check out the new release and watch for further news on Apache Mahout to come in the next days and weeks.\nDetails on what\u0026rsquo;s included can be found in the release notes. Downloads are available from the Apache Mirrors\n"},{"id":311,"href":"/seminar-on-scaling-learning-at-dima-tu-berlin364/","title":"Seminar on scaling learning at DIMA TU Berlin","section":"Inductive Bias","content":" Seminar on scaling learning at DIMA TU Berlin # Last Thursday the seminar on scaling learning problems took place at DIMA at TU Berlin. We had five students give talks.\nThe talks started with an introduction to map reduce. Oleg Mayevskiy first explained the basic concept, than gave an overview of the parallelization architecture and finally showed how jobs can be formulated as map reduce jobs.\nHis paper as well as his slides are available online.\nSecond was Daniel Georg - he was working on the rather broad topic of NoSQL databases. Being too fuzzy to be covered in one 20min talk, Daniel focussed on distributed solutions - namely Bigtable/HBase and Yahoo! PNUTS.\nDaniel\u0026rsquo;s paper as well as the slides are available online as well.\nThird was Dirk Dieter Flamming on duplicate detection. He concentrated on algorithms for near duplicate detection needed when building information retrieval systems that work with real world documents: The web is full of copies, mirrors, near duplicates and documents made of partial copies. The important task is to identify near duplicates to not only reduce the data store but to potentially be able to track original authorship over time.\nAgain, paper and slides are available online.\nAfter a short break, Qiuyan Xu presented ways to learn ranking functions from explicit as well as implicit user feedback. Any interaction with search engines provides valuable feedback about the quality of the current ranking function. Watching users - and learning from their clicks - can help to improve future ranking functions.\nA very detailedpaper as well as slides are available for download.\nLast talk was be Robert Kubiak on topic detection and tracking. The talk presented methods for identifying and tracking upcoming topics e.g. in news streams or blog postings. Given the amount of new information published digitally each day, these systems can help following interesting news topics or by sending notifications on new, upcoming topics.\nPaper and slides are available online.\nIf you are a student in Berlin interested in scalable machine learning: The next course IMPRO2 has been setup. As last year the goal is to not only improve your skills in writing code but also to interact with the community and if appropriate to contribute back the work created during the course.\n"},{"id":312,"href":"/chris-male-on-spatial-search-with-lucene137/","title":"Chris Male on spatial search with Lucene","section":"Inductive Bias","content":" Chris Male on spatial search with Lucene # Last week the March 2010 Hadoop Get Together took place in Berlin. Last speaker was Chris Male on spatial search with Lucene and Solr. The video is now available online:\nLucene Chris Male from Isabel Drost on Vimeo.\nFeel free to share and distribute the video to anyone who might be interested. Thank you Chris, for traveling over from Amsterdam for an awesome talk on spatial search.\nIf you want to learn more on what people over at Lucene and Solr are currently working one, head over to Berlin Buzzwords - a conference on scalable search, storage and data analysis. If you yourself have interesting projects - feel free to submit a talk.\nThanks to Nokia for sponsoring the video taping - and again as always thanks to newthinking for providing the location for free.\n"},{"id":313,"href":"/definition-of-a-blogger150/","title":"Definition of a Blogger","section":"Inductive Bias","content":" Definition of a Blogger # While at lunch yesterday the topic of what Bloggers do, how they earn money and most important of all - what the hack a blogger really is - came up. Well, some criteria those who went to a restaurant nearby came up with the following criteria:\nBlog is read by more than 5 people. (Well, in my opinion a very low barrier, really.)\nBloggers tend to get invited to give talks at conferences. (Yeah, well, not only people with blogs get those invitations?)\nOver time bloggers tend to get contracts, do consultancy and the like to earn money. (Hmm, yeah, blogs do help to get visibility\u0026hellip;)\nBloggers tend to be involved with traditional media people. (Phew - finally something that disqualifies myself as a blogger. Though, come to think of it - no having published one or two articles does not count. Period.)\nThey are those people you tend to see in cafes with Mac books surfing the web. (Oh, well, who hasn\u0026rsquo;t done that once in a while?)\nJudging from that very unscientific case-study even though taking into account the very informal nature, the result still appeared to be very scary to me: I had fought the temptation of actually creating a blog for years until publishing content for the Hadoop Get Together the very old-fashioned way (vim + scp) became too much of a burden. Now I have to realise that against my own judgement that I do tend to use this blog not exactly particularly seldom. Still trying to avoid to become one of these funny new media types and remain a typical free software developer :)\n"},{"id":314,"href":"/third-apache-dinner-berlin409/","title":"Third Apache Dinner Berlin","section":"Inductive Bias","content":" Third Apache Dinner Berlin # Today the third Dinner for Apache committers and friends took place in Berlin. We met in Schöneberg at Marcello Berlin for pizza, pasta, wine, beer \u0026hellip; and lots of discussions.\nIt always surprises me to see how many Apache related people there are in Berlin. This time we had Peter from Tomcat, Daniel and Simon with Vera from Lucene, Eric from http components, four guys from the svn project (Welcome again at the ASF), Oswald and myself and Thilo.\nWe scheduled the meetup comparably early - at about 6:30p.m. - giving the parents among us the chance to attend with their children: Looks like some projects recruit their new project members pretty early ;)\nNext time we will meet some time before or after Apache Retreat in April. Then again organised (as in \u0026ldquo;set a date\u0026rdquo;, \u0026ldquo;reserve a table at your favourite restaurant\u0026rdquo;, \u0026ldquo;call your friends\u0026rdquo;) by another Apache committer in Berlin. If you would like to join us, or are a committer yourself interested in finding out about other people in town with the same affiliation, do not hesitate to contact me: I\u0026rsquo;ll make sure you are included in the next vote on the dinner date.\nPS: Why on earth do user meetups in Berlin always have the tendancy of growing and growing and growing? ;)\n"},{"id":315,"href":"/building-a-hadoop-job-jar-with-maven129/","title":"Building a Hadoop Job Jar with Maven","section":"Inductive Bias","content":" Building a Hadoop Job Jar with Maven # Put here as a reminder, so I do not forget about it. There is a really nice tutorial online on Building Hadoop Job with Maven.\n"},{"id":316,"href":"/call-for-presentations-berlin-buzzwords132/","title":"Call for presentations - Berlin Buzzwords","section":"Inductive Bias","content":" Call for presentations - Berlin Buzzwords # Call for Presentations Berlin Buzzwords\nhttp://berlinbuzzwords.de\nBerlin Buzzwords 2010 - Search, Store, Scale\n7/8 June 2010\nThis is to announce the opening of the Berlin Buzzwords 2010 call for presentations. Berlin Buzzwords is the first conference on scalable and open search, data processing and data storage in Germany, taking place in Berlin.\nThe event will comprise presentations on scalable data processing. We invite you to submit talks on the topics:\nInformation retrieval, search - Lucene, Solr, katta or comparable solutions\nNoSQL - like CouchDB, MongoDB, Jackrabbit, HBase and others\nHadoop - Hadoop itself, MapReduce, Cascading or Pig and relatives Closely related topics not explicitly listed above are welcome. We are looking for presentations on the implementation of the systems themselves, real world applications and case studies.\nImportant Dates (all dates in GMT +2)\nSubmission deadline: April 17th 2010, 23:59 Notification of accepted speakers: May 1st, 2010. Publication of final schedule: May 9th, 2010. Conference: June 7/8. 2010. High quality, technical submissions are called for, ranging from principles to practice. We are looking for real world use cases, background on the architecture of specific projects and a deep dive into architectures built on top of e.g. Hadoop clusters.\nProposals should be submitted at http://berlinbuzzwords.de/content/cfp no later than April 17th, 2010. Acceptance notifications will be sent out on May 1st. Please include your name, bio and email, the title of the talk, a brief abstract in English language. Please indicate whether you want to give a short (30min) or long (45min) presentation and indicate the level of experience with the topic your audience should have (e.g. whether your talk will be suitable for newbies or is targeted for experienced users.)\nThe presentation format is short: either 30 or 45 minutes including questions. We will be enforcing the schedule rigorously.\nIf you are interested in sponsoring the event (e.g. we would be happy to provide videos after the event, free drinks for attendees as well as an after-show party), please contact us.\nFollow @hadoopberlin on Twitter for updates. News on the conference will be published on our website at http://berlinbuzzwords.de\nProgram Chairs: Isabel Drost, Jan Lehnardt, and Simon Willnauer.\nSchedule and further updates on the event will be published on http://berlinbuzzwords.de "},{"id":317,"href":"/slides-are-available371/","title":"Slides are available","section":"Inductive Bias","content":" Slides are available # Slides for the last Hadoop Get Together are available online:\nSpatial Search by Chris Male\nHBase Patterns by Bob Schulze\nScaling product search with Hadoop and Lucene by Dragan Milosevic\nMy own little introduction, just in case you are interested.\nVideos will follow as soon as the are ready. Watch this space for further updates.\n"},{"id":318,"href":"/apache-hadoop-get-together-march-201060/","title":"Apache Hadoop Get Together March 2010","section":"Inductive Bias","content":" Apache Hadoop Get Together March 2010 # Today (or more correctly, yesterday) the March 2010 Hadoop Get Together took place in newthinking store. I arrived rather early to have some time to do some planning for Berlin Buzzwords - got there nearly one hour before the meetup. However it did not take very long until first guests came to the store. So I quickly got my introductory slides in place - Martin from newthinking already had the room setup, camera in place and audio working.\nWhen starting the meetup the room was already packed with some 60 people - we ended up having over 70 people interested in the mix of talks on Hadoop, HBase and Spatial search with Lucene and Solr. Doing the regular \u0026ldquo;Who are you\u0026rdquo;-round, we learned that there were people from nurago, Xing, StudiVZ, lots and lots of people from Nokia, Zanox, eCircle, nugg.ad and many others.\nThe meetup was kindly supported by newthinking store (venue for free) and Nokia (sponsored the videos). Steffen Bickel took his chance during the introduction to give a brief overview of Nokia and - guess - explain, that Nokia is a great place to work and yeah - they are hiring!\nThe first talk was given by Bob Schulze who joined the meetup coming from eCircle in Munich. Given his previous experience with scaling their infrastructure from a regular database/ datawarehouse setup he explained how HBase helped when processing really large amounts of data. Being an e-mail marketing provider, eCircle does have quite a bit of data to process. And yes, eCircle is hiring.\nSecond talk was by Dragan Milosevic from Zanox on scaling product search and reporting with Hadoop. Just as eCircle, Zanox came from a regular RDMS setup that became too expensive and too complex too scale before switching over to a Hadoop/Lucene stack. He used his chance to make the Lucene developers aware of the fact that there are users who would were actually using Lucene\u0026rsquo;s compression features. Zanox, as well, is looking for people to hire.\nLast talk was by Chris Male from JTeam in Amsterdam on the developments in Lucene and Solr to support for spatial search. There are various development routes being followed: Cartesian tiers as well as numeric range searches. He also explained that most of the features are still under heavy development. He finished his talk with a demo on what can be done with spatial search in Lucene/ Solr. You already guessed so, JTeam is hiring as well ;)\nAfter the talks we went to Cafe Aufsturz for beers, drinks and some food. People enjoyed talking to each other exchanging experiences. A Lucene focussed table quickly formed - main topics: Spatial search, Lucene/Solr merge threads, heavy committing, Mike McCandless (is this guy real or just an alter-ego of the Lucene community?).\nAt some time around 11p.m. the core of the guests (well - the Lucene part of the meetup, that is Simon, Uwe and the guys from JTeam) moved over to a bar close by next to cinema central for some more beer and drinks. At about 1a.m. it finally was time to head home.\nI\u0026rsquo;d like to say thanks: First of all to the speakers. Without you the meetup would not be possible. Second to newthinking and Nokia for their support. And of course to all attendees for having grown the meetup to its current size.\nI had a really nice evening with people from the Hadoop, HBase and Lucene community. Special thanks to you guys from JTeam for traveling 6h to Berlin just for a \u0026ldquo;little\u0026rdquo;, though no longer that tiny, Hadoop meetup. Promise stands, to visit one of your next Lucene meetups in Amsterdam and present Mahout there - however I need some help finding affordable accomodation ;)\nHope to see you all in June at Berlin Buzzwords.\n"},{"id":319,"href":"/google-summer-of-code-starting214/","title":"Google Summer of Code starting","section":"Inductive Bias","content":" Google Summer of Code starting # As published on the Google Open Source blog the application period for mentoring organizations for GSoC starts now. The ASF is already in the process of applying. If you are a student, looking for an interesting project to work on during the coming summer - you might consider participating in GSoC. It does give you are great opportunity to get in touch with successful free software projects, learn how to work in global teams, improve your communication skills and last but not least show and publish your fantastic coding skills.\nIf you want to learn more on Why you should contribute to open source, the article by Shalin Shekhar Mangar is a great summary of some of the reasons why people work on open source projects.\n"},{"id":320,"href":"/learning-to-rank-challenge263/","title":"Learning to Rank Challenge","section":"Inductive Bias","content":" Learning to Rank Challenge # In one of his recent blog posts, Jeff Dalton published an article on currently running machine learning challenges. Especially interesting for those working on search engines and interested in learning new rankings from data should be the Yahoo! Learning to Rank Challenge to be held in conjunction with this year\u0026rsquo;s ICML 2010 in Haifa, Israel. The goal is to show that your algorithm does not only scale on real-world data provided by Yahoo!. Tasks are split in two. The first one focusses on traditional learning to rank procedures, the second one on transfer learning. Tracks are open to participants from industry and research.\nA second challenge was published by the machine learning theory blog. The challenge is hosted by Yahoo! as well and deals with Key scientific challenges in statistics and machine learning.\nBoth programs look pretty interesting - would be great to lots of people from the community participating and comparing their systems.\n"},{"id":321,"href":"/early-bird-registration-for-berlin-buzzwords-is-open166/","title":"Early bird registration for Berlin Buzzwords on June 7th/8th open","section":"Inductive Bias","content":" Early bird registration for Berlin Buzzwords on June 7th/8th open # Silently registration was opened in the past days for Berlin Buzzwords - a conference on scaling search, data processing and storage taking place on June 7th/8th in Berlin/ Germany. First 100 tickets will be sold for 250 Euros + tax. Registration is possible at later dates as well, however expect prizes to rise shortly before the conference starts.\nIf you clicked on it earlier this week and were wondering what those strange German terms were all about: We have put online an English version as well, so language shouldn\u0026rsquo;t be much of a problem anymore.\nTo avoid any confusion: Conference talks will be in English - no German language skills needed for that. It is perfectly well possible to get around in Berlin w/o speaking German, however knowing a few words as always will make it easier to make friends with people in shops and hotels ;)\n"},{"id":322,"href":"/chemnitzer-linuxtage136/","title":"Chemnitzer Linuxtage","section":"Inductive Bias","content":" Chemnitzer Linuxtage # Title: Chemnitzer Linuxtage\nLocation: Chemnitz\nLink out: Click here\nStart Date: 2010-03-13\nEnd Date: 2010-03-14\nNext week the Chemnither Linuxtage take place in - well - Chemnitz. It is the second largest Linux event after Linuxtag Berlin. However only obvious for speakers and exhibitors: It is one of those events that are known for its fantastic organisation. Nearly no problems, be it WiFi, admission to the exhibitors area, food or any help in general.\nI will be at the event again. You can find me at the FSFE booth, telling people what the FSFE is all about and trying to convince them to become fellows (and yes, since last summer, I am a fellow myself and own one of those really cool green crypto cards).\n"},{"id":323,"href":"/berlin-ignite274/","title":"Mahout at Berlin ignite","section":"Inductive Bias","content":" Mahout at Berlin ignite # This evening the first Berlin ignite event took place in the \u0026ldquo;Festsaal\u0026rdquo; in Berlin X-Berg. Organiser of the event was Matt Biddulph from Nokia Gate 5. We had eleven fantastic talks (ok, to be more precise: At least ten fantastic ones, my own can only be judged by the audience ;) ).\nTopics included things you can learn when starting to collect data, themes from (agile) project management, RepRap machines (see also the Rep Rap FOSDEM 2010 talk), bots and robots. The talks finished with a presentation of a Part time scientist\u0026rsquo;s vision of getting to the moon - an article on the project is available on heise newsticker.\nThe room was filled with more then 120 people resulting in a location packed with interested attendees. It was great seeing the talks on such diverse topics. Hope to have more events of this format here in Berlin. Thanks go to Matt, all speakers and everyone involved in generally making the event a big success. For those who didn\u0026rsquo;t make it to the event, slides and audio should go online soon. At least the slides on Mahout are available online.\n"},{"id":324,"href":"/preliminary-schedule-online-for-ignite-berlin326/","title":"Preliminary schedule online for ignite Berlin","section":"Inductive Bias","content":" Preliminary schedule online for ignite Berlin # Today first talks scheduled for ignite Berlin were published. If you yourself would like to give a talk: Submission seems to still be open.\n"},{"id":325,"href":"/alan-atlas-at-scrumtisch-berlin8/","title":"Alan Atlas at Scrumtisch Berlin","section":"Inductive Bias","content":" Alan Atlas at Scrumtisch Berlin # At the last Berlin Scrumtisch, @AlanAtlas gave a presentation on how he introduced Scrum at Amazon (starting as early as back in 2004). Introducing Scrum at Amazon by that time seemed natural due to a few factors:\nAmazon was and is always very customer centric. The original methodology of working backwards in time - that is starting with the press release, from there writing the FAQ and manual and finally get to the code - really made people concentrate on the product.\nTeams were sized according to the two-pizza rule: Each team is supposed to be no larger than can reasonably be fed for lunch with two pizzas. Turns out that is about five to ten people, given regular american pizzas.\nManagement didn\u0026rsquo;t really care exactly how the job of software development was done. They only cared that it was done. This proved as both - an advantage as it gave quite a bit of freedom - and a disadvantage - as indifference leads to impediments in the process that cannot be easily resolved.\nThe goal of Alan\u0026rsquo;s team was to build an infinitely scalable, zero downtime storage system that had a web service based interface - it was the S3 team. Given the task at hand, it was clear that the task wasn\u0026rsquo;t going to be anything even close to trivial. Alan\u0026rsquo;s idea: Try Scrum on that.\nHe himself came to the idea after attending an Agile conference and hearing about Scrum for the first time there. Alan requested a Scrum training from his manager, who approved it, provided it took place in Seattle - close to Amazon HQ that is. Turns out, the Scrum trainer available in Seattle happened to be Ken Schwaber.\nThe idea of Scrum basically was spread by word of mouth propaganda: In the hallways, in the smokers lounge, close to the coffee machine. People started adopting it - with some teams doing better, some worse. Allan himself got two days a quarter to give people an introduction to this funny new methodology called Scrum.\n18 months later, S3 was shipped - as a side effect, the number of subscribers to the internal scrum mailing list had increased from three to 100, there were 150 CSM graduates, still three teams using XM, reputation of scrum was mostly positive.\nIn January 2007 others started giving their own introductions to Scrum. Developers generally liked it, there were significant failure cases, lots of resistance and misunderstanding mostly on the management side who sometimes confused it with lean production.\nIn June 2008 Allen became a fulltime internal Scrum trainer. First thing organised was a Scrum gathering - in an open spaces like setup people were invited to discuss their issues and questions on Scrum.\nIn January 2009, about 50% Scrum usage was reached, their were 600 subscribers on the Scrum mailing list, 450 CSM graduates, reputation of Scrum was as good (or bad) as months before. However it became more and more clear that further restructuring would only be possible against a lot of resistance.\nThere were a few lessons learned from introducing Scrum in other companies as well:\nYou cannot introduce Scrum if there is no notion of comparably stable teams (as in for at least six months, not five projects at a time per developer).\nYou need permission and ownership of committments to really get going.\nYou need to know at least the basics of Scrum.\nThere are a few factors that make introduction easier: Community matters. People need to be able to talk to each other, exchange experiences and learn from one another. Coaching matters. More support, immediate success in lots of cases is the result of getting a coach on the team. Credibility for management increases, Scrum implementation is easier understood that way, skepticism and resistance reduced. Management matters. Middle management must be part of the process. They will mess up scrum if not educated correctly. Going bottom-up works up to a point - but to go the whole way, you do need management.\nFor Amazon, that is the point where implementation got stuck for Allan: Management was neither interested nor really involved in Scrum. However there are some impediments that cannot be fixed w/o. Still Scrum is working to some extend - teams are still trying to get better and improve the process. So it is not really \u0026ldquo;Scrum, but\u0026rdquo;. Even going only part of the way helped a lot already.\nAs for the questions: There were a few unusual ones\ne.g. on what to do with a team that is itself skeptical about Scrum introduction - especially if that was done by higher management. There are a few ways to remedy that: Point out success to the negative team member. Make that member a Scrum master to let him go through the process and really understand what is going on. And above all make the reasons for going that way clear and transparent. Including promises and wishes from that change.\nAnother question dealt with how to convince management of Scrum. Clearly better performing teams with happy developers (\u0026ldquo;you cannot buy developers with money only\u0026rdquo;) are some valid reasons.\nYet another question dealt with convincing customers. Allan\u0026rsquo;s way is to sneak Scrum into the process: Speak the customers language. If he does not like spring planning, call it a project status meeting.\nAsked for cases of developers feeling like they work in a hamster wheel when doing Scrum, the general consensus was that it needs to be made sure that people do not only get more responsibilities but get the benefits from Scrum as well. Otherwise Scrum with all its numbers and measurements can be perfectly abused and turned into an amazing micro management tool.\nAsked for other bay area companies using Scrum - yes, apple does so, Microsoft at least tries to. Google certainly uses the ideas, without calling it Scrum. Facebook seems not to use it. Xing (not bay area) uses Scrum. There seem to be about 50% of all software development companies worldwide who are using Scrum.\nAfter the talk there was time to gather and have some pizza and pasta, time for drinks and discussions. I really appreciated the comments and ideas exchanged after the talk. Hope to see you all next time around. "},{"id":326,"href":"/apache-brand-name-survey11/","title":"Apache brand name - survey","section":"Inductive Bias","content":" Apache brand name - survey # Sally Khudairi (VP, ASF Marketing \u0026amp; Publicity) asked for distributing the following survey to people who might be interested in contributing their views to a study on how the brand name Apache is perceived. Me personally, I would be especially interested in finding out more on whether there are any differences in perception inside the ASF vs. outside\u0026hellip;\nWe have been working with PhD candidate Roland Schroll over the past two years as he\u0026rsquo;s been compiling information on the value of the Apache brand. His advisor is community-based innovation expert Dr. Johann Füller. This is a joint project of the University of Innsbruck and the Massachusetts Institute of Technology.\nIf you have 10 minutes to help, it would be much appreciated. The survey is at http://surveys.hyvelive.de/10_apache/p1.php?refGroup=Apache They would like the surveys to be completed this month (February). They are seeking at least 300 respondents. As such, if you know others who are interested in Apache from a market perspective, feel free to forward the link to them as well.\nThanks in advance for your interest!\nKind regards,\nSally Khudairi\nVP, ASF Marketing \u0026amp; Publicity\n"},{"id":327,"href":"/mloss-workshop-at-icml-accepted285/","title":"MLOSS workshop at ICML accepted","section":"Inductive Bias","content":" MLOSS workshop at ICML accepted # The workshop on machine learning open source software has been accpted. Find further details on the workshop homepage.\nSubmissions are open until April 10th, Samoa time.\n"},{"id":328,"href":"/fosdem-video-recordings-online183/","title":"FOSDEM - video recordings online","section":"Inductive Bias","content":" FOSDEM - video recordings online # As published in the FOSDEM blog the video recordings are available online - at least for the main track and the lightning talks. Happy video watching!\n"},{"id":329,"href":"/fsfe-happy-valentine200/","title":"FSFE Happy Valentine","section":"Inductive Bias","content":" FSFE Happy Valentine # Today I got woken up with a friendly hug and roses waiting for me:\nI do not really care about presents for sort-of-artificial celebration days like valentines day. However, FSFE had a very nice idea: The proposal was to use valentines day to show your love for free software. The website proposed to e.g. hug a free software developer, to make a gift to a team of free software developers:\nHappy Valentine!\n"},{"id":330,"href":"/open-community-camp-2010315/","title":"Open Community Camp 2010","section":"Inductive Bias","content":" Open Community Camp 2010 # The following information just reached my via Marten Vijn. Thought it might be interesting to you:\nI am pleased to announce OpenCommunityCamp 2010.\nThe camp is from 10th to 18th July, in Oegstgeest, the Netherlands.\nThe website[1] is refreshed and the first speakers are booked.\nIt is time to register[2] if you plan be there (please do this\nquickly). Currently we need to find more people to attend, self-organizing groups\nfor the day program and interesting speakers for the evening program.\nI look forward to hear your ideas and plan if you come. If you have\nany questions don\u0026rsquo;t hesitate to mail me.\nkind regards,\nMarten Vijn\n1. http://opencommunitycamp.org\n2. http://opencommunitycamp.org/site/?q=node/20\n3. http://opencommunitycamp.org/site/?q=contact\n4. http://opencommunitycamp.org/svn/doc/OCC2010flyer.pdf\n"},{"id":331,"href":"/berlin-buzzwords-june-2010114/","title":"Berlin Buzzwords - June 2010","section":"Inductive Bias","content":" Berlin Buzzwords - June 2010 # As announced at FOSDEM: Early June (currently scheduled for 7th/8th) a conference on the topics scalable search, storage and processing will take place in Kalkscheune/Berlin. The conference is co-organised by newthinking store, Jan Lehnardt, Simon Willnauer, Thilo Fromm, and Isabel Drost.\nThe focus will be on NoSQL databases like CouchDB, Jackrabbit, MongoDB, HBase. Search tracks will cover topics like Lucene, Solr, katta and others. Data munging tracks will focus mainly on Hadoop, MapReduce in general and distributed systems.\nMore information including the call for presentations will be made available online next week on a separate webpage. Early registration starts in March. Watch this blog for more information or follow @hadoopberlin.\n"},{"id":332,"href":"/apache-hadoop-get-together-march-2010-update44/","title":"Apache Hadoop Get Together - March 2010 - Update","section":"Inductive Bias","content":" Apache Hadoop Get Together - March 2010 - Update # Due to conflicts in the schedule of newthinking store, we had to change the time of the Get Together slightly. We will start one hour earlier than announced.\nWhen: March 10th, 4p.m.\nWhere: newthinking store, Tucholskystr. 48, Berlin Mitte\nLooking forward to seeing you there.\n"},{"id":333,"href":"/fosdem-2010-part-3187/","title":"FOSDEM 2010 - part 3","section":"Inductive Bias","content":" FOSDEM 2010 - part 3 # Sunday started in Janson with Andrian Bowyer\u0026rsquo;s talk on RepRap machines, that is devices that can be used as manufacturing devices and are able to replicate themselves. After that I went over to the Mono dev room to listen to Miguel de Icaza on Mono Edge. A great talk on the history of Mono, the way the community interacts with Microsoft, the C# language itself and special features only available in Mono.\nAfter this talk we went over to Janson for Andrew Tanenbaum\u0026rsquo;s talk on Minix. We knew quite a bit of the talk already from Froscon two years ago, however Andrew is an awesome speaker, so it\u0026rsquo;s always fun to catch up on the news on Minix.\nThe scalability talk started with an introduction to Hadoop by myself and continued with a talk on the facebook infrastructure by David Recordon. According to feedback I got after the talk, laughing with Thilo helped quite a bit to get myself calm. Before the talk I received one very good recommendation of one of the audio guys: Imagine you are giving the talk to one of your best friends - and forget about the microphone. Though I had way more slides than minutes to talk, we had enough time for the Q\u0026amp;A session after the talk. I started the talk by learning more about the audience - however this time not by handing the microphone to those listening (room too large) - I just asked them \u0026ldquo;have you heard about Hadoop?\u0026rdquo; - half of the audience. Are you Hadoop users: one quarter maybe. How large are your clusters? - 10 to 100 nodes mostly. Have you heard of Zookeeper? - some, Hive - some more, Pig - a few, Lucene - a lot, Solr - a little less, Mahout - maybe 5, Mahout users: 1.\nTurns out the Mahout user in the audience was Olivier: It\u0026rsquo;s so nice to meet people you know are active on the mailing lists for real and have a chat with them. Hope to see you more often on the lists - and meet you face to face again.\nI used the chance to announce the Berlin Buzzwords 2010, a two day event on search and scalability buzzwords like cloud computing, Hadoop, Lucene, NoSQL and more. It takes place on June 7th and 8th in the center of Berlin. Follow this blog for further information. Judging from the input I got after the announcement there is quite some need for such a conference in Europe.\nThe slides of my talk are soon to be available online.\nAfter my talk I could stay in Janson: A talk on the Facebook infrastructure (not only the Hadoop side of things) followed. After that I met Lars George at the NoSQL dev room - unfortunately I did not manage to actually talk to Steven Noels, who organised the room.\nThe afternoon was reserved for Greg Kroah-Hartman on how to \u0026ldquo;Write and submit your first Linux Kernel Patch\u0026rdquo; - my personal conclusion: git is really awesome. I really, really need to find a few spare minutes to learn how to effectively use it.\nIn the evening we met with Pieter Hintjens for dinner - and to finalize an awesome weekend in Brussels and a great 10th anniversary FOSDEM. A huge Thank You to all volunteers and organisers of FOSDEM - you did a great job this year putting together an awesome schedule, you did a fantastic job making the now pretty huge event (with 306 talks and about 5000 hackers attending) run smoothly. Even the wireless was working from minute one. See you again at FOSDEM 2011.\n"},{"id":334,"href":"/fosdem-2010-part-2186/","title":"FOSDEM 2010 - part 2","section":"Inductive Bias","content":" FOSDEM 2010 - part 2 # The event itself featured 306 talks - so pretty hard to choose what to watch on two days. This time, not only the main tracks were awesome, but also several dev rooms featured very interesting talks by well known FOSS developers.\nSaturday started with a FOSDEM birthday dance done by all attendees. The first keynote speaker Brooks Davis explained his experiences promoting open source methods at a large company. After that Richard Clayton gave an amazing talk on the evil on the internet. He explained not only how phishing works on a technical level but also included an explanation of the economics behind these attacks, explained how the money flow from victims to attackers works.\nOn the afternoon Bernard Li gave an introduction to the cluster monitoring tool Ganglia. Directly after that Lindsay Holmwood gave an overview of the monitoring and notification tools flapjack and cucumber-nagios.\nThe evening was filled with the speakers dinner. Thanks for the organisers for providing that. We had a really nice evening together with some of the organisers, Andrew Tanenbaum and Elena Reshetova at our table.\n"},{"id":335,"href":"/fosdem-visitor-seems-to-like-my-car190/","title":"FOSDEM visitor seems to like my baby","section":"Inductive Bias","content":" FOSDEM visitor seems to like my baby # \u0026lt;img src=\u0026ldquo; http://img.mobypicture.com/72f4dd04dc8f74e0bf9203189744f9ed_view.jpg\" alt=\u0026ldquo;Posted using Mobypicture.com\u0026rdquo; /\u0026gt;\nAnother picture that was taken before the first session early in the morning:\n"},{"id":336,"href":"/fosdem-2010-part-1185/","title":"FOSDEM 2010 - part 1","section":"Inductive Bias","content":" FOSDEM 2010 - part 1 # Four years ago I was working in Saarbrücken. From there it is a very short ride over to FOSDEM (little more than 300km). So I decided - hey, why not stay there for a weekend. I found a very nice Brussels bed and breakfast hotel called Rovignon - featuring not only comfortable rooms at reasonable prizes but also cats in the house.\nBack then, I barely knew anyone at the conference. However the lineup of speakers including St Peter from XMPP and Georg Greve from FSFE was impressive.\nAs a result it became a loved tradition of Thilo and myself to drive over to Brussels, attend FOSDEM and watch great talks. Over time there were more and more familiar faces, e.g. at the FSFE booth, among the Debian people\u0026hellip;\nLast weekend I had an awesome time in Brussels at FOSDEM for the fourth time in a row. I am honoured to have been invited by the FOSDEM organisers for a main track talk on Hadoop in the scalability slot (in Janson\u0026hellip;). We arrived on Friday afternoon, however being awefully tired we unfortunately could not join the Friday evening beer event (though, as I am not drinking beer, I would probably have missed quite a bit of the fun).\n"},{"id":337,"href":"/fosdem-2010-10-years-fosdem184/","title":"FOSDEM 2010 - 10 years FOSDEM","section":"Inductive Bias","content":" FOSDEM 2010 - 10 years FOSDEM # The final schedule of FOSDEM 2010 is up: Looks like bad news - 306 interesting talks within just one weekend. Lots of interesting talks in the main track including Greg Kroah-Hartman on \u0026ldquo;Write and Submit your first Linux kernel Patch\u0026rdquo;, David Recordon from Facebook on \u0026ldquo;Scaling Facebook with OpenSource tools\u0026rdquo;, Bernard Li on \u0026ldquo;Ganglia: 10 years of monitoring clusters and grids\u0026rdquo;, Andrew Tanenbaum with his \u0026ldquo;MINIX 3: a Modular, Self-Healing POSIX-compatible Operating System\u0026rdquo; talk, Benoît Chesneau on \u0026ldquo;CouchDB! REST and Database!\u0026rdquo; and many, many more.\nIn addition there will be many interesting DevRooms, including one on NoSQL, one on Free Java, the Mono DevRoom featuring a talk by Miguel de Icaza\u0026hellip;\nLooks like a weekend packed with interesting talks and discussions. If you are going there and are interested in an ad-hoc Hadoop-Beer-drinking meetup, make sure to contact me before the event.\n"},{"id":338,"href":"/hadoop-trainings-in-europe224/","title":"Hadoop trainings in Europe","section":"Inductive Bias","content":" Hadoop trainings in Europe # Recently I received this mail from Christophe Bisciglia on Cloudera Hadoop trainings. Thought it might be interesting to the Hadoop Berlin community: Hadoop Fans,\nOver the next year, you\u0026rsquo;ll see new options for Hadoop training and\ncertification from Cloudera. One of the first things you\u0026rsquo;ll see will\nbe live sessions outside the US, tentatively planned for the April /\nMay time frame.\nWe\u0026rsquo;ve seen strong interest in Hadoop on all of our international\ntrips, so we\u0026rsquo;d like to ask for community input as we decide exactly\nwhich cities to visit next. For cities we come to, we\u0026rsquo;ll offer our 3\nday developer training + certification, and with sufficient interest,\nwe may also include a 1 day training + certification program for\nsystem administrators.\nIf you are interested in attending one or both of these sessions,\nplease fill out a brief survey (link below). If you\u0026rsquo;re using Hadoop at\nwork, and it\u0026rsquo;s time to train more of your team, you can let us know\nhow large of a group you have. Survey responses aren\u0026rsquo;t a commitment to\nattend, but we may reach out to respondents before we schedule a\nsession to get a better understanding of actual attendance.\nYou can fill out survey here: http://www.surveymonkey.com/s/MKGZHG9\nIf you have any trouble with the survey, or are interested in a\nprivate training session, please don\u0026rsquo;t hesitate to reach out directly.\nCheers,\nChristophe\n"},{"id":339,"href":"/shopping-at-ikea370/","title":"Shopping at Ikea","section":"Inductive Bias","content":" Shopping at Ikea # Some weeks ago, Thilo had a tiny little gadget not to be missed in an average geek\u0026rsquo;s appartment: A server - admittedly a little old and a bit slow, but still usable for playing around. He installed Ubuntu server on it. At the evening we got it configured to run Hadoop. Little later we found out that some friends of us probably, maybe have some usable hardware left as well - we\u0026rsquo;ll see on Monday.\nHowever having a server on your dinner table is not really practical: There\u0026rsquo;s always some danger of spilling tea over it\u0026hellip; However last week, one of my colleagues posted a link to the Lack Rack wiki page in the eth-0 Wiki on one of our mailing lists. So yesterday was one of the (very rare) days, when I got Thilo to join me on a trip to Ikea. The result can be seen in the images above. Looks like elephants invaded our living room ;)\n"},{"id":340,"href":"/hadoop-at-heise-ct221/","title":"Hadoop at Heise c't","section":"Inductive Bias","content":" Hadoop at Heise c\u0026rsquo;t # \u0026lt;surreptitious_advertising\u0026gt;\nInteresting for those readers speaking German: Heise published an introductory article on Hadoop in its latest issue. Have fun reading.\n\u0026lt;surreptitious_advertising/\u0026gt;\nThanks to Simon for proof-reading and providing valuable input. Thanks to Thilo Fromm for the hadoop graphics (unfortunately none of them got published in its original form), the catchy title, proof-reading the text over and over again and for keeping me sane during several past and coming months.\nIf you want to know more on Apache Hadoop, come watch my FOSDEM Hadoop talk next weekend. If you want to join discussions on Apache Hadoop and Lucene, stay tuned for a conference in Berlin on these topics.\n"},{"id":341,"href":"/march-2010-apache-hadoop-get-together-berlin284/","title":"March 2010 Apache Hadoop Get Together Berlin","section":"Inductive Bias","content":" March 2010 Apache Hadoop Get Together Berlin # This is to announce the next Apache Hadoop Get Together that will take place in newthinking store in Berlin.\nWhen: March 10th, 4p.m.\nWhere: Newthinking store Berlin\nAs always there will be slots of 20min each for talks on your Hadoop topic. After each talk there will be a lot time to discuss. You can order drinks directly at the bar in the newthinking store. If you like, you can order pizza. We will go to Cafe Aufsturz after the event for some beer and something to eat.\nView Larger Map\nTalks scheduled so far:\nChris Male (JTeam/ Amsterdam): Spatial Search with Solr\nAbstract: The rise in popularity of Google Maps and mobile devices with GPS have resulted in a trend in the search field. People are no longer content with finding results that match a text query, they also want to find results which are near a location. So called spatial search differs considerably from traditional free text search in that it cannot be achieved through common search techniques such as inverted indexes. Instead, new algorithms and data structures had to be developed that achieve efficient and accurate spatial search, that also allow spatial search to have a role in the determination of a result\u0026rsquo;s relevance. This technology has primarily been found in proprietary closed source search applications, however in the last 12-18 months, considerable effort has been invested into bringing open source spatial search support to Apache Solr and Lucene. While much is still left to be done, this talk will introduce how spatial search is currently supported in Solr, what work is happening currently, and a roadmap for future developments.\nDragan Milosevic (zanox/ Berlin: Product Search and Reporting powered by Hadoop\nAbstract:\nTo efficiently process and index 80 million products, as well as store and analyse 30 million clicks and 500 million views daily, Zanox AG is using Hadoop HDFS and Map?Reduce technologies. This talk will present product-processing and reporting frameworks running on 17 node Hadoop cluster, being able to (1) robustly store products and tracking data in distributed manner, (2) rapidly consolidate, normalise and categorise products, (3) merge and aggregate tracking data and (4) efficiently builds indexes for supporting distributed search and reporting, running in several search clusters.\nBob Schulze (eCircle/ Munich): Database and Table Design Tips with HBase\nAbstract: Recurring design patterns for the BigTable/HBase storage model.\nA big Thanks goes to the newthinking store for providing a room in the center of Berlin for us. Another big thanks goes to Nokia Gate 5 for sponsoring videos of the talks. Links to the videos will be posted here.\nPlease do indicate on the following Upcoming event if you are planning to attend to make planning (and booking tables at Aufsturz) easier. Registration through Xing is possible as well.\nLooking forward to seeing you in Berlin,\nIsabel\n"},{"id":342,"href":"/the-7-deadly-sins-of-java-software-developers407/","title":"The 7 deadly sins of (Java) software developers","section":"Inductive Bias","content":" The 7 deadly sins of (Java) software developers # On Lucid Imaginations Blog Jay Hill published a great article on The seven deadly sins of solr. Basically it is a collection of his experiences \u0026ldquo;analyzing and evaluating a great many instances of Solr implementations, running in some of the largest Fortune 500 companies\u0026rdquo;. It is a collection of common mistakes, mis-configurations and pitfalls in Solr installations in production use.\nI loved the article very much. However, many of the symptoms that Jay described in his article do not apply to Solr installations only. In the following I will try to come up with a more general classification of errors that occur when your average Java developer starts using a sufficiently large framework that is supposed to make his work easier. Happy about any input on your favourite production issues.\nRemark: What is printed in italic is quoted as is.\nSin number 1: Sloth - I\u0026rsquo;ll do it later\nLet’s define sloth as laziness or indifference. This one bites most of us at some time or another. We just can’t resist the impulse to take a shortcut, or we simply refuse to acknowledge the amount of effort required to do a task properly. Ultimately we wind up paying the price, usually with interest. There is even a name for it in Scrum: Technical debt. It may be ok to take a shortcut, given this is done based on an informed decision. As with regular debt, you may get a big advantage like launching way earlier than your competitor. However as with real debt, it does come at a prize.\nLack of commitment\nJay describes the problems that are especially frequent when switching search applications: Humans in general do not like giving up their habits. A nice example described in more detail in a recent Zeit article is what happens each year in December/ January when the first snow falls: It is by no means irregular or not to be expected that it starts snowing in December in Germany. However there will be lots of people who are not prepared for that. They refuse to put on winter tiers in late autumn. They use their car instead of public transport despite warnings in public press. The conclusion of the article was simple: People are simply not willing to change habits they got used to. It does take longer and is a bit less flexible to get to work by public transport instead of your own car. It does require adjusting your daily routine, optimising your processes.\nSomething similar happens to a developer that is \u0026ldquo;forced\u0026rdquo; to switch technology, be it the search server, the database, the build system or simply the version control system: The old ways of doing stuff simply may not work as expected. New tools might be called for. New technologies to learn. However in not so seldom cases developers just blame the new tools: \u0026ldquo;But with the old setup this would always work.\u0026quot;\nDeveloping software - probably more than anything else - means constant development, constant change. Technologies shift as tasks shift, tools are improved as workflows change. Developing software means to constantly watch closely what you are doing, reflecting on what works and what doesn\u0026rsquo;t and changing things that don\u0026rsquo;t work. Accepting change, seeing it as a chance rather than an obstacle is critical.\nIf however change is imposed on developers though good arguments in favour of the old approach exist, it may be worth the effort to at least take the technical view into account to make an informed decision.\nNot reviewing, editing, or changing the default configuration files.\nI have extended this one a bit: Developers not changing default configuration files are not that uncommon. Be it the default database configuration, default logging configuration for your modules or default configuration of your servlet container. Even if you are using software pre-packed by your distribution, it is still worth the effort to review configuration files for your services and adjust them to your needs. Usually they are to be used as examples that still need tweaking and customization after roll-out.\nJVM settings and GC\nIf you are running Java application there is no way around to adjust GC settings as well as general JVM settings to your particular use case. There are great tutorials at sun.com that explain both the settings themselves as well as several rules-of-thumb of where to start. Still nothing should stop you from measuring your particular application and its specific needs - both, before and after tuning. Along with that goes the obvious recommendation to simply \u0026ldquo;know-your-tools\u0026rdquo; - learning load testing tools shortly before launch time is certainly no good choice. Trying to find out more on Java memory analysis late in the development cycle just because you need to find that stupid memory leak like now is no good idea neither.\nThere are several nice talks as well as several tutorials available online on the topic of JVM tuning, debugging memory as well as threading issues, one of them being the talk by Rainer Jung at Frocson 2008.\nSin number 2: Greed\nRunning a service on insufficient hardware (be it main memory, harddisks, bandwidth, \u0026hellip;) is not only an issue with Solr installations. There are many cases where just adding hardware may help in the short run, but is a dead-end in the long run:\nGiven a highly inefficient implementation, identifying bottlenecks, profiling, benchmarking and optimization go a long way.\nGiven an inappropriate architecture, redesign, reimplementation and maybe even switching base technologies does help.\nHowever as Jay pointed out, running production servers with less power than your average desktop Mac has does not help neither.\nSin number 3: Pride\nEngineers love to code. Sometimes to the point of wanting to create custom work that may have a solution in place already, just because: a) They believe they can do it better. b) They believe they can learn by going through the process. c) It “would be fun”. This is not meant to discourage new work to help out with an open-source project, to contribute bug fixes, or certainly to improve existing functionality. But be careful not to rush off and start coding before you know what options already exist. Measure twice, cut once.\nDon’t re-invent the wheel.\nAs described in Jay\u0026rsquo;s post, there are developers who seem to be actively searching for reasons to re-invent the wheel. Sure, this is far easier with open source software than with commercial software. Access to code here makes the difference: Understanding, learning from, sharing and improving the software is key to free software.\nHowever there are so many cases where improve does not mean re-implement but submitting patches, fixing bugs, adding new features to the orignal project or just refactoring the original code and ironing out some well known bumbs to make life easier for others.\nEvery now and then a new query abstraction language for map reduce pops up. Some of those really solve distinct problem settings that cannot (and should not) be solved within one language. Especially if a technology is young, this is pretty usual as people try out different approaches to see what works and what does not work out so well. Good and stable things come from that - in general the fittest approach survives. However, too often I have heard developers excusing their re-invention by \u0026ldquo;having had too few time to do a throughough evaluation of existing frameworks and libraries\u0026rdquo;. The irony here really is that usually, coding up your own solution does take time as well. In other cases the excuse was missing support for some of the features needed. How about adding those features, submitting them upstream and benefitting from what is already there and an active community supporting the project, testing it, applying fixes and adding further improvements?\nMake use of the mailing lists and the list archives.\nCommunication is key to success in software development. According to Conway\u0026rsquo;s law \u0026ldquo;Organizations which design systems are constrained to produce systems which are copies of the communication structures of these organizations.\u0026rdquo; I guess it is pretty obvious that developing software today generally means designing complex systems.\nIn Open source, mailing lists (and bug trackers, the code itself, release notes etc.) are all ways for communication. (See also Bertrand\u0026rsquo;s brilliant talk on open source collaboration tools for that). With in-house development there is even added benefit as face-to-face communication or at least teleconferencing is possible. However software developers in general seem to be reluctant to ask questions, to discuss their design, their implementation and their needs for changes. It just seems simpler to work-around a situation that disturbs you instead of propagating the problem to its source - or just asking for the information you need. A nice article on a related topic was published recently it-republik.\nHowever asking these questions, taking part in these discussions is what makes software better. It is what happens regularly within open source projects in terms of design discussions on mailing lists, discussions on focussed issues in the bug tracker as well as in terms of code review.\nThere are several best practices that come with Agile Development that help starting discussions on code. Pair programming is one of these. Code reviews are another example. Having more than two eye balls look at a problem usually makes the solution more robust, gives confidence in what was implemented and as a nice side effect spreads knowledge on the code avoiding a single point of failure with just one developer being familiar with a particular piece of code.\nSin number 4: Lust\nMust have more!You’ll have to grant me artistic license on this one, or else we won’t be able to keep this blog G-rated. So let’s define lust as “an unnatural craving for something to the point of self-indulgence or lunacy”. OK.\nSetting the JVM Heap size too high, not leaving enough RAM for the OS.\nJay describes how setting the JVM RAM allocation too high can lead to Java eating up all memory and leaving nothing for the OS. The observation does not apply to Solr deployments only. Tomcat is just yet another application where this applies as well. Especially with IO-bound applications giving too much memory to the JVM is grave as the OS does not longer have enough space for disk caches.\nThe general take-away probably should be to measure and tune according to the real observed behaviour of your application. A second take-home message would be to understand your system - not only the Java part of it, but the whole machine from Java, the OS down to the hardware - to tune it effectively. However that should be a well known fact anyway. For Java developers, it sometimes helps to simply talk to your operations guys to get the bigger picture.\nToo much attention on the JVM and garbage collection.\nThere are actually two aspects here: For one, as described by Jay it should not be necessary to try every arcane JVM or GC setting unless you are a JVM expert. More precisely, simply trying various options w/o understanding, what they mean, what side-effects they have and in which situations they help obviously isn\u0026rsquo;t a very good idea.\nThe second aspect would be developers starting with JVM optimization only to learn later on that the real problem is within their own application. Tuning JVM parameters really should be one of the last steps in your optimization pipeline. First should be benchmarking and profiling your own code. At the same stage you should review configuration parameters of your application (size of thread pools, connection pools etc.) as well your libraries and frameworks (here come solr\u0026rsquo;s configuration files, Tomcat\u0026rsquo;s configuration, RDBMs configuration parameters, cache configurations\u0026hellip;). Last but not least should be JVM tuning - starting with adjusting memory to a reasonable amount, setting the GC configuration that makes most sense to your application.\nSin number 5: Envy\nBah!\nWanting features that other sites have, that you really don’t need.\nIt should be good engineering practice to start with your business needs and distill user stories from that and identify the technology that solves your problem. Don\u0026rsquo;t go from problem to solution without first having understood your problem. Or even worse: Don\u0026rsquo;t go from solution (that is from a technology you would love to use) to searching for a problem that this solution might solve: \u0026ldquo;But there must be a RDBMS somewhere in our architecture, right?\u0026quot;\nWanting to have a bigger index than the other guy.\nThe antithesis of the “greed” issue of not allocating enough resources. “Shooting for the moon” and trying to allow for possible growth over the next 20 years. Another scenario would be to never fix your system but leave every piece open and configurable, in the end leading to a system that is harder to configure than sendmail is. Yet another scenario would be to plan for billions of users before even launching: That may make sense for a new Google gadget, however for the \u0026ldquo;new kid on the block\u0026rdquo;? Probably unlikely, unless you have really good marketing guys. Plan for what is reasonable in your project, observe real traffic and identify real bottlenecks once you see them. Usually estimations of what bottlenecks could be are just plain wrong unless you have lot\u0026rsquo;s of experience with the type of application you are building. As Jeff Dean pointed out in his WSDM 2009 keynote, the right design for X users may still be right with 10x the amount of users. But do plan a rewrite at about the time you start having 100x and more the amount of users. Sin number 6: Gluttony\n“Staying fit and trim” is usually good practice when designing and running Solr applications. A lot of these issues cross over into the “Sloth” category, and are generally cases where the extra effort to keep your configuration and data efficiently managed is not considered important.\nLack of attention to field configuration in the schema.\nStoring fields that will never be retrieved. Indexing fields that will never be searched. Storing term vectors, positions and offsets when they will never be used. Unnecessary bloat. Understand your data and your users and design your schema and fields accordingly.\nOn a more general scale that might be wrapped into the general advise of keeping only data that is really needed: Rotate logs on a schedule fit to your business, operations needs and based on available machines. Rotate data written into your database backend: It may make sense to keep users that did not interact with your application for 10 years. If you have a large datacenter for storage that may make even more sense. However usually keeping inactive users in your records simply eats up space.\nUnexamined queries that are redundant or inefficient.\nQueries that catch too much information, are redundant or multiple queries that could be folded into one are not only a problem for Solr users. Anyone using data sources that are expensive to query probably knows how to optimize those queries for reduced cost.\nSin number 7: Wrath\nNow! While wrath is usually considered to be synonymous with anger, let’s use an older definition here: “a vehement denial of the truth, both to others and in the form of self-denial, impatience.”\nAssuming you will never need to re-index your data.\nHmm - don\u0026rsquo;t only backup. Include recovery in your plans! Admittedly with search applications, this includes keeping the original documents - it is not unusual to add more fields or to want to parse data differently from the first indexing run. Same applies if you are post-processing data that has been entered by users or spidered from the web for tasks like information extraction, classifier training etc.\nRushing to production.\nOf course we all have deadlines, but you only get one chance to make a first impression. Years ago I was part of a project where we released our search application prematurely (ahead of schedule) because the business decided it was better to have something in place rather than not have a search option. We developers felt that, with another four weeks of work we could deliver a fully-ready system that would be an excellent search application. But we rushed to production with some major flaws. Customers of ours were furious when they searched for their products and couldn’t find them. We developed a bad reputation, angered some business partners, and lost money just because it was deemed necessary to have a search application up and running four weeks early.\nLeaving that as is - just adding, this does not apply to search applications only ;)\nSo keep it simple and separate, stay smart, stay up to date, and keep your application on the straight-and-narrow (YAGNI ;) ). Seek (intelligently) and ye shall find.\n"},{"id":343,"href":"/apache-dinner-january-201034/","title":"Apache Dinner January 2010","section":"Inductive Bias","content":" Apache Dinner January 2010 # This evening in X-Berg several local committers met for the second \u0026ldquo;Apache Dinner\u0026rdquo; - an informal gathering of local Apache committers, friends and associates for food, beer and interesting discussions. Next one is probably to be scheduled some time in February. Feel free to send a message to Torsten Curdt to be included on the next invitation mail. Thanks for organizing a nice evening, Torsten. Hope to see even more Apache friends at the next dinner ;)\n"},{"id":344,"href":"/mahout-in-action275/","title":"Mahout in Action","section":"Inductive Bias","content":" Mahout in Action # As noted earlier by Grant Ingersoll, the first chapters of Mahout in Action are already online at Manning:\nSean, Robin, keep up the great work! I would love to read more of the book in the near future.\n"},{"id":345,"href":"/how-much-of-scrum-is-implemented235/","title":"How much of Scrum is implemented?","section":"Inductive Bias","content":" How much of Scrum is implemented? # I have started using Scrum for various purposes: It has inspired the way software is developed at my current employer. I use it to organize a students\u0026rsquo; project at university. In addition we are using it at home to get all personal tasks (preparing breakfast, doing the laundry, meeting with friends\u0026hellip;) in line for each week.\nConstantly looking for ways to evaluate, refine and improve work - I am also looking for ideas on how to evaluate which aspects of the Scrum implementation can actually be improved. One pretty common way to do this evaluation is to do the so-called \u0026ldquo;Nokia-Test\u0026rdquo;. A set of questions on the project management that gives a possibility to judge your implementation of Scrum. As an example lets just have a closer look at our \u0026ldquo;Scrum Housework\u0026rdquo; implementation.\nQuestion 1 - Iterations No iterations - 0\nInterations \u003e 6 weeks - 1 Variable length \u003c 6 weeks - 2 Fixed iteration length 6 weeks - 3 Fixed iteration length 5 weeks - 4 Fixed iteration 4 weeks or less - 10 Currently we are doing one week iterations - planning ahead for longer just seems impossible, except for events like going to conferences or regular birthdays. So that would be 10 points for iterations.\nQuestion 2 - Testing within the Sprint No dedicated QA - 0 Unit tested - 1 Feature tested - 5 Features tested as soon as completed - 7 Software passes acceptance testing - 8 Software is deployed - 10 Hmm. Admittedly there is no real testing in place except for smoke testing for stuff like emptying the dish washer.\nQuestion 3 - Agile Specification No requirements - 0 Big requirements documents - 1 Poor user stories - 4 Good requirements - 5 Good user stories - 7 Just enough, just in time specifications - 8 Good user stories tied to specifications as needed - 10 We do not have big documents describing how to setup the christmas tree. But at the beginning of each sprint there is a set of user stories, if needed with acceptance criteria specified. So something like \"Tidy up computer desk\" would be augmented by the information: \"To the extend that there are no items except for the laptop on the desk afterwards and the desk was dusted\". That might probably make a 10.\nQuestion 4 - Product Owner No Product Owner - 0 Product Owner who doesn’t understand Scrum - 1 Product Owner who disrupts team - 2 Product Owner not involved with team - 2 Product owner with clear product backlog estimated by team before Sprint Planning meeting (READY) - 5 Product owner with release roadmap with dates based on team velocity - 8 Product owner who motivates team - 10 We are both familiar with Scrum. However, due to the nature of the tasks and due to lack of people in the loop we are exchanging the role of the product owner regularly. We are still missing a product backlog - currently it is loosely defined as a pile of post-it notes with estimates put beside each item that define its complexity. So I would give some 3 points on that one.\nQuestion 5 - Product Backlog No Product Backlog - 0 Multiple Product Backlogs - 1 Single Product Backlog - 3 Product Backlog clearly specified and prioritized by ROI before Sprint Planning (READY) - 5 Product Owner has release burndown with release date based on velocity - 7 Product Owner can measure ROI based on real revenue, cost per story point, or other metrics - 10 We only have one product backlog though it is very informal. So that would make 2 points.\nQuestion 6 - Estimates Product Backlog not estimated - 0 Estimates not produced by team - 1 Estimates not produced by planning poker - 5 Estimates produced by planning poker by team - 8 Estimate error \u003c 10% - 10 Naturally those doing the tasks are those producing the estimates. Thanks to Agile42 in Berlin we now even have a set of planning poker cards: Yeah! That makes 8 points. Just as an example: Getting returnable bottles back to the shop makes for 8 complexity points, going to cinema are 16 just like storing the christmas decoration back in it's boxes, preparing breakfast is just about 3 points ;)\nQuestion 7 - Sprint Burndown Chart No burndown chart - 0 Burndown chart not updated by team - 1 Burndown chart in hours/days not accounting for work in progress (partial tasks burn down) - 2 Burndown chart only burns down when task in done (TrackDone pattern) - 4 Burndown only burns down when story is done - 5 Add 3 points if team knows velocity Add two point if Product Owner release plan based on known velocity Our we do have whiteboard with post-it notes on them that are checked out and moved to done as soon as they are done - there is no arguing about the laundry being done before it is cleaned, dried, ironed and back to the closet. ;) So that would make for 5 points. In addition we know our velocity, which would make another 3 points:\nNaturally we are pretty capable of telling what can reasonably be expected to be done within the coming sprints. That might add another 2 points for the release being based on that velocity.\nQuestion 8 - Team Disruption Manager or Project Leader disrupts team - 0 Product Owner disrupts team - 1 Managers, Project Leaders or Team leaders telling people what to do - 3 Have Project Leader and Scrum roles - 5 No one disrupting team, only Scrum roles - 10 There are events and people interrupting running sprints: Say, NoSQL meetups that are planned spontaneously or new articles that get written and printed within less than a week. But usually these events are rather seldom and are kept to a minimum due to the short sprint length. So that might make for 3 points.\nQuestion 9 - Team Tasks assigned to individuals during Sprint Planning – 0 Team members do not have any overlap in their area of expertise – 0 No emergent leadership - one or more team members designated as a directive authority -1 Team does not have the necessary competency - 2 Team commits collectively to Sprint goal and backlog - 7 Team members collectively fight impediments during the sprint - 9 Team is in hyperproductive state - 10\nCurrently we are in a state where we have identified and started emerging impediments - declining tasks that cannot reasonably be done within the given timeframe, getting a real product backlog up, tracking even minor tasks like writing e-mails to organize the Apache Hadoop Get Together. So that makes for 9 points.\nIn total that makes for 54 points (excuse for computing incorrectly: It is 23:34, I am a little tired but cannot sleep due to caffeine). How does your team score on the Nokia test? "},{"id":346,"href":"/third-december-hadoop-get-together-video-online408/","title":"Third \"December Hadoop Get Together\" video online","section":"Inductive Bias","content":" Third \u0026ldquo;December Hadoop Get Together\u0026rdquo; video online # In the following video taken at the last Hadoop Get Together in Berlin Jörg Möllenkamp explains why Hadoop is interesting for Sun - and why Sun Hardware might be a good fit for Hadoop applications:\nHadoop Jörg Möllenkamp from Isabel Drost on Vimeo.\nIn a blog post published after the event, Jörg gives more details on his idea of Parasitic Hadoop he introduced at the meetup.\n"},{"id":347,"href":"/second-december-hadoop-get-together-video361/","title":"Second December Hadoop Get Together video","section":"Inductive Bias","content":" Second December Hadoop Get Together video # Richard Hutton from nugg.ad explained how they scaled their ad recommendation system to an increasing number of users with the help of Hadoop. To learn more on their use case and details on which problems they solved with Hadoop, watch the video below:\nHadoop Richard Hutton from Isabel Drost on Vimeo.\n"},{"id":348,"href":"/with-a-little-help-from-my-friends435/","title":"With a little help from my friends","section":"Inductive Bias","content":" With a little help from my friends # The end of the year 2009 is quickly approaching. To me it feels a little like it ran away far too quickly. So instead of taking part in the annual review of past events, I would like to use it as an opportunity to say thank you: The past twelve months were a lot of fun with lots of interesting, nice people from all over the world. I got the chance to meet quite a bit of the Mahout community, I got lots and lots of new developers from all over Germany - or more precisely the EU - to attend the Apache Hadoop Get Together in Berlin. The interest in Mahout has grown tremendously over the past year.\nAll of this would not have been possible without the help of many people: First of all I\u0026rsquo;d like to thank Thilo Fromm - for making me happy whenever I was disappointed, for solacing me when I when I was sad, for patiently listening to me nervously whining before each and every talk, for kindly reviewing my slides and last but not least for helping me fix some of the problems that bugged me. Oh - and, thanks for helping me fix the issue in the zookeeper c-client within minutes that puzzled me for days. Another big Thanks goes to family, first and foremost my mum, who kindly took care of organizing quite a bit of my paperwork and kept me on schedule with so many \u0026ldquo;unimportant\u0026rdquo; tasks like getting an appointment with some hospital to finally get the screws taken out of my knee ;)\nA special thanks goes to the growing Mahout community as well as to the Lucene people - you know, who you are\nkeep up the great work: You rock! Furthermore there are students at TU Berlin who have shown that with Mahout it is \u0026ldquo;dead-simple\u0026rdquo; to write an application that, given a stream of documents, groups them by topic and makes the result searchable in Solr. Thanks to you for solving the minor and major problems, for communicating with the community, for transparently communicating problems. Looking forward to continue working together with you next year.\nFinally a big thank you to all of the speakers, sponsors and attendees of the Apache Hadopp Get Together, the NoSQL conference and the Apache Dinner Berlin - without you these events would never have been possible. Looking forward to seeing you again in January/ March 2010!\nI hope I didn\u0026rsquo;t forget too many people - just in case: I am pretty grateful for all the input, help and feedback I got this year.\nPS: Another thanks to the spaceboyz visiting Berlin for 26C3 for helping Thilo tidy up our apartment after Congress was over this year ;) "},{"id":349,"href":"/first-december-apache-hadoop-berlin-video-online175/","title":"First December Apache Hadoop Berlin video online","section":"Inductive Bias","content":" First December Apache Hadoop Berlin video online # The video of Nikolaus\u0026rsquo; Pohle\u0026rsquo;s talk at the December Apache Hadoop Get Together Berlin is online already - more to come soon.\nhadoop nikolaus pohle from Isabel Drost on Vimeo.\nThanks to Martin from newthinking for video taping and uploading. Thanks to StudiVZ for sponsoring the video.\n"},{"id":350,"href":"/screws-are-out338/","title":"Screws are out","section":"Inductive Bias","content":" Screws are out # Before:Some time in between:After:\nOn December 22nd those screws got taken out of my knee: Early in the morning (early as in arrive at 6:45am) I was to be at the hospital. In return I was allowed to go home the same day in the afternoon: Finally some time for reading and refining MAHOUT-85 ;) "},{"id":351,"href":"/winter-arrived-at-berlin434/","title":"Winter arrived at Berlin","section":"Inductive Bias","content":" Winter arrived at Berlin # Finally winter seems to have arrived at Berlin as well:\nLooks a little like Christmas is drawing closer. Only disadvantage of the weather: One of the breaks of my bike was frozen after very few minutes. Luckily for me, my bike has one of those old-fashioned back pedal brakes ;)\n"},{"id":352,"href":"/summary-december-get-together386/","title":"Summary - December Get Together","section":"Inductive Bias","content":" Summary - December Get Together # Today the seventh Apache Hadoop Get Together took place in Berlin. The room was again packed with more than 40 people from various companies with and without practical experience with Hadoop: There were people from Nokia Gate 5, Sun, nurago, StudiVZ, Dawanda, Last.fm, nugg.ad. There were people from academia, e.g. HPI Potsdam. And a few Freelancers interested in the topic or providing help with Hadoop. We had three very interesting talks. The first one was given by Richard Hutton from nugg.ad on their usage of Hadoop. They provide targeted advertisement services to their clients. Naturally they do need to process lots of user interactions to be able to draw reliable conclusions. nugg.ad started out with a traditional system setup: Erlang loggers in front, data got fed to well known data warehouse infrastructures, analysed and results pushed back to the frontends. However this architecture would scale only so far. So in the beginning of 2009 they started migrating their systems over to Hadoop. (A Thanks from the speaker to Tom White for publishing the Hadoop book at O\u0026rsquo;Reilly that obviously helped the developers a lot.). Today, nugg.ad is down from one to two days for analysis to one to two hours. I will link the slides of the talk as soon as I have the pdf version available.\nSecond talk was given by Jörg Möllenkamp on what Sun is doing with Hadoop. Sun does have \u0026ldquo;special hardware\u0026rdquo; - special in that the have systems with up to 512 virtual processors on one chip. With Solaris they do have an operating system that scales to that architecture. But now they are looking for applications that can use such hardware efficiently as well. Hadoop is well suited for distributing computations - so it looked like a great fit for Sun. Slides are available online.\nThe last talk was given by Nikolaus Pohle from nurago. They switched to Hadoop only recently. Coming from online market analysis, they have to analyse lots of user interaction data. Currently they are moving away from a MySQL based architecture to a distributed system based on HDFS and Map/Reduce. In order to ease writing M/R jobs for their employees they built their own abstract language on top of Hadoop that helps formulating recurring jobs. That does sound a lot like what PIG or Cascading already does - but is specially targeted at the type of jobs they have. Slides are available online. There is also a pdf version for users who prefer open formats.\nIf anyone should be interested in it, I also put my introductory slides online.\nNext meetup will be in March 2010. It will feature a talk by Zanox on their Hadoop usage, one talk by eCircle from Munich as well as one talk by Nokia. You are very welcome to join us. If you would like to give a presentation yourself - please do contact me. If you would like to sponsor the event, please send me an e-mail.\nA big Thank You to all the speakers - Nikolaus Pohle from nurago, Jörg Möllenkamp from Sun and Richard Hutton from nugg.ad - without you, the event would not be possible. Another big Thank You to newthinking for providing the venue for free. And, last but not least, another big Thank You to StudiVZ for sponsoring the videos. They will be linked to from here as well as from the StudiVZ blog as soon as they are available.\n"},{"id":353,"href":"/on-thursday-open-hadoop-user-group-munich310/","title":"On Thursday: Open Hadoop User Group Munich","section":"Inductive Bias","content":" On Thursday: Open Hadoop User Group Munich # If one evening of Apache Hadoop is not enough for you: The next Christmas Meetup in Germany takes place one day later in Munich.\n\u0026lt;li\nWhen: Thursday December 17, 2009 at 5:30pm open end\nWhere: eCircle AG, Nymphenburger Straße 86, 80636 München (\u0026ldquo;Bruckmann\u0026rdquo; Building, \u0026ldquo;U1 Mailinger Str\u0026rdquo;, map in German http://www.ecircle.com/de/kontakt/anfahrt.html and look for the signs)\nTalks scheduled by Bob and Lars:\nBob Schulze from eCircle will be giving the first presentation on how eCircle is planning to use the Hadoop stack.\nDave Butlerdi will be giving an overview of his usage of Hadoop.\nLars George will give a state of affairs of the HBase project. What is it, what does it do and how he is using it (since early 2008).\nThere is a quick connect via train from Berlin to Munich. So if you are attending the Berlin Get Together, it is very easy to travel south to Munich one day later and visit the Munich event as well.\n"},{"id":354,"href":"/december-apache-hadoop-berlin311/","title":"On Wednesday: December Apache Hadoop @ Berlin","section":"Inductive Bias","content":" On Wednesday: December Apache Hadoop @ Berlin # This week on Wednesday at 5p.m. the December Hadoop Get Together takes place in newthinking store Berlin.\nTalks scheduled so far:\nRichard Hutton (nugg.ad): “Moving from five days to one hour.” Jörg Möllenkamp (Sun): “Hadoop on Sun” Nikolaus Pohle (nurago): “M/R for MR - Online Market Research powered by Apache Hadoop. Enable consultants to analyze online behavior for audience segmentation, advertising effects and usage patterns.”\nThere will be videos after the event linked to by StudiVZ (thanks for sponsoring) after the Meetup is over.\nAs this is the last Meetup before Christmas there will be cookies waiting for you.\nIf you want to get notifications of future events on Apache Hadoop, NoSQL, Apache Lucene - be it trainings, meetups or conferences - feel free to subscribe to the Mailinglist or join the Xing Group that accompanies the Berlin Get Together.\n"},{"id":355,"href":"/photos-the-traditional-way325/","title":"Photos the traditional way","section":"Inductive Bias","content":" Photos the traditional way # After one year of taking pictures at various occasions and at various places the pile of photos grew frighteningly large:\nI am not counting the images taken at Apache Con US Oakland - they are not yet developed. All other photos were taken either with an old fashioned Olympus µ-zoom or a Praktica Nova 1st.\nSeveral hours of work later, I ended up with one more book containing memories of an exciting year\u0026hellip;\n"},{"id":356,"href":"/winter-finally2/","title":"\"Schneeflöckchen, Weißröckchen\"","section":"Inductive Bias","content":" \u0026ldquo;Schneeflöckchen, Weißröckchen\u0026rdquo; # First snow seen this morning - seems like finally it\u0026rsquo;s winter:\n"},{"id":357,"href":"/apache-hadoop-at-fosdem-201040/","title":"Apache Hadoop at FOSDEM 2010","section":"Inductive Bias","content":" Apache Hadoop at FOSDEM 2010 # Though the official schedule is not yet online: I will be giving an introductory talk about Apache Hadoop at next year\u0026rsquo;s FOSDEM (Free and Open Source Developer European Meeting) in Brussles. This will be the 10th birthday of the event - looking forward to a fun event, meeting other free and open source software developers from all over Europe.\nIf you are a Apache Hadoop developer and would like me to include some particular topic in the talk - please feel free to contact me. If you are an Apache Hadoop user and would like to learn more on the project, please come to the talk and ask questions. If you are an Apache Hadoop Newbie - feel free to join us.\nIn addition there will be a NoSQL Dev Room at FOSDEM as well. The call for presentations is up already. So if you are doing fun stuff with CouchDB, HBase and friends or are a developer of these projects - submit a talk and join us in early-February in Brussles.\n"},{"id":358,"href":"/reminder-apache-hadoop-get-together-next-week333/","title":"Reminder: Apache Hadoop Get Together next week","section":"Inductive Bias","content":" Reminder: Apache Hadoop Get Together next week # Just a tiny little reminder: The Apache Hadoop Get Together Berlin is scheduled to take place next week on Wednesday.\nWhen: 16th of December 8PM\nWhere: newthinking store Tucholskystr. 48, Berlin Mitte\nKindly sponsored by: newthinking store (location) and StudiVZ (videos).\nPlease regist er (or use Xing for registration) so planning becomes a bit easier.\nTalks scheduled: Richard Hutton (nugg.ad): \u0026ldquo;Moving from five days to one hour.\u0026quot;\nJörg Möllenkamp (Sun): \u0026ldquo;Hadoop on Sun\u0026rdquo;\nNikolaus Pohle (nurago): \u0026ldquo;M/R for MR - Online Market Research powered by Apache Hadoop. Enable consultants to analyze online behavior for audience segmentation, advertising effects and usage patterns.\u0026quot;\nLooking forward to seeing you in Berlin next week.\n"},{"id":359,"href":"/berlin-hospital-chaos-charite121/","title":"Berlin hospital chaos (Charite)","section":"Inductive Bias","content":" Berlin hospital chaos (Charite) # What getting screws out of your knee looks like at Charite: Got sent to \u0026ldquo;Virchow Klinikum\u0026rdquo; by my doctor back home: I got some recommendations for it from my colleagues and is easy to reach for me. Called up there earlier this week and got an appointment for today at 8:30 a.m. (was already wondering, that this was possible so quickly). Arrived this morning: But seems like I was out of luck - no one at the hospital knew about the appointment. Obviously it didn\u0026rsquo;t get tracked in any of their systems. Seeing that I was back at Charite Mitte one year ago, they were like \u0026ldquo;but those screws need to be taken out by the doc who put them in\u0026rdquo; - WTF? Really great way to start a day - about two hours of time wasted.\n"},{"id":360,"href":"/sesat-moving-from-fast-to-solr369/","title":"Sesat - moving from FAST to Solr","section":"Inductive Bias","content":" Sesat - moving from FAST to Solr # The article was first published on Sesat Weblog. As the whole site went down several days ago, I retrieved the content from the Google cache and posted it here for perservation.\nUpdate: Original site is up again.\n"},{"id":361,"href":"/scrumtisch-berlin-november358/","title":"ScrumTisch Berlin - November","section":"Inductive Bias","content":" ScrumTisch Berlin - November # This evening Marion organised another successful Scrumtisch: Usually we either meet for timeboxed discussions on Scrum and agile development questions or a speaker is invited to present his favourite Scrum or Lean or Agile topic.\nToday Markus Frick gave a presentation of how Scrum was introduced at SAP, a German software company. They started implementing Scrum in a \u0026ldquo;small\u0026rdquo; team of about 60 people, organised in about six to seven teams. The idea was to get people together who are already sort of familiar with agile technologies and let them evaluate what works in the companies context at what doesn\u0026rsquo;t. The conversion started with trainings, teams were organised by features as opposed to components. The main goal was to get people to learn Scrum and then spread the idea across the whole company.\nSoon upper management were fascinated by the methodology - not shortly after that the goal was reset to converting 2500 employees working at four different locations (Bangalore, Berlin, Waldorf and Sofia/France) on diverse topics ranging from developers to managers to agile methods. The question thus turned from scaling Scrum to quickly scaling the conversion process: Where do we get enough trainers? Where does Scrum expertise come from? How should communication be organised? How do we adapt our sales and governance processes?\nThe way to do this chosen back than was to use Scrum itself for the conversion process. That is to introduce teams for training and conversion and let them work according to a backlog. Also managers were set up to participate by organising work according to a backlog containing management tasks. This first let to quite some confusion: Conversion does take time and working according to sprint backlogs makes it pretty much obvious how much time it actually takes and how much time people really spent on these tasks. On the other hand, the whole process was made very transparent for everyone - and open for participation.\nThe process started about two years ago - it has not finished to date and processes continue to evolve, get improved and refined as people go along. A very rough estimation was that it might take another three years to get to a stable, clean state. However most - if not all - problems were not caused by Scrum. They were made visible and trackable by Scrum.\nThe main take home messages were that Scrum does bring improvement:\nIt makes goals transparent and communicates clearly the current state.\nYou get a short feedback cycle so people actually see where problems are.\nIt inherently allows for reflection and analysis of problems.\nAs introduced here it also made the work of management people transparent by making backlog and velocity of managers accessible by everyone.\nInternal trainings helped to get feedback from teams who are already practicing what is introduced.\nAmong the people who were very skeptical there usually were quite a few people from middle management. Uncertain about how future development should work they usually feared a loss in influence. Most positive feedback came from developers themselves: After explaining what Scrum is all about, that is includes shore release cycles and fast feedback, most developers that were in the teams already for quite some time reacted by stating that this basically resembled development \u0026ldquo;in the good old days\u0026rdquo; with a bit of development process added on top.\nIf you are interested in hearing more stories on how Scrum is or was introduced in companies of various sizes, I would like to recommend visiting the German Scrum Day in Düsseldorf. The talk by Thilo Fromm gives a nice overview of how a transition from traditional Waterfall to Scrum can look like. And agile42 Andrea Tomasini will talk about the Scrum implementation in distributed teams at be2 ltd.\nUpdate: This blog post was re-posted at the Agile42 blog.\n"},{"id":362,"href":"/first-apache-dinner-berlin174/","title":"First Apache Dinner Berlin","section":"Inductive Bias","content":" First Apache Dinner Berlin # A few days ago, I received a mail from Torsten Curdt that read something like: \u0026ldquo;[\u0026hellip;] For a long time now I wanted to organise an Apache Dinner Berlin. What do you think, when would be a good time for that?\u0026rdquo;. As that was about the third time I heard of that idea (and the third person mentioning the idea), I included some Berlin-based Apache-people asking whether they would be interested in having an Apache Dinner on November 24st in X-Berg. General answer: Yes! Sure!\nThe idea was to make it open to anyone interested in the ASF and send invitations to committers who are living in the greater-Berlin-area. Then book a table, have some food, get some drinks\u0026hellip;\nWe met at Graefekiez - we, that is Torsten (Jakarta and Hadoop), Jan and Daniel (CouchDB), Simon+Vera (Lucene), oswald (xampp), Eric (Http Components) and myself - for a great \u0026ldquo;small menu\u0026rdquo; at La Buona Forchetta (Thanks to Torsten for coming up with that restaurant and booking the table). After that some of us moved over to a bar close to the restaurant.\nAfter a long evening with lots of interesting (cross-project as well as non-technical) discussions, the general conclusion was to organize another Apache Dinner some time in January after Christmas-time is over:\nThanks guys for a great evening. Hope to see you all - as well as a few more Apache people from around Berlin - in January. Date and location to be set.\nFinal note to self: No Club Mate for Isabel after 02:00 a.m. \u0026hellip;\n"},{"id":363,"href":"/moving-from-fast-to-solr286/","title":"Moving from Fast to Solr","section":"Inductive Bias","content":" Moving from Fast to Solr # Sesat has published a nice in-depth report on why to move from Fast to Solr. The article also includes a description of the steps taken to move over as well as several statistics:\nhttp://sesat.no/moving-from-fast-to-solr-review.html\nOn a related topic, the following article details, where Apple is using Lucene/Solr to power it\u0026rsquo;s search. Spoiler: Look at Spotlight, their desktop search, as well as on the iTunes search with about 800 QPS.\nUpdate: As the site above could not be reached for quite some time, you should either look into the Google Cache version.\n"},{"id":364,"href":"/apachecon-oakland-roundup88/","title":"ApacheCon Oakland Roundup","section":"Inductive Bias","content":" ApacheCon Oakland Roundup # Two weeks ago ApacheCon US 2009 ended in Oakland California. Shane published a set of links to articles that contain information on what happened at Apache Con. Some of them are officially published by the Apache PRC project, others are write-ups of individuals on which talks they attended and which topics they considered particularly interesting.\n"},{"id":365,"href":"/mahout-02-released273/","title":"Mahout 0.2 released","section":"Inductive Bias","content":" Mahout 0.2 released # Apache Mahout 0.2 has been released and is now available for public download at http://www.apache.org/dyn/closer.cgi/lucene/mahout\nUp to date maven artifacts can be found in the Apache repository at\nhttps://repository.apache.org/cont ent/repositories/releases/org/apache/mahout/\nApache Mahout is a subproject of Apache Lucene with the goal of delivering scalable machine learning algorithm implementations under the Apache license. http://www.apache.org/licenses/LICENSE-2.0\nMahout is a machine learning library meant to scale: Scale in terms of community to support anyone interested in using machine learning. Scale in terms of business by providing the library under a commercially friendly, free software license. Scale in terms of computation to the size of data we manage today.\nBuilt on top of the powerful map/reduce paradigm of the Apache Hadoop project, Mahout lets you solve popular machine learning problem settings like clustering, collaborative filtering and classification\nover Terabytes of data over thousands of computers.\nImplemented with scalability in mind the latest release brings many performance optimizations so that even in a single node setup the library performs well.\nThe complete changelist can be found here:\nhttp://issues.apache.org/jira/browse/MAHOUT/fi xforversion/12313278\nNew Mahout 0.2 features include\nMajor performance enhancements in Collaborative Filtering, Classification and Clustering\nNew: Latent Dirichlet Allocation(LDA) implementation for topic modelling\nNew: Frequent Itemset Mining for mining top-k patterns from a list of transactions\nNew: Decision Forests implementation for Decision Tree classification (In Memory \u0026amp; Partial Data)\nNew: HBase storage support for Naive Bayes model building and classification\nNew: Generation of vectors from Text documents for use with Mahout Algorithms\nPerformance improvements in various Vector implementations\nTons of bug fixes and code cleanup\nGetting started: New to Mahout?\nDownload Mahout at http://www.apache.org/dyn/closer.cgi/lucene/mahout\nCheck out the Quick start: http://cwiki.apache.org/MAHOUT\nRead the Mahout Wiki: http://cwiki.apache.org/MAHOUT\nJoin the community by subscribing to mahout-user@lucene.apache.org\nGive back: http://www.apache.org/foundation/getinvolved.html\nConsider adding yourself to the power by Wiki page: http://cwiki.apache.org/MAHOUT/poweredby.html\nF or more information on Apache Mahout, see http://lucene.apache.org/mahout\nA very BIG Thank You to all those who made this release happen!\n"},{"id":366,"href":"/open-source-expo-09318/","title":"Open Source Expo 09","section":"Inductive Bias","content":" Open Source Expo 09 # I spent last Sunday and the following Monday at Open Source Expo Karlsruhe - co-located with web-tech and php-conference organized by the Software-and-Support Verlag. Together with Simon Willnauer I ran the Lucene/Mahout booth at the expo.\nSo far the conference is still very small (about 400 visitors) compared to free software community events. However the focus was set to be more on professional users, accordingly several projects showed that free software can be used successfully for various business use cases. Visitors were invited to ask Sun about their free software strategy. Questions concerning OpenJDK or MySQL were not uncommon. Large distributors like SuSE or Mandriva were present as well. But also smaller companies e.g. providing support for Apache OfBIZ were present.\nThe Apache Lucene project was invited as exhibitor as well. Together with PRC and ConCom we organized for an Apache banner. Lucid Imagination sponsored several Lucene T-Shirts to be distributed at the conference. At the very last minute information (abstract, links to projects and mailing lists and current users) was put together on flyers.\nWe arrived on Saturday, late evening. Together with a friend of mine we went for some indian food at a really good restaurant close to the hotel. Big thanks to her, for being our tourist guide - hope to see you back in Waldheim in December ;)\nSunday was pretty quiet - only few guests arrived at the weekend. I was invited by David Zuelke to give a brief introduction to Mahout during his MapReduce Hadoop tutorial workshop. Thanks, David. Though lunch was served already, people did stay to hear my presentation on large scale machine learning with Mahout. I got contacted by one of the students of Katarina Morik who was pretty interested in the project. Back at her research group people are working on Rapid Miner - a tool for easy machine learning. It comes with a graphical user interface that makes it simple to explore various algorithm configurations and data workflow setups. It would be interesting to see how this tool helps people to understand machine learning. Would also be very interesting to learn what form of contribution might be interesting and appropriate for research groups to contribute to Mahout. Maybe not code-wise but more in terms of discussions and background knowledge.\nSunday was a bit more busy, with more people attending the conferences. Simon got a slot to present Lucene at the Open Stage track and show off the new features of Lucene 2.9. Those using Lucene already could be tricked into telling their Lucene success-story at the beginning of the talk. At the booth we had a wide variety of people: From students trying to find a crawling and indexing system for their information retrieval course homework up to professionals with various questions on the Apache Lucene project. The experience of people at the conference varied widely. That proved to be a pretty good reality-check. Being part of the Lucene and the ASF community one might be tempted to think that not knowing about Lucene is almost impossible. Well, it seems to be less impossible than at least I expected.\nOne last success: As the picture shows, Yacy now is powered by Lucene as well - at least in terms of T-Shirt ;)\n"},{"id":367,"href":"/apache-con-us-wrap-up24/","title":"Apache Con US Wrap Up","section":"Inductive Bias","content":" Apache Con US Wrap Up # some weeks ago I attended ApacheConUS09 in Oakland/ California. In the mean time, videos of one of the sessions have been published online:\nYou can find a wrap up of the most prominent topics at the conference at heise (unfortunately Germany-only).\nBy far the largest topics at the conference:\nLucene - there was a meetup with over 100 attendees as well as two main tracks with Lucene focussed talks. New features of Lucene 2.9.* were in the center of interest: The new range search capabilities, segment search that improves caching, a new token stream api that makes annotating terms more flexible as well as a lot of performance improvements. Shortly after the conference, Lucene 2.9.1 as well as Solr 1.4 was released so end-users switching to the new version now benefit from better performance and several new features.\nHadoop - large scale data processing currently is one of the biggest topics. Be it logfile analysis, business intelligence or ad-hoc analysis of user data. Hadoop was covered by a user meetup as well as one track on the first conference day. The track started with an introduction by Owen O\u0026rsquo;Malley and Doug Cutting. It continued with talks on HBase, Hive, Pig and other projects from the Hadoop ecosystem.\nBut also projects like Apache Tomcat and Apache HTTPD were well covered within one to two sessions each.\nCurrently a hot topic within the foundation is the challenge of bringing the community together face-to-face. Apache projects have become so numerous that covering them all within 3+2 days of conference and trainings seems no longer feasable. One way to mitigate these problems might be to motivate people to do more local meetups potentially supported by ConCom as has already happened in the Lucene- and Hadoop-communities. A related topic is the task of community building and community growth within the ASF. Google Summer of Code has been a great way to integrate new people. However the model does not scale that well for the foundation. With ComDev a new project was founded with the goal to work on community development issues, talking to research, getting students into open source early on. The project is largely supported by Ross Gardler, who already has experience with teaching and promoting open source and free software in the research context being part of the open source watch project in the UK.\nApache Con US 09 brought together a large community of Apache software developers and users from all over the world who gathered in California, not only for the talks but also for face-to-face communication, coding together and exchanging ideas.\nUpdate: Slides of my Mahout talk are now online.\n"},{"id":368,"href":"/december-apache-hadoop-get-together-berlin148/","title":"December Apache Hadoop Get Together @ Berlin","section":"Inductive Bias","content":" December Apache Hadoop Get Together @ Berlin # As announced at ApacheCon US, the next Apache Hadoop Get Together Berlin is scheduled for December 2009.\nWhen: Wednesday December 16, 2009 at 5:00pm Where: newthinking store, Tucholskystr. 48, Berlin\nAs always there will be slots of 20min each for talks on your Hadoop topic. After each talk there will be a lot time to discuss. You can order drinks directly at the bar in the newthinking store. If you like, you can order pizza. We will go to Cafe Aufsturz after the event for some beer and something to eat.\nTalks scheduled so far:\nRichard Hutton (nugg.ad): \u0026ldquo;Moving from five days to one hour.\u0026rdquo; - This talk explains how we made data processing scalable at nugg.ad. The company\u0026rsquo;s core business is online advertisement targeting. Our servers receive 10,000 requests per second resulting in data of 100GB per day.\nAs the classical data warehouse solution reached its limit, we moved to a framework built on top of Hadoop to make analytics speedy,data mining detailed and all of our lives easier. We will give an overview of our solution involving file system structures, scheduling, messaging and programming languages from the future.\nJörg Möllenkamp (Sun): \u0026ldquo;Hadoop on Sun\u0026rdquo;\nAbstract: Hadoop is a well known technology inside of Sun. This talk want to show some interesting use cases of Hadoop in conjunction with Sun technologies. The first show case wants to demonstrate how Hadoop can used to load massive multicore system with up to 256 threads in a single system to the max. The second use case shows how several mechanisms integrated in Solaris can ease the deployment and operation of Hadoop even in non-dedicated environments. The last usecase will show the combination of the Sun Grid Engine and Hadoop. Talk may contain command-line demonstrations ;).\nNikolaus Pohle (nurago): \u0026ldquo;M/R for MR - Online Market Research powered by Apache Hadoop. Enable consultants to analyze online behavior for audience segmentation, advertising effects and usage patterns.\u0026quot;\nWe would like to invite you, the visitor to also tell your Hadoop story, if you like, you can bring slides - there will be a beamer.\nA big Thanks goes to the newthinking store for providing a room in the center of Berlin for us. Another big thanks goes to StudiVZ for sponsoring videos of the talks. Links to the videos will be posted here as well as on the StudiVZ blog.\nPlease do indicate on the following upcoming event if you are planning to attend to make planning (and booking tables at Aufsturz) easier:\nhttp://upcoming.yahoo.com/event/4842528/\nLooking forward to seeing you in Berlin,\nIsabel\n"},{"id":369,"href":"/lucene-meetup-oakland269/","title":"Lucene Meetup Oakland","section":"Inductive Bias","content":" Lucene Meetup Oakland # Though pretty late in the evening the room is packed with some 100 people. Most of them solr or pure lucene java users. There are quite a few Lucene committers at the meetup from all over the world. Several even have heard about Mahout - some even used it :)\nSome introductiory questions to index sizes and query volumn: 1 Mio documents seem pretty standard for Lucene deployments - several people run 10 Mio neither. Some people even use indexes with up to billions of documents in Lucene - but at low query volumn. Usually people run projects with about 10 queries per second, but up to 500.\nEric\u0026rsquo;s presentation gives a nice introduction to what is going on with Lucene/Solr in terms of user interfaces. He starts with an overview of the problems that libraries face when building search engines - especially the facetting side of life. Especially interesting seem Solaritas - a velocity response writer that makes it easy to render search responses not in xml but in simple templated output. He of course also included an overview of the features of LucidFind, the Lucid hosted search engine for all Lucene projects and sub-projects. Take Home message: The interface is the application, as are the urls. Facets are not just for lists.\nSecond talk is given by Uwe giving an overview of the implementation of numeric searches and range queries and numeric range filters in Lucene.\nThird presenter is Stefan on katta - a project on top of Lucene that adds index splits, load balancing, index replication, failover, distributed TFIDF. The mission of katta is to build a distributed Lucene for large indexes under high query load. The project heavily relies on zookeeper for coordination. It uses Hadoop IPC for search communication.\nLighting talks include talks by Zoie: A realtime search extension for Lucene, developed inside of LinkedIn and now open sourced at google code.\nJukka proposed a new project: A Lucene-based content mangement system.\nNext presenter highlighted the problem of document-to-document search. The problem here is that queries are not just one or two terms but more like 40 terms.\nNext talk shared some statistics: more than 2s at average leads to 40% abandonance rate for sites. The presenter is very interested in the Lucene Ajax project. Before using solr the speaker set up projects with solutions like Endeca or Mercato. Solr to him is an alternative that supports facetting.\nAndzrej gives an overview of index pruning in 5min - giving details on which approaches are currently being discussed in research as well as in the Lucene jira for index pruning.\nNext talk was on Lucy - a lucene port to C.\nLast talk gave an overview of the findings on analysing the Lucene community.\nOne other lightning talk by a guy using and deploying Typo3 pages. Typo3 does come with an integrated search engine. The presenter\u0026rsquo;s group built an extension to Typo3 that integrates the CMS with Solr search. The final last talk is done by Grant Ingersoll on Mahout. Thanks for that!\nBig Thanks to Lucid for sponsoring the meetup.\n"},{"id":370,"href":"/hadoop-get-together-berlin-apache-con-us-barcamp222/","title":"Hadoop Get Together Berlin @ Apache Con US Barcamp","section":"Inductive Bias","content":" Hadoop Get Together Berlin @ Apache Con US Barcamp # This is my first real day at ApacheCon US 2009. I arrived yesterday afternoon, was kept awake by three Lucene committers until midnight: \u0026ldquo;Otherwise you will have a very bad jetlag\u0026rdquo;\u0026hellip; Admittedly it did work out: I slept like a baby until about 08:00a.m. the next morning and am not that tired today.\nToday Hackthon, Trainings and barcamp Apache happen in parallel. Ross Gardler tricked me into doing a presentation on my experiences on doing local user meetups. I put the slides online.\nThe general consent was, that it is actually not that hard to do such a meetup - at least if you are have someone locally to help organizing or do it in a town you know very well. There are ways to get support from the ASF for doing such meetups - people help you get speakers, talk to potential sponsors or find a location. In my experience if doing the event in your hometown, finding a location is not that hard: Either you are lucky having someone like newthinking store around. Or you can contact you local university or even your employer to find some conference room that you can use for free.\nGetting the first two to three meetups up and running - especially finding speakers - is hard. However you should be able to benefit from being part of an Apache project already and probably know your community and know who would be willing to speak at one of those meetups. Once the meetup is well established, it should be possible to find sponsors to pay for video taping, free beer and pizza.\nKeep in mind that having a fixed schedule ready in advance helps to attract people - it\u0026rsquo;s always good to know why one should travel to the meetup by train or plane. Don\u0026rsquo;t forget to plan for time for socializing after the event - having some beer and maybe food together makes it easy for people to connect after the meetup.\n"},{"id":371,"href":"/apache-hadoop-get-together-berlin-345/","title":"Apache Hadoop Get Together Berlin","section":"Inductive Bias","content":" Apache Hadoop Get Together Berlin # Title: Apache Hadoop Get Together Berlin\nLocation: newthinking store, Tucholskystr. 48, Berlin Mitte\nLink out: Click here\nDescription: The upcoming Apache Hadoop Get Together Berlin will feature four talks by people explaining how they put Hadoop to good use in their entreprise. Table at Cafe Aufsturz is booked already. Talks will be announced late next week.\nStart Time: 17:00\nDate: 2009-12-16\n"},{"id":372,"href":"/open-source-expo317/","title":"Open Source Expo","section":"Inductive Bias","content":" Open Source Expo # Title: Open Source Expo\nLocation: Karlsruhe\nLink out: Click here\nDescription: There will be a booth at Open source expo introducing interested visitors to the Apache projects Lucene and Mahout. Of course we are also happy to answer any questions on the ASF in general.\nStart Date: 2009-11-15\nEnd Date: 2009-11-16\n"},{"id":373,"href":"/apache-con-us-program-up23/","title":"Apache Con US - Program up","section":"Inductive Bias","content":" Apache Con US - Program up # The final program is available for download over at http://us.apachecon.com. The schedule is packed with interesting talks on Hadoop, Lucene, Tomcat, httpd, web services, osgi. For those less tech-savvy there is a business track explaining how to best use open source software in an entreprise environment. There is also a community track explaining what makes open source projects successful.\nLooking forward to seeing you in Oakland.\n"},{"id":374,"href":"/lucene-29-white-paper268/","title":"Lucene 2.9 White Paper","section":"Inductive Bias","content":" Lucene 2.9 White Paper # Lucid recently published a white paper that explains the changes and improvements that the new 2.9 release incorporates. Interesting for all who are thinking about upgrading to the new lucene version or generally want to know what is going on at Lucene.\n"},{"id":375,"href":"/nosql-berlin-meetup293/","title":"NoSQL Berlin Meetup","section":"Inductive Bias","content":" NoSQL Berlin Meetup # Yesterday evening the NoSQL Berlin Meetup took place in newthinking store, Berlin Mitte. We had planned for some 50 to 70 people. It quickly became clear that the room would be full - at startup I counted about 80 guests interested in NoSQL topics both locally from Berlin but also traveling here from New York.\nSome pictures are available on flickr - thanks to @langalex for sending the url to me:\nThe meetup started with an introduction to basic principles on consistancy and agreement protocols that are the basis of many scalable storage solutions, including Scalaris. Monika Moser explained, why one can have only two of the three goals of consistency, availability and partition tolerance. After that she gave an introduction to Paxos - a scalable, partition tolerant agreement protocol.\nIn the second talk, Mathias Meyer introduced Redis - a wicket fast key value store that supports strings, lists and sets as values. It is implemented in C, comes with a persistence mechanism. Only problem: All the data stored in Redis needs to fit in memory for this store to work.\nAfter a short break Jan Lehnardt gave an overview of building P2P applications with CouchDB. He showed how CouchDB can be scaled to large deployments with modules that build distribution and sharding on top of CouchDB. But CouchDB can also be scaled down to run on mobile devices. As synchronization is so simple with that DB it is a perfect fit for Ubuntu One - the initiative of Canonical that brings a personal cloud to everyone for sharing and distributing your data.\nMartin Scholl gave an overview of Riak - a highly distributed key-value store with support for map-reduce style queries, sharding of data and a rest-Interface.\nThe last session included a talk by Mathias Stearn on MongoDB - a key-value store that does not come with json formatted documents but uses bson for document encoding. This makes it easy to support for compact and fast object (de-)serialization.\nThe final talk was given by Prof. Stefan Edlich on object oriented databases.\nAfter the event, speakers and attendees switched over to Cafe Aufsturz for some drinks, beer and food - and of course for further discussions.\nBig thanks goes to the sponsors (Versant, Peritor (drinks at newthinking), StudiVZ (videos), Sociomantic (drinks at Aufsturz), Soundcloud (food at Aufsturz). Another big thanks to Jan Lehnardt and Thomas Nicolai for helping me set up this event.\nLooking forward to seeing you guys either in Oakland this November or probably next year at the next NoSQL conference in Berlin.\n"},{"id":376,"href":"/videos-are-up429/","title":"Videos are up","section":"Inductive Bias","content":" Videos are up # As of yesterday the videos of the last Apache Hadoop Get Together Berlin are available online.\nTh anks to the speakers for providing insight in their projects and thanks to Cloudera for sponsoring the videos.\nThe next meetup will be announced soon - three talks have already been proposed. In addition, StudiVZ offered to sponsor video taping of the next Get Together. Looking forward to seeing you in Berlin in December.\n"},{"id":377,"href":"/scrumtisch-with-mary-and-tom-poppendiek360/","title":"Scrumtisch with Mary and Tom Poppendiek","section":"Inductive Bias","content":" Scrumtisch with Mary and Tom Poppendiek # Yesterday evening the Scrumtisch Berlin hosted a talk by Mary Poppendiek on Lean Development. Mary started the session with a talk on what lean development is all about and why it goes further than Scrum ever did. Some of the core principles she explained:\nThe first goal of every lean project should be to strive for customer satisfaction. The low hanging fruit is to do exactly what the customer wanted to have. The second step is to give features he took for granted as well - think performance, think extensibility. The ultimate goal should be to fulfill what the customer never knew he even wanted. Customers don\u0026rsquo;t want software - they want their problems solved, the only way they are aware of to solve their problem is through software. Show them how to really solve their problems: Before the iPhone was invented, no one could have predicted what a great smart phone could look like.\nShow technical excellence: Testing is not optional. To be a good developer you should always be able to prove that your code is correct. As proving the correctness of software in the mathematical sense is hard, if not impossible today, the recommendation was to use unit testing and integration testing as a poor man\u0026rsquo;s proof for correctness. Without (automated) testing, any agile project is doomed to develop into a big ball of mud that cannot be easily extended.\nThe third principle is to deliver reliably, to design the system such that it meets its constraints. Constraints here are not only technical requirements - constraints include budget, time, features, stability and scalability.\nThe fourth observation was that no development process can fix the problems that Junior-level-only teams experience. Every lean organisation must provide for mentoring. The mentor\u0026rsquo;s job is to climb the same path as the mentee and pull him along - instead of pushing. In contrast to many setups, not only should new hires be mentored, but basically every employee should have a mentor. Only that way learning - which is essential in current project (not only it) environments - is made possible.\nA very interesting observation came from the audience: Every single advise given above is well known and appreciated by any reasonably good software engineer. Problem is: Why don\u0026rsquo;t we adopt these rules in practice? The guy asking the question gave the answer himself: \u0026ldquo;The fish starts smelling from its head\u0026rdquo; (German proverb). So the goal of lean developers in the end would be to talk to their management: Do short iterations of a few weeks. Deliver early, deliver often, deliver in time. Usually that alone is enough to exceed expectations. Choose the best solution (both from business and technical perspective). Your results should speak for themselves to step by step change the way your organisation works.\nSlides are online on the Scrumtisch blog.\n"},{"id":378,"href":"/katta-berlin255/","title":"katta @ Berlin","section":"Inductive Bias","content":" katta @ Berlin # After finishing the slides for next week\u0026rsquo;s Mahout course at TU Berlin (if you are not subscribed yet: Subscribe now!) I spent half of the day in Tierpark Berlin: Watching ice-bears, taking pictures of tigers. On my way through the park I met those cute little guys:\nThe plate next to the bawn gave them away as \u0026hellip; kattas - so that is what they look like!\nOh - just in case you were searching for the real distributed lucene katta \u0026hellip; that is available over on Sourceforge and not to be confused with those little animals ;)\n"},{"id":379,"href":"/lucene-29-heise267/","title":"Lucene 2.9 @ Heise","section":"Inductive Bias","content":" Lucene 2.9 @ Heise # After last week\u0026rsquo;s Hadoop Get Together heise published an in-depth article on the changes and improvements that come with the latest Lucene 2.9 release.\nThanks to Simon Willnauer for helping me write this article and patiently explaining several new features. Thanks also to Uwe Schindler for kindly proof-reading the article before it was sent out to Heise.\n"},{"id":380,"href":"/scrum-day-dusseldorf341/","title":"Scrum Day Düsseldorf","section":"Inductive Bias","content":" Scrum Day Düsseldorf # On 01. to 02. December the Scrum Day is going to take place in Düsseldorf.\nIf you are working in a non-Scrum company and would like to use agile methods both for development and management, I would like to recommend going to the talk by Thilo Fromm: Scrum in a waterfall. He explains how he transformed his project to an agile way in a waterfall environment.\n"},{"id":381,"href":"/getting-hadoop-0210-up-and-running-from-source211/","title":"Getting Hadoop trunk up and running from source","section":"Inductive Bias","content":" Getting Hadoop trunk up and running from source # Having told Thilo about the possibility to write Hadoop jobs in Python with Dumbo, we spent some time getting Dumbo 0.21 up and running over the past weekend. The first option the wiki proposes is to take a pre-0.21 release and patch that to work with the current Dumbo release. The second option described takes the not-yet-released version of Hadoop that can be used w/o any patches.\nWe decided to follow the latter suggestion. After the latest split of the project, we downloaded common, hdfs and mapreduce. Building each project was easy - assuming that ant, Sun JDK 6 (for Hadoop), Forrest (for the documentation pages) and Sun JDK 5 (for forrest) is installed.\nDeviating from the documentation, the distributed filesystem as well as map reduce are now started from separate scripts (start-dfs.sh/ start-mapred.sh instead of start-all.sh). These scripts are located in the common project. In addition the variables HADOOP_HDFS_HOME and HADOOP_MAPRED_HOME must be set to point to respective projects for cluster setup to work. Other than that the setup currently is identical to the previous version.\n"},{"id":382,"href":"/dev-house-berlin-20152/","title":"Dev House Berlin 2.0","section":"Inductive Bias","content":" Dev House Berlin 2.0 # This weekend DevHouseBerlin took place in the Box119, kindly organized by Jan Lehnardt, sponsored by Upstream and StudiVZ. There were about 30 people gathered in Friedrichshain, hacking and discussing various projects: Mostly Python/ Django, Ruby/ Rails and Erlang people. The first day was reserved for hacking and exchanging ideas. Late afternoon attendees put together a list of talks that were than rated, ranked with the top three chosen for presentation on Sunday. The list included topics on CouchDB, RestMS, Hadoop, Concurrency in Erlang, P2P CouchDB and many more. The first three topics were chosen by the participants for presentation.\nDuring the time at DevHouse I finally got a list of topics and papers up at Mahout TU project - now only the exact credit system for the Mahout course at TU is missing. I got some time to work on Mahout improvements and documentation. Unfortunately I was too tired today to complete the code review for MAHOUT-157 - promise to do that early next week.\nSpending one weekend with equal-minded people, being able to pair with someone else in case of more complex problems made the weekend a great time for me. Planning to be there again next year. Thanks to the sponsors and organisers for making this happen.\n"},{"id":383,"href":"/one-year-coding-with-das-keyboard314/","title":"One year coding with \"Das Keyboard\"","section":"Inductive Bias","content":" One year coding with \u0026ldquo;Das Keyboard\u0026rdquo; # Little more than a year ago, I got myself a keyboard from newthinking store. The special thing about it: It is completely black, except for three blue LEDs. With completely I mean no labels on any key:\nMy main motivation to buy the thing was not the missing labels (although that certainly does make it a cool gadget). I am typing 8h at work and do spend quite a bit of time in front of my laptop after work as well. So I wanted a keyboard that is fun to use: This includes typing speed and the tactile feedback of each key.\nI must admit that today, I am hardly ever happy with any other keyboard - usually my hands start protesting after about half an hour. So, despite some odd looks I get at work, it did help increase coding fun :)\n"},{"id":384,"href":"/slides-are-up372/","title":"Slides are up","section":"Inductive Bias","content":" Slides are up # The slides for yesterday\u0026rsquo;s talks just arrived. They are available online at:\nIsabel Drost: Brief introduction.\nThorsten Schuett: Solving puzzles with map reduce.\nThilo Goetz: An introduction to jaql.\nUwe Schindler: Lucene 2.9 developments.\nVideos will be online early next week.\n"},{"id":385,"href":"/apache-hadoop-get-together-berlin-248/","title":"Apache Hadoop Get Together Berlin","section":"Inductive Bias","content":" Apache Hadoop Get Together Berlin # The Get Together started just a few minutes ago. The room is packed with more than 35 people this time. This is the first Hadoop Get Together in Berlin that will be recorded on video, thanks to Martin from newthinking for doing the recording and post processing as well as to Cloudera for sponsoring the videos.\nThe first talk was given by Thorsten Schuett on solving puzzles with map reduce. His disclaimer: Working at ZIB Berlin he had a large cluster in the basement to put to good use. However the cluster does not run Hadoop. It is based on Lustre FS and does not rely on commodity hardware. So he implemented a solver for 4x4 sliding puzzles in a map reduce framework targeted for \u0026ldquo;his\u0026rdquo; cluster.\nSecond talk was by Thilo Goetz on JAQL, a language for querying JSON documents that can run queries on top of a Hadoop cluster.\nIn the third and last talk, Uwe Schindler gave an overview of the new features and performance improvements of last weeks Lucene 2.9 release.\nAfter raffling the Hadoop books donated by O\u0026rsquo;Reilly, we will move to a bar close by after the talks are over to have some beer and continue discussions. A summary that includes more details as well as links to the slides will be online soon.\nUpdate: I had reserved a table at Cafe Aufsturz close to newthinking store for about 15 people - maybe less, maybe more. We ended up going there with more than 25 people - really glad there were still enough tables left for us :)\nUpdate 2: Next meetup - December 16th, I already got one definite and two tentative proposals for talks.\n"},{"id":386,"href":"/aws-user-group-berlin108/","title":"AWS User Group Berlin","section":"Inductive Bias","content":" AWS User Group Berlin # On Monday the first AWS user group took place in newthinking store, Berlin. The event featured talks by Martin Buhr from Amazon as well as presentations of AWS users like Dawanda, Peritor and Sound Cloud.\nUnfortunately the most interesting question concerning Elastic Map Reduce was left unanswered by Martin: Does using EMR facilitate exploiting data locality/ rack locality optimizations that are possible in Hadoop? The question on whether Amazon is using the AWS APIs internally as well was answered positively, though of course they did not publish all of their systems infractructure.\nNext meeting was scheduled to take place in two months time. Thanks to Peritor for organizing the meetup.\n"},{"id":387,"href":"/looking-for-a-dancing-school-in-berlin266/","title":"Looking for a dancing school in Berlin","section":"Inductive Bias","content":" Looking for a dancing school in Berlin # I am looking for a dancing school (standard as well as Salsa Cubana) in Berlin Schöneberg. So in case you have any recommendations - please leave a comment.\n"},{"id":388,"href":"/apache-hadoop-get-together-coming-up416/","title":"Upcoming: Apache Hadoop Get Together Berlin","section":"Inductive Bias","content":" Upcoming: Apache Hadoop Get Together Berlin # This is a friendly reminder that the next Apache Hadoop Get Together takes place next week on Tuesday, 29th of September* at newthinking store (Tucholskystr. 48, Berlin).\nThorsten Schuett, Solving Puzzles with MapReduce.\nThilo Götz, Text analytics on jaql.\nUwe Schindler, Lucene 2.9 Developments.\nBig thanks goes to newthinking store for providing the venue for free and to Cloudera for sponsoring videos of the talks. Links to the videos will be posted on http://isabel-drost.de/hadoop, on the upcoming page linked above, as well as on the Cloudera Blog soon after the event. Yet another thanks goes to O\u0026rsquo;Reilly for providing three \u0026ldquo;Hadoop: The Definitive Guide\u0026rdquo; books to be raffled at the event.\nThe 7th Get Together is scheduled for December, 16th. If you would like to submit a talk or sponsor the event, please contact me.\nHope to see you in Berlin next week.\n* The event is scheduled right before the UIMA workshop in Potsdam, which may be of interest to you if you are a UIMA user.\n"},{"id":389,"href":"/scrum-tisch-3348/","title":"Scrum Tisch","section":"Inductive Bias","content":" Scrum Tisch # Title: Scrum Tisch\nDescription: The Scrumtisch on October 11th will feature a talk by Mary Poppendieck.\nShe will join that extraordinary Scrumtisch at 6pm.\nThe location is not yet defined yet, because Marion first needs to know how many of are coming.\nStart Time: 18:00\nDate: 2009-10-11\n"},{"id":390,"href":"/scrum-tisch-2349/","title":"Scrum Tisch","section":"Inductive Bias","content":" Scrum Tisch # Title: Scrum Tisch\nLocation: La Vecchia Trattoria\nDescription: The next Scrum Tisch organized by Marion Eickmann takes place this Thursday. Since a pretty long time the format will be open for questions, prioritized by the participants again.\nThe location is in Niederbarnimstraße 25, near U-Bahn Samariterstrasse.\nStart Time: 18:30\nDate: 2009-09-24\n"},{"id":391,"href":"/mahouttu-ws-0910282/","title":"Mahout@TU WS 09/10","section":"Inductive Bias","content":" Mahout@TU WS 09/10 # Title: Mahout@TU WS 09/10\nThere is going to be a project/seminar course at TU Berlin on Apache Mahout. The goal is to introduce students to the work on a free software project, interact with the community and build production ready software.\nStudents will be given several potential tasks ranging from optimizing existing implementations, implementing new algorithms and (depending on their prior knowledge) improving, scaling and parallelizing existing algorithms.\nSuccessful completion of the course depends on a number of factors: Interaction of the student with the community, ability to write tested (as in test-first-developed) code that performs well in a large scale environments, ability to show incremental development progress at each iteration, ability to review patches and improvements, usage of tools like SCM, Issue-tracker and mailinglists. Of course theoretical background - that is understanding existing publications as well extending their ideas is crucial as well.\nIf you are a student interessted in Mahout missing some course work, consider subscribing to the Mahout course at DIMA Berlin (linked below). Goal is that your work is to be integrated in one of the next releases, once the community is satisfied.\nIf you are a Mahout developer or user and have some issue that you consider suitable for a student to solve, please to provide your ideas.\nLocation: TU Berlin\nLink out: Click here\nStart Date: 2009-10-01\u0026lt;br /\u0026gt;End Date: 2010-03-31\n"},{"id":392,"href":"/gsoc-at-mahout219/","title":"GSoC at Mahout","section":"Inductive Bias","content":" GSoC at Mahout # GSoC 2009 is about to finish: Final evaluations are through, most of the code submitted by Mahout\u0026rsquo;s students has been committed to svn, code samples are on their way to Google.\nIn Mahout, we had three students joining the project: Robin working on an HBase based Naive Bayes extension and on frequent itemset discovery. David contributing a distributed LDA implementation. Deneche was working on a Random Forest implementation. All three of them have done great work during this summer, contributing not only code but valuable input on the project\u0026rsquo;s mailinglists as well. As a result, all three of them have been given committer status by the end of GSoC.\nApart from three new additions to the code base, summer also brought quite some traffic to the user list - not only in terms of subscriptions but also in terms of developers contributing to the discussions online. Currently, it looks like the project is really gaining momentum, as also noted in Grant Ingersoll\u0026rsquo;s post.\nDiscussions on the dev list on the future road map of Mahout clearly showed that the developers share the vision of a scalable, potentially distributed, stable machine learning library. That the focus should be on production ready code under a commercially friendly license instead of bleeding edge research implementations. Last but no least the goal is to build a lively, diverse community around the project to guarantee further development and user support.\n2009 brought quite a few talks both in Germany as well as the US on the topic of Mahout (besides all the events on Hadoop, scalable databases and cloud computing in general) with an Apache Con US talk introducing Mahout in Oakland still to come.\nYesterday, a great article indroducing Apache Mahout with hands-on examples was published on IBM Developerworks by Grant Ingersoll. Check it out, if you want to learn more on Mahout, and Machine Learning in general.\n"},{"id":393,"href":"/first-nosql-meetup-in-germany176/","title":"First NoSQL Meetup in Germany","section":"Inductive Bias","content":" First NoSQL Meetup in Germany # On October 22nd 2009 the first NoSQL Meetup Germany is going to take place in newthinking store/ Berlin: http://nosqlberlin.de\nPlease submit your presentation proposals until September 22nd, accepted speakers will be notified soon after.\nIf you would like to sponsor the event, feel free to contact us: We would be very happy to provide videos after the event and free drinks for everyone during the event.\nHope to see you soon in Berlin.\n"},{"id":394,"href":"/two-days-on-rugen415/","title":"Two days on Rügen","section":"Inductive Bias","content":" Two days on Rügen # Below: Sunday morning on Rügen, feet in the East Sea. It looks colder than it actually was: Air was about 17 °C, but the water was fine. At least for my feet.\nThanks to the colleague who sold two spare tickets for the Störtebecker Festspiele last Thursday and caused us to drive to isle Rügen yesterday afternoon.\n"},{"id":395,"href":"/apache-con-drawing-closer17/","title":"Apache Con drawing closer","section":"Inductive Bias","content":" Apache Con drawing closer # By November I will be traveling to Oakland - for me it is the first Apache Con US ever. And the first Apache Con I will be giving a talk in one of the main tracks:\nI will be presenting Apache Mahout, give an overview of the project, of our current status and explain which problems can be solved with the current implementation. The talks will conclude with an outlook to upcoming tasks and features our users can expect in the near future.\nThere is great news already: First commercial users like Mippin are explaining their experiences with Mahout.\nCurrently, I am looking forward to meeting several Mahout (and Lucene, Hadoop, Solr, \u0026hellip;) committers there. I met some at Apache Con EU already, but it\u0026rsquo;s always nice to talk to people in person who before one only knew from mailing lists. Of course I am also looking forward to having time to review and write code. Hope to see you there.\nUpdate: Flights booked.\n"},{"id":396,"href":"/gnuplot-tutorial-link212/","title":"gnuplot tutorial link","section":"Inductive Bias","content":" gnuplot tutorial link # As I am happily watching myself searching for the gnuplot tutorial over and over again - the direct link stored here to save future searching:\nhttp://t16web.lanl.gov/Kawano/gnuplot/index-e.html\n"},{"id":397,"href":"/inglourious-basterds238/","title":"Inglourious Basterds","section":"Inductive Bias","content":" Inglourious Basterds # This evening I went to the cinema Odeon in Berlin Schöneberg. It is a pretty traditional, old-fashioned and very lovely cinema that has specialised on showing non-dubbed, original versions of movies.\nShowing the great movie Inglourious Basterds, the cinema was completely sold out today. Fortunately we were able to grab some of the last tickets.\nJust in case the entrance seemed familiar to those who have attended a Mahout presentation in the recent past - a picture of the Odeon usually visualises one part of my motivation on the Mahout slides ;)\n"},{"id":398,"href":"/apache-hadoop-event-blog41/","title":"Apache Hadoop Event Blog","section":"Inductive Bias","content":" Apache Hadoop Event Blog # As Apache Hadoop becomes ever more popular both in industry as well as in research, user groups, conferences and hacking days are being scheduled around the world. The goal of the event calendar blog hosted on wordpress.com is to provide a common space for organizers to announce their events and potential participants to look for new conferences.\n"},{"id":399,"href":"/september-apache-hadoop-get-together-berlin366/","title":"September Apache Hadoop Get Together @ Berlin","section":"Inductive Bias","content":" September Apache Hadoop Get Together @ Berlin # The upcoming Apache Hadoop Get Together Berlin is to take place on September 29th in newthinking store. Details are up on the web page at upcoming and will be sent out to the mailing list soon.\n"},{"id":400,"href":"/fellow-now173/","title":"Fellow now","section":"Inductive Bias","content":" Fellow now # After two years volunteering as booth staff for the FSFE at the Chemnitzer Linuxtage explaining the advantages of becoming a FSFE fellow I am a fellow myself for two days ;)\nI first got in contact with the FSFE through Fernanda Weiden during my time in Zürich in 2006. In the meantime I have learned more and more about the political activities of FSFE: Mostly during the local Berlin meetups in newthinking store and as a booth member in Chemnitz.\nIf you yourself want to support the work of FSFE, join the fellowship.\n"},{"id":401,"href":"/flying-back-home-from-cologne178/","title":"Flying back home from Cologne","section":"Inductive Bias","content":" Flying back home from Cologne # Last weekend FrOSCon took place in Sankt Augustin, near Cologne. FrOSCon is organized on a yearly basis at the university of applied sciences in Sankt Augustin. It is a volunteer driven event with the goal of bringing developers and users of free software projects together. This year, the conference featured 5 tracks, two examples being cloud computing and the Java track.\nUnfortunately this year the conference started with a little surprise for me and my boyfriend: Being both speakers, we had booked a room in Hotel Regina via the conference committee. Yet on Friday evening we had to learn that the reservation never actually reached the hotel\u0026hellip; So, after several minutes talking to the receptionist, calling the organizers we ended up in a room that was booked for Friday night by someone who was known to arrive no earlier than Saturday. Fortunately for us we have a few friends close by in Düsseldorf: Fnord was so very kind to let us have his guest couch for the following night.\nCheckin time next morning: On the right hand side the regular registration booth. On the left hand side the entrance for VIPs only. The FSFE quickly realized it\u0026rsquo;s opportunity: They soon started distributing flyers and stickers among the waiting exhibitors and speakers.\nSet aside the organizational issues, most of the talks were very interesting and well presented. The Java track featured two talks by Apache Tomcat committer Peter Roßbach, the first one on the new Servlet 3.0 API, the second one on Tomcat 7. Too sad, my talk was in parallel to his Tomcat talk, so I couldn\u0026rsquo;t attend that. I appreciate several of the ideas on cloud computing highlighted in the keynote: Cloud computing as such is not really new or innovative, it is several good ideas so far known for instance as utility computing that are now being improved and refined to make computation a commodity. At the very moment however cloud computing providers tend to sell their offers as new, innovative products. There is no standard API for cloud computing services. That makes switching from one provider to another extremely hard and leads to vendor-lockin for its users.\nThe afternoon was filled by my talk. This time I tried something, that so far I only have done in user groups of up to 20 people: I first gave a short introduction into who I am and than asked the audience to describe themselves in one sentence. There were about 50 people, after 10 minutes everyone had given is self-introduction. It was a nice way of getting detailed information of what knowledge to expect from people, and it was interesting to hear people from IBM and Microsoft being in the room.\nAfter that I attended the RestMS talk by Thilo Fromm and Peter Hintjens. They showed a novel, community driven way to standards creation. RestMS is a messaging standard that is based on a restful way for communication. So far the standard itself is still in it\u0026rsquo;s very early stages, still there are some very “alpha, alpha, alpha” implementations out there that can be used for playing around. According to Peter there are actually people who already use these implementations for production servers and send back bug reports.\nSunday started with an overview of the DaVinci VM by Dalibor Topic, the author of the OpenJDK article series in the German Java Magazin. Second talk of the day was an introduction to Scala. I already know a few details of the language, but the presentation made it easy to learn more: It was organised as an open question and answer session with live coding leading through the talk.\nAfter lunch and some rest, the last two topics of interest were on details on the campaigns of FFII against software patents and an overview of the upcoming changes in gnome3.\u00100.\nThis year\u0026rsquo;s FrOSCon did have some organizational quirks but the quality of most of the talks was really good with at least one interesting topic in one of the sessions at nearly every time slot - though I must admit that that was easy in my case with Java and cloud computing being of interest to me.\nUpdate: Videos are up online.\n"},{"id":402,"href":"/converting-a-git-repo-to-svn144/","title":"Converting a git repo to svn","section":"Inductive Bias","content":" Converting a git repo to svn # Pretty unlikely though it may seem, but there are cases when one might want to convert a git repo to svn and still keep all revisions intact. There is a nice explanation online on how to do that in the Google Open Source blog.\n"},{"id":403,"href":"/hadoop-get-together-berlin-2365/","title":"September 2009 Hadoop Get Together Berlin","section":"Inductive Bias","content":" September 2009 Hadoop Get Together Berlin # The newthinking store Berlin is hosting the Hadoop Get Together user group meeting. It features talks on Hadoop, Lucene, Solr, UIMA, katta, Mahout and various other projects that deal with making large amounts of data accessible and processable. The event brings together leaders from the developer and user communities. The speakers present projects that build on top of Hadoop, case studies of applications being built and deployed on Hadoop. After the talks there is plenty of time for discussion, some beer and food.\nThere is also a related Xing Group on the topic of building scalable information retrieval systems. Feel free to join and meet other developers dealing with the topic of building scalable solutions.\nAgenda:\nPlease see upcoming page for updates.\nThilo Götz: JAQL\nUwe Schindler: Lucene 2.9\nnugg.ad: Ad Recommendation with Hadoop\nT. Schuett: Solving puzzles with Hadoop.\nIf you yourself would like to give a presentation: There are additional slots of 20 minutes each available. There is a beamer provided. Just bring your slides. To include your topic on this web site as well as the upcoming.org entry, please send your proposal to Isabel. After the talks there will be time for an open discussion. We are going into a nearby restaurant after the event so there will be plenty of time for talking, discussing and new ideas.\nLocation\nThe Apache Hadoop Get Together takes place at the newthinking store Berlin:\nnewthinking store GmbH\nTucholskystr. 48\n10117 Berlin\nView Larger Map\nAccomodation\nHomeli - not exactly in walking distance, but only a few S-Bahn stations away. Very nice Bed and Breakfast hotel. (The offer is only valid if you stay for at least three nights.)\nCircus Berlin is a combination of hostel and hotel close by.\nZimmer in Berlin is yet another Bed and Breakfast hotel.\nHouse boat near Friedrichshain\nAnnouncements\nIf you would like to be notified on news please subscribe to our mailinglist. The meetings usually are also announced on the project mailing lists as well as on the newthinking store website.\nContact\nIn case you have any trouble reaching the location or finding accomodation feel free to contact the organiser Isabel.\nPast events\nSeptember 2009 Hadoop Get Together\nJune 2009 Hadoop Get Together March 2009 Hadoop Get Together\nDecember 2008 Hadoop Get Together\nSeptember 2008 Hadoop Get Together\nFirst Hadoop Get Together @ Berlin\n"},{"id":404,"href":"/amqp-erlang-user-group-talk9/","title":"AMQP Erlang user group talk","section":"Inductive Bias","content":" AMQP Erlang user group talk # Last Wednesday at the Erlang user group Berlin Matthias Radestock from the RabbitMQ project gave a talk on RabbitMQ, AMQP and messaging in general. Slides are available online.\nFirst Matthias motivated the need for an open standard for messaging: So far, their are a few provides of middleware systems like Tibco and IBM. But those solutions are usually closed, expensive, cumbersome to handle. In short they do not fit into a world where people rely on open standards for communication, free software for development and lightweight implementations.\nAMQP aims to povide an open standard for messaging - that is decoupled communication between processes that may reside on separate boxes or in different datacenters. There are a few providers of AMQP implementations. Some examples are iMatix focussed on low latency communication, Apache Qpid and the corresponding project inside of RedHat and RabbitMQ.\nRabbitMQ is implemented in Erlang (after all, the talk was hosted by the Erlang User Group Berlin ;) ). With about 7000 lines of code the code base is rather compact. The goal was not to built a super-fast implementation, but one that is scalable and highly available.\nSo far there is no facility for building reliable cross datacenter communication built into RabbitMQ. Yet, there are several projects available that aim at providing just that.\n"},{"id":405,"href":"/solr-at-aol376/","title":"Solr at AOL","section":"Inductive Bias","content":" Solr at AOL # Grant Ingersoll has posted a very interesting interview with Ian Holsman on Solr at Relegance, now AOL. It describes the business side of the decission to switch to an open source solution, provides some inside on the size of the installation and details which technological reasons have driven the decission to switch from a proprietary implementation to Solr:\nhttp://www.lucidimagination.com/Community/Hear-from-the-Experts/Podcasts-and-Videos/Interview-Ian-Holsman-Relegence\u0026lt;/ a\u0026gt;\n"},{"id":406,"href":"/lucene-slides-online270/","title":"Lucene slides online","section":"Inductive Bias","content":" Lucene slides online # The slides of the Lucene talk at the last Apache Hadoop Get Together Berlin are available online: Lucene Slides. Especially interesting to me are the last few slides which detail both index size and machine setup:\nThe installation is running on two standard PCs with 2 dual-core processors (usual speed, bought in January 2008 for about 4000 Euro). They have 32GB RAM, 24 GB are used as ramdisk for the index. Without ramdisk initial queries especially those accessing fields are slower but still acceptable. The index contains about 19 million documents, that is 80GB of indexed text + billions of annotated tags.\n"},{"id":407,"href":"/data-serialization146/","title":"Data serialization","section":"Inductive Bias","content":" Data serialization # XML, JSON and others are currently standard data exchange formats. Being human-readable but still structured enough to be easily parsable by programs is their main benefit. Problems are overhead in size and parsing time. In addition at least xml is not really as human-readable as it could be.\nAn alternative are binary formats. Yet those often are not platform independent (either C++ or Java or Python bindings) or are not upgradable (what if your boss comes along and wants you to add yet another field? Do you need to process all your data again?).\nThere are a few libraries that promise to solve at least some of these problems. Usually you specify your data format with an IDL, generate (Byte-)code from it and use mechanisms provided by the libraries to upgrade your format.\nYesterday at the Berlin Apache Hadoop Get Together Torsten Curdt gave a short introduction to two of these solutions: Thrift and Protocol buffers. He explained why Joost decided to use one of those libraries and highlighted why they went with Thrift instead of Protocol Buffers.\nThis morning I have gathered a list of data exchange libs that are currently available:\nThrift \u0026hellip; developed at Facebook, now in the Apache incubator, active community, Bindings for C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, Smalltalk, and OCaml.\nProtoBuf \u0026hellip; developed at Google, mainly one developer only, bindings for C++, Java und Python.\nAvro \u0026hellip; started by Doug Cutting, skips code generation.\nETCH \u0026hellip; developed at Cisco, now in the Apache Incubator, Bindings for Java, C#, JavaScript.\nThere are some performance benchmarks online. Another recent, extensive comparison of serialization performance of various frameworks.\n"},{"id":408,"href":"/large-scalability-papers-and-implementations259/","title":"Large Scalability - Papers and implementations","section":"Inductive Bias","content":" Large Scalability - Papers and implementations # In recent years the Googles and Amazons on this world have released papers on how to scale computing and processing to terrabytes of data. These publications have led to the implementation of various open source projects that benefit from that knowledge. However mapping the various open source projects to the original papers and assigning tasks that these projects solve is not always easy.\nWith no guarantee of completeness this lists provides a short mapping from open source project to publication.\nThere are further overviews available online as well as a set of slides from the NOSQL debrief.\nMap Reduce\nHadoop Core Map ReduceDistributed programming on rails, 5 Hadoop questions, 10 Map Reduce Tips\nGFSHDFS (Hadoop File System)Distributed file system for unstructured data\nBigtableHBase, HypertableDistributed storage for structured data, When to use HBase.\nChubbyZookeeperDistributed lock- and naming service\nSawzallPIG, Cascading, JAQL, HiveHigher level langage for writing map reduce jobs\nProtocol BuffersProtocol Buffers, Thrift, Avro, more traditional: Hessian, Java serializationData serialization, early benchmarks\nSome NoSQL storage solutionsCouchDB, MongoDBCouchDB: document database\nDynamoDynomite, Voldemort, CassandraDistributed key-value stores\nIndexLuceneSearch index\nIndex distributionkatta, Solr, nutchDistributed Lucene indexes\nCrawlingnutch, Heritrix, droids, Grub, ApertureCrawling linked pages\n"},{"id":409,"href":"/july-2009-apache-hadoop-get-together-berlin248/","title":"June 2009 Apache Hadoop Get Together @ Berlin","section":"Inductive Bias","content":" June 2009 Apache Hadoop Get Together @ Berlin # Just a brief reminder: Next week on Thursday the next Apache Hadoop Get Together is scheduled to take place in Berlin. There are quite a few interesting talks scheduled:\nTorsten Curdt: Data Legacy - the challenges of an evolving data warehouse\nChristoph M. Friedrich, Fraunhofer Institute for Algorithms and Scientific Computing (SCAI): \u0026ldquo;SCAIView - Lucene for Life Science Knowledge Discovery\u0026rdquo;.\nUri Boness from JTeam in Amsterdam: Solr - From Theory to Practice.\nSee http://upcoming.yahoo.com/event/2488959/ for more information.\nFor those interested in NOSQL Meetups, the discussion over at the NOSQL mailing list might be of interest to you: http://blog.oskarsson.nu/2009/06/nosql-debrief.html\n"},{"id":410,"href":"/scrum-table-berlin345/","title":"Scrum Table Berlin","section":"Inductive Bias","content":" Scrum Table Berlin # Last week I attended the scrum table Berlin. This time around Phillippe gave a presentation on \u0026ldquo;backlog colours\u0026rdquo;, that is types of work items tracked in the backlog.\nThe easiest type to track are features - that is items that generate revenue and are on the wishlist of the customer. Second type of items he sees are infrastructure items - that is, things needed to implement several features but invisible to the customer. Third type are bugs. Basically these are diminishing the value of features one had already classified as done earlier in the process. Fourth and last type are technical debt items - that is shortcuts taken or bad design choices (either knowingly as intentional decision made to meet some deadline or unintentional due to lack of experience).\nA very simple classification could be the following matrix:\nNameValueCost\nFeatureVisible PositivePositive\nInfrastructureInvisible PositivePositive\nBugVisible NegativePositive\nTechnical DebtInvisible NegativePositive\nAll four types of items exist in the real world. The interesting part is making these visible, assigning costs to each of them and scheduling these items in the regular sprint intervals.\nThe full presentation can be downloaded: http://scrumorlando09.pbworks.com/f/kruchten_backlog_colours.pdf\n"},{"id":411,"href":"/open-street-map-fsfe-meetup320/","title":"Open Street Map @ FSFE meetup","section":"Inductive Bias","content":" Open Street Map @ FSFE meetup # At the last meeting of the local FSFE group here in Berlin Sabine Stengel from cartogis gave a presentation on Open Street Map. But instead of focussing on the technical side she described the legal issues and showed the broad variety of commercial projects that are possible with this type of mapping information.\nIt was interesting to learn of how detailed and high quality the information provided by volunteers really is. I think it will be interesting to see, how the project keeps traction after \u0026ldquo;everything is mapped\u0026rdquo; - how it remains interesting to stay involved, to keep the information up to date over a longer period of time.\n"},{"id":412,"href":"/keeping-changesets-small257/","title":"Keeping changesets small","section":"Inductive Bias","content":" Keeping changesets small # One trick of successful and efficient software development is tracking changes in the sources in source code management systems, be it centralized systems like svn or perforce or decentralized systems like git or mercurial. I started working with svn while working on my Diploma thesis project in 2003, continued to use this systems while researcher at HU Berlin. Today I am using svn at work as well as for Apache projects and have come to like git for personal sandboxes.\nOne thing that bothered me for the last few months was the question of how to keep changesets reasonably small to be easy to code-review but also complete in that they contain enough to implement at least part of a feature fully.\nSo what makes a clean changeset for me: It contains at least one unit test to show that the implementation works. It contains all sourcecode needed to make that test work and not break anything else in the source tree. It might contain every change needed to implement one specific feature. The second sort of changeset that comes to my mind might be rather large and contains all changes that are part of refactoring the existing code.\nThere are a few bad practices that in my experience lead to large unwieldy changesets:\nMaking two or more changes in one checkout. Usually this happens whenever you checkout your code, start working on a cool new feature but get distracted by some other incoming feature request, by some bugfix or by mixing your changes with a patch from another developer you are about to review. Mixing changes makes it extremely difficult to keep track of which change belongs to which task. Usually the result is not checking in some of your changes and breaking the build.\nRefactoring while working on a feature. Imagine the following situation: You are happily working along implementing your new feature. But suddenly you realize that some of your code should be refactored to better match your needs. And whoops: Suddenly it is no longer clear whether changes were simply made due to the refactoring steps (that might even be automated) or due to your new feature. I tend to at least try to do refactorings either before my changes, in a new checkout or after I finished changing the code (if that is feasable).\nThe definition of feature is too large. I tend to get large changesets whenever I try to do too much in one go. That is, the \u0026ldquo;feature\u0026rdquo; that I am trying to implement simply is too large. Usually it is possible to break the task up into a set of smaller tasks that are easier to manage.\nIf using git, there is a nice option to avoid to re-checkout a project: You can simply \u0026ldquo;stash\u0026rdquo; away changes up to the current point in time, do all that is needed for what distracted you, check that in and re-apply the changes for your previous task.\nThis list will be updated as I learn about (that is \u0026ldquo;make\u0026rdquo;) more mistakes that can be cleanly classified into new categories.\n"},{"id":413,"href":"/scrum-tisch347/","title":"Scrum Tisch","section":"Inductive Bias","content":" Scrum Tisch # Title: Scrum Tisch\nLocation: Divino FHain\nLink out: Click here\nDescription: Philippe will present his speech from the Orlando scrum Gathering where he will speak about backlog and time-box, about value versus cost, about visible features versus invisible features (and in particular software architecture), about defects and technical debt, and more generally about release planning and sprint planning for non-trivial and long-lived software development projects.\nStart Time: 18:00\nDate: 2009-06-16\n"},{"id":414,"href":"/ken-schwaber-in-berlin-xberg258/","title":"Ken Schwaber in Berlin XBerg","section":"Inductive Bias","content":" Ken Schwaber in Berlin XBerg # Last week I attended a discussion meetup with Ken Schwaber in Berlin/ Kreuzberg. The event was scheduled pretty shortly\nstill quite a few developers and project managers from various companies in Berlin showed up.\nKen started with a brief summary of the history of Scrum: Before there was such a thing as an IT industry programming actually was a lot of fun. But somehow the creative job was turned into something people tend to suffer from pretty quickly as people tried to apply principles from manufacturing industries to software \u0026ldquo;production\u0026rdquo;. Suddenly there was a distinction between testers, programmers, architects\u0026hellip; People tried to plan ahead for months or even years noticing only very late in the process that the outcome was by no means what was needed when the product finally was ready.\nIn contrast to waterfall Scrum comes with very short feedback loops. It comes with developers working with very strong focus on one task at a time. Change is not hated but embraced and built into development.\nSome features of Scrum that are often forgotten but never the less essential that were discussed that evening:\nScrum is all about transparency - it\u0026rsquo;s about telling your customers what is going on. It is about telling your customer honest estimations. It is about telling development to the best of your knowledge all that can makes up for a feature.\nScrum is neither easy nor a solution in itself. It is simply a way of uncovering problems very quickly that are easier to hide in waterfall processes. You have one person who is an isle of knowledge in your company? At every sprint planning this problem will become obvious until you find a way to solve it.\nScrum is about giving developers a box of time that is not to be interrupted. Developing software asks for a lot of concentration. Getting interrupted and resuming work on the task again is so expensive that there is close to nothing this can be justified with.\nA nice way of doing Scrum is to use Scrum for management and XP for development. Scrum does not provide any solutions on how to reach the goals set - it does not tell you exactly how to arrive at a stable release by the end of your sprint. It just sets the goal for you. On the other hand XP holds quite a few development best practices that can help achieve these goals.\nIt needs time to change how customers and developers are working: Yearlong experience has trained them to think in certain ways. So at the beginning Scrum is all about teaching and training people. It takes time to learn a new way of getting things done.\nThere are ways to do fixed price contracts with Scrum. You just have a few more freedoms to offer to your customer:\nTell your customer that your clients usually change their mind underway. Give them the freedom to change anything not yet implemented. An item can be exchanged with an item of equal cost for no increase in prize. An item can be exchanged with a cheaper item with a decrease of cost, it can be exchanged with a more expensive item for a rise in cost.\nTell your customer that you already have pre-priorized items. The client is free to re-prioritize items as he wishes - as long as the item was not implemented already.\nTell you customer that as you are implementing those items at first that have a high priority you may come to a point where those items not done are not important for release so he could eventually stop early and pay less.\nIn summary the evening was very interesting and insightful for me. It helps to talk about Scrum implementation problems. To learn which problems others have and how they attack these problems. "},{"id":415,"href":"/open-source-development-is-good-for-you316/","title":"Open Source Development is good for you","section":"Inductive Bias","content":" Open Source Development is good for you # GSoC (Google summer of code) - one of the open source programs of Google - has started again in 2009. Students come to work for open source projects during the summer and on success are paid by Google a fair amount of money.\nThis program is an ideal oportunity for students to get into open source projects: You get a mentor, you have pre-defined task to work on with a goal you set yourself. And in the end there is money.\nAt the beginning of GSoC student ranking Ted Dunning posted a very interesting mail on his view on why students should participate in open source development:\nIt is a perfect chance to work together with senior developers that are passionate about what they do.\nUsually universities teach the theoretical side of life, which is good. But if working in industry later, students need experience with current development best practices and tools. They need to be aware of test driven development, they need to know how to use source control systems, continuous integration tools, build management frameworks, bug tracking tools. Open source projects usually are a great place to try out these technologies and learn how to best apply them.\nWorking on open source students need to coordinate with their peers. They need to learn that development is not only about coding, but about communication as well.\nLast but not least this is a chance to chose yourself what you are working on and achieve so much more than when starting yet another brand new single developer project.\nIn the end all this adds up to learning and practicing the skills needed to successfully work on software development projects with more than just a few developers.\n"},{"id":416,"href":"/tomcat-tuesday-talk411/","title":"Tomcat Tuesday talk","section":"Inductive Bias","content":" Tomcat Tuesday talk # Since several months at neofonie we have a talk given by external or internal developers on various subjects each Tuesday. Usually these presentations are a nice way to get an overview of new emerging technologies, to get an overview of current conference topics or to gain insight into interesting internal projects.\nThis week we had Apache Tomcat Committer and PMC Peter Rossbach here at neofonie to talk about the Tomcat architecture and Tomcat clustering solutions. He gave two pretty in-depth presentations on the Tomcat internals, Tomcat optimization and extension points.\nSome points that were especially interesting to me: The project started out in the late nineties, initiated by a bunch of developers who just wanted to see what it takes to write a web application container and that fullfills the spec. The goal basically was a reference implementation. Soon enough however users defined the resulting code as production ready and used it.\nThere are a few caveats from this history that are still visible. One is the lack of tests in the codebase. Sure, each release is tested agains the Sun TCK - but these tests cannot be opened to the general public. So if you as a developer make extensions or modifications to the code base there is no easy way of knowing whether you broke something or not.\nFor me as a developer it was interesting to see really how complex it quickly gets to cluster tomcat deployments and make them failure resistant. Some tools mentioned that help automatic with easier deployment are Puppet and FAI. One issue however that is still on the developer\u0026rsquo;s agenda is Tomcat monitoring.\nTo summarize: The conference room was packed with developers expecting two very interesting talks. Thanks to Peter Rossbach for coming to neofonie and explaining more on the internals of the Tomcat software, the project and the community behind.\n"},{"id":417,"href":"/back-from-zurich109/","title":"Back from Zürich","section":"Inductive Bias","content":" Back from Zürich # I spend the last five days in Zurich. I wanted to visit the city again - and still owed one of my friends there a visit. I am really happy the weather was quite nice over the weekend. That way I could spend quite some time in town (got another one of those puzzles) and go for a hike on the Ütli mountain: I took the steep way up that had quite a lot of stairs. Interestingly though, despite being quite tired when I finally arrived on top, my legs did not have sore muscles the next day. Seems going to work and back again by bike does indeed help a bit, even if we have no hills in Berlin.\nYesterday I was allowed to present the Apache project Mahout in a Google tech talk. Usually I am talking to people well familiar with the various Apache projects. Giving my talk I asked people who was familiar with Lucene, with Hadoop. To me it was pretty unusual that very few engineers were aware of these. It almost seemed like it is unusual to have a look at what is going outside the company? Or was it just the selection of people that were interested in my talk?\nI tried to cover most of the basics, put Mahout into the context of the Lucene umbrella project. I tried to show some of the applications that can be built with Mahout and detailed some of the things that are on our agenda.\nSome of the questions I received were on the scalability of Hadoop, on the general distribution of people being paid to work on Free Software projects vs. those working on them in their freetime. Another question was whether the project is targeted to text only applications (which of course it is not, as feature extraction so far has been left to the user). Last but not least the relation to UIMA was brought up by a former IBM-UIMA engineer.\nTo summarize: For me it was a pretty interesting experience to give this tech talk. I hope it did help me to do away with some of my \u0026ldquo;Apache bias\u0026rdquo;. It is always valuable to look into what is going outside one\u0026rsquo;s community.\n"},{"id":418,"href":"/dima-tu-berlin162/","title":"DIMA @ TU Berlin","section":"Inductive Bias","content":" DIMA @ TU Berlin # On Friday, the 24th of April Prof. Volker Markl organised a Welcome Workshop at TU Berlin. The day started with an introduction by the Dekan of the faculty. First talk was given by Rudolf Bayer on the topic \u0026ldquo;From B-Trees to UB-Trees\u0026rdquo;. Second presentation was by Guy Lohman on \u0026ldquo;LEO, DB2\u0026rsquo;s Learning Optimizer\u0026rdquo;.\nAfter the coffee break, Volker Markl gave an introduction to his selected research field, outstanding tasks and the way he is going to accomplish his goals. Seems like scalability is playing a major role in his tasks. Interestingly Hadoop was chosen as an infrastructure basis.\nIn his talk Volker Markl announced the newly started BBI Colloquium. It is a regular meeting in Berlin dedicated to the scientific discurs on topics relevant to the participating researchers. Participating researchers are Prof. Oliver Günther, Prof. Johann-Christoph Freytag, Prof. Ulf Leser from HU Berlin, Prof. Dr. Volker Markl from TU Berlin, Prof. Dr. Heinz Schweppe from FU Berlin and Prof. Dr. Felix Naumann from HPI Potsdam.\n"},{"id":419,"href":"/scrum-table-with-thoralf-klatt346/","title":"Scrum Table with Thoralf Klatt","section":"Inductive Bias","content":" Scrum Table with Thoralf Klatt # On Wednesday, the 22nd of April, about 20 people interested in Scrum gathered in the DiVino in Friedrichshain/Berlin. The event was split in two parts: In the first half we gathered topics participants were interested in, put priorities next the them and discussed the most highly ranked topic: \u0026ldquo;Scrum in large teams, splitting large tasks across teams.\u0026quot;\nThe basic take home messages of the discussion:\nOne way to cleanly split a task across teams is to first do a design sprint together, fix the API and then split up. Problem with that: Integration and validation of what you do theoretically up front.\nAnother way is to continously integrate all parts, that way you get direct feedback. Might be impractical without a sort of fixed API though.\nDo keep in mind that increasing the team exponentially increases management overhead.\nDo track the progress and performance with well known values (delivered value per sprint, velocity, define KPIs etc.)\nThe second part of the meetup was covered by the talf of Thoralf from Nokia Siemens networks on how they do scrum across countries and continents. Main interessting points for me:\nFace to face communication is necessary - good video equipment can help with that.\nIntegrating ready made products into new solutions create new challenges to solve.\nTransparency and communication with developers become a challenge.\nMore information on the event can be found on the blog of the round table.\n"},{"id":420,"href":"/feedback-from-the-hadoop-user-group-uk172/","title":"Feedback from the Hadoop User Group UK","section":"Inductive Bias","content":" Feedback from the Hadoop User Group UK # A few weeks after the Hadoop User Group UK is over, there are quite a few postings on the event online. I will try to keep this page updated if there are any further reviews. The one I found so far:\nhttp://huguk.org/2009/04/huguk-2-wrap-up.html - the wrap-up of the event itself.\nhttp://blog.oskarsson.nu/2009_04_01_archive.html - a short summary by the organiser - Thanks again for a great event.\nhttp://www.cloudera.com/blog/2009/04/21/hadoop-uk-user-group-meeting/ - a short summary on the Cloudera blog.\nhttp://people.kmi.open.ac.uk/adam/?p=26 - a quick overview with a Mahout focus by Adam Rae.\n"},{"id":421,"href":"/apache-hadoop-get-together-berlin247/","title":"June 2009 Apache Hadoop Get Together @ Berlin","section":"Inductive Bias","content":" June 2009 Apache Hadoop Get Together @ Berlin # Title: Apache Hadoop Get Together @ Berlin\nLocation: newthinking store Berlin Mitte\nLink out: Click here\nDescription: I just announced the fifth Apache Hadoop Get Together in Berlin at the newthinking store. Torsten Curdt offered to give a talk on data serialization with Thrift and Protocol Buffers.\nIf you have a topic you would like to talk about: Feel free to just bring your slides - there will be a beamer and lots of people interested in scalable information retrieval.\nStart Time: 17:00\u0026lt;br /\u0026gt;Date: 2009-06-25\n"},{"id":422,"href":"/mahout-on-ec2281/","title":"Mahout on EC2","section":"Inductive Bias","content":" Mahout on EC2 # Amazon released Elastic Map Reduce only a few weeks ago. EMR is based on a hosted Hadoop environment and offers machines to run map reduce jobs against data in S3 on demand.\nLast week Stephen Green has spent quite some effort to get Mahout running on EMR. Thanks to his work Mahout is running on EMR since last Thursday night. Read the weblog of Tim Bass for further information.\n"},{"id":423,"href":"/hadoop-user-group-uk-2226/","title":"Hadoop User Group UK","section":"Inductive Bias","content":" Hadoop User Group UK # On Tuesday the 14th the second Hadoop User Group UK took place in London. This time venue and pizza was sponsored by Sun. The room quickly filled http://www.thecepblog.com/2009/04/19/kmeans-clustering-now-running-on-elastic-mapreduce/with approximately 70 people.\nTom opened the session with a talk on 10 practical tips on how to get the most benefit from Apache Hadoop. The first question users should ask themselves is which type of programming language they want to use. There is a choice between structured data processing languages (PIG or Hive), dynamic languages (Streaming or Dumbo), or using Java which is closest to the system.\nTom\u0026rsquo;s second hint dealt with the size of files to process with Hadoop: Both\ntoo large unsplittable and too small ones are bad for performance. In the first case, the workload cannot be easily distributed across the nodes in the latter case each unit of work is to small to account for startup and coordination overhead. There are ways to remedy these problems with sequence files and map files though. Another performance optimization would be to chain individual jobs - PIG and Hive do a pretty decent job in automatically generating such jobs. ChainMapper and ChainReducer can help with creating chained jobs.\nAnother important task when implementing map reduce jobs is to tell Hadoop the progress of your job. For once, this is important for long running jobs in order for them to remain alive and not be killed by the framework due to timeouts. Second, it is convenient for the user as he can view the progress in the web UI of Hadoop.\nUsual suspects for tuning a job: Number of mappers and reducers, usage of combiners, compression customised data serialisation, shuffling tweaks. Of course there is always the option to let someone else do the tuning: Cloudera does provide support as well as pre-built packages init scripts and the like ;)\nIn the second talk I did a brief Mahout intro. It was surprising to me that half of the attendees already employed machine learning algorithm implementations in their daily work. Judging from the discussion after the talk and from questions I received after it the interest in the project seems pretty high. The slide I liked the most: The announcement of our first 0.1 release. Thanks to all Mahout committers and contributors who made this possible.\nAfter the coffee break Craig gave an introduction to Terrier an extensible information retrieval plattform developed at the university of Glasgow. He mentioned a few other open IR platforms namely Tuple Flow, Zettair, Lemur/Indri, Xapian, as well as of course nutch/Solr/Lucene.\nWhat does Terrier have to do with the HugUK? Well index creation in Terrier is now based on an implementation that makes use of Hadoop for parallelization. Craig did some very interesting analysis on scalability of the solution: The team was able to achieve scaling near linear in the number of nodes added (at least as long as more than reducer is used ;) ).\nAfter the pizza Paolo described his experiences implementing the vanilla pagerank computation with Hadoop. One of his test datasets was the Citeseer citation graph. Interestingly enough: Some of the nodes in this graph have self references (maybe due to extraction problems), duplicate citations, and the data comes in an invalid xml format.\nThe last talk was on HBase by Michael Stack. I am really happy I attended HugUK as I missed that talk in Amsterdam at the ApacheCon. First Michael gave an overview of which features of a typical RDBMS are not supported by HBase: Relations, joins, and of course JDBC being among the limitations. On the pro site HBase offers a multiple node solutions that has scale out and replication built in.\nHBase can be used as source as well as as sink for map reduce jobs and thus integrates nicely with the Apache Hadoop stack. The framework provides a simple shell for administrative tasks (surgery on sick clusters forced flushes non sql get scan and put methods). In addition the master comes with a UI to monitor the cluster state.\nYour typical DBA work though differs with HBase: Data locality and physical layout do matter and can be configured. Michaels recommendation was to start out testing with the XL instance on EC2 and decrease instances if you find out that it is too large.\nThe talk finished with an outlook of the features in the upcoming release the issues on the todo list and an overview of companies already using HBase. After talks were finished quite a few attendees went over to a pub close by: Drinking beer, discussing new directions and sharing war stories.\nI would to thank Johan Oskarsson for organising the event. And a special thanks to Tom for letting me use his Laptop for the Apache Mahout presentation: the hard disk of mine broke exactly one day before.\nLast but not least thank you to Sylvio and Susi for letting me stay at their place - and thanks to Helene for crying only during daytime when I was out anyway ;)\nHope to see at least some of the attendees again at the next Hadoop Meetup in Berlin. Looking forward to the next Hadoop User Group UK. "},{"id":424,"href":"/announcing-apache-mahout-0110/","title":"Announcing Apache Mahout 0.1","section":"Inductive Bias","content":" Announcing Apache Mahout 0.1 # This morning I received Grant\u0026rsquo;s release mail of Apache Mahout. I am really happy that after little more than one year we now have our first release out there to test and scrutinate by anyone interested in the project. Thanks to all the committers who have helped make this possible. A special thanks to Grant Ingersoll for putting so much time into getting many release issues out of the way as well as to those who reviewed the release candidates and all the major and minor problems.\nFor those who are not familiar with Mahout: The goal of the project is to build a suite of machine learning libraries under the Apache license. The main focus is on:\nBuilding a viable community that develops new features, helps users with software problems and is interested in the data mining problems Mahout users.\nDeveloping stable, well documented, scalable software that solves your problems.\nThe current release includes several algorithms for clustering (k-Means, Canopy, fuzzy k-Means, Dirichlet based), for classification (Naive Bayes and Complementary Naive Bayes). There is some integration with the Watchmaker evolutionary programming framework. The Taste Collaborative Filtering framework moved to Mahout as well. Taste has been around for a while and is much more mature than the rest of the code.\nWith this being a 0.1 release we are looking for early adopters that are willing to work with cutting edge software and gain benefits from working closely together with the community. We are seeking feedback on use cases as well as performance numbers. If you are using Mahout in your projects or plan to use it or even only evaluate it - we are happy about hearing back from you on our mailing lists. Tell us what you like, what works well, but do not forget to tell us what you would like to improve. Contributions and Patches as always are very welcome.\nFor more information see the project homepage, especially the wiki and the Lucene weblog by Grant Ingersoll.\n"},{"id":425,"href":"/gsoc-student-applications-closed220/","title":"GSoC: Student applications.","section":"Inductive Bias","content":" GSoC: Student applications. # Title: GSoC: Student applications closed\nLink out: Click here\nDescription: After this date no more student applications are accepted. Internal ranking at Apache starts 7 days earlier. The ranking process finishes at 16th of April.\nDate: 2009-04-03\n"},{"id":426,"href":"/students-begin-coding-for-their-gsoc-projects1/","title":"Students begin coding for their GSoC projects","section":"Inductive Bias","content":" Students begin coding for their GSoC projects # Title: Students begin coding for their GSoC projects\nLink out: Click here\nDescription: GSoC time.\nDate: 2009-05-26\n"},{"id":427,"href":"/apache-con-2009-part-320/","title":"Apache Con Europe 2009 - part 3","section":"Inductive Bias","content":" Apache Con Europe 2009 - part 3 # Friday was the last conference day. I enjoyed the Apache pioneers panel with a brief history of the Apache Software Foundation as well as lots of stories on how people first got in contact with the ASF.\nAfter lunch I went to the testing and cloud session. I enjoyed the talk on continuum and its uses by Wendy Smoak. She gave a basic overview of why one would want a CI system and provided a brief introduction to continuum. After that Carlos Sanchez showed how to use the cloud to automate interface tests with Selenium: The basic idea is to automatically (initiated through maven) start up AMIs on EC2, each configured with another operating system and run Selenium tests against the application under development in these. Really nice system for running automated interface tests.\nThe final session for me was the talk by Chris Anderson and Jan Lehnardt on CouchDB deployments.\nThe day ended with the Closing Event and Raffle. Big Thank You to Ross Gardler for including the Berlin Apache Hadoop Get Together in newthinking store in the announcements! Will sent the CfP to concom really soon, as promised. Finally I won one package of caffeinated sweets at the Raffle - does that mean less sleep for me in the coming weeks?\nNow I am finally back home and had some time to do a quick writeup. If you are interested in the complete notes, go to http://fotos.isabel-drost.de (default login is published in the login window). Looking forward to the Hadoop User Group UK on 14th of April. If you have not signed up yet - do so now: http://huguk.org\n"},{"id":428,"href":"/apache-con-europe-2009-part-219/","title":"Apache Con Europe 2009 - part 2","section":"Inductive Bias","content":" Apache Con Europe 2009 - part 2 # Thursday morning started with an interesting talk on open source collaboration tools and how they can help resolving some collaboration overhead on commercial software projects. Four goals can be reached with the help of the right tools: Sharing the project vision, tracking the current status of the project, finding places to help the project and documenting the project history as well as the reasons for decisions along the way. The exact tool used is irrelevant as long as it helps to solve the four tasks above.\nThe second talk was on cloud architectures by Steve Loughran. He explained what reasons there are to go into the cloud, what a typical cloud architecture looks like. Steve described Amazon\u0026rsquo;s offer, mentioned other cloud service providers and highlighted some options for a private cloud. However his main interest is in building a standardised cloud stack. Currently choosing one of the cloud provides means vendor lock-in: Your application uses a special API, your data are stored on special servers. There are quite a few tools necessary for building a cloud stack available at Apache (Hadoop, HBase, CouchDB, Pig, Zookeeper\u0026hellip;). The question that remains is how to integrate the various pieces and extend where necessary to arrive at a solution that can compete with AppEngine or Azure?\nAfter lunch I went to the Solr case study by JTeam. Basically one great commercial for Solr. They even brought the happy customer to Apache Con to talk about the experience of Solr from his point of view. Great work, really!\nThe Lightning Talk session ended the day - with a \u0026ldquo;Happy birthday to you\u0026rdquo; from the community.\nAfter having spent the last 4 days from 8a.m. to 12p.m. at Apache Con I really did need some rest on Thursday and went to bed pretty early: At 11p.m. \u0026hellip;\n"},{"id":429,"href":"/apache-con-europe-2009-part-118/","title":"Apache Con Europe 2009 - part 1","section":"Inductive Bias","content":" Apache Con Europe 2009 - part 1 # The past week members, committers and users of Apache software projects gathered in Amsterdam for another Apache Con EU\nand to celebrate the 10th birthday of the ASF. One week dedicated to the development and use of Free Software and the Apache Way.\nMonday was BarCamp day for me, the first BarCamp I ever attended. Unfortunately not all participants proposed talks. So some of the atmosphere of an unconference was missing. The first talk by Danese Cooper was on \u0026ldquo;HowTo: Amsterdam Coffee Shops\u0026rdquo;. She explained the ins and outs of going to coffee shops in Amsterdam, gave both legal and practical advise. There was a presentation of the Open Street Map project, several Apache projects. One talk discussed transfering the ideas of Free Software to other parts of life. Ross Gardler started a discussion on how to advocate contributions to Free Software projects in science and education.\nTuesday for me meant having some time for Mahout during the Hackathon. Specifically I looked into enhancing matrices with meta information. In the evening there were quite a few interesting talks at the Lucene Meetup: Jukka gave an overview of Tika, Grant introduced Solr. After Grant\u0026rsquo;s talk some of the participants shared numbers on their Solr installations (number of documents per index, query volumn, machine setup). To me it was extremely interesting to gain some insight into what people actually accomplish with Solr. The final talk was on Apache Droids, a still incubating crawling framework.\nThe Wednesday tracks were a little unfair: The Hadoop track (videos available online for a small fee) was right in parallel to the Lucene track. The day started with a very interesting keynote by Raghu from Yahoo! on their storage system PNUTS. He went into quite some technical detail. Obviously there is interest in publishing the underlying code under an open source license.\nAfter the Mahout introduction by Grant Ingersoll I changed room to the Hadoop track. Arun Murthy shared his experience on tuning and debugging Hadoop applications. After lunch Olga Natkovich gave an introduction to Pig - a higher language on top of Hadoop that allows for specifications of filter operations, joins and basic control flow of map reduce jobs in just a few lines of Pig Latin code. Tom White gave an overview of what it means to run Hadoop on the EC2 cloud. He compared several options for storing the data to process. Today it is very likely that there will soon be quite a few more providers of cloud services in addition to Amazon.\nAllen Wittenauer gave an overview of Hadoop from the operations point of view. Steve Lougran finally covered the topic of running Hadoop on dynamically allocated servers.\nThe day finished with a pretty interesting BOF on Hadoop. There still are people that do not clearly see the differences of Hadoop based systems to database backed applications. Best way to find out whether the model fits: Set up a trial cluster and do experiment yourself. Noone can tell which solution is best for you except for yourself (and maybe Cloudera setting up the cluster for you :) ).\nAfter that the Mahout/UIMA BOF was scheduled - there were quite a few interesting discussions on what UIMA can be used for and how it integrates with Mahout. One major take home message: We need more examples integrating both. We developers do see the clear connections. But users often do not realize that many Apache projects should be used together to get the biggest value out. "},{"id":430,"href":"/cloud-camp-berlin141/","title":"Cloud Camp Berlin","section":"Inductive Bias","content":" Cloud Camp Berlin # Title: Cloud Camp Berlin\nLink out: Click here\nDate: 2009-04-30\u0026lt;br /\u0026gt;\n"},{"id":431,"href":"/fsfe-booth-at-the-chemnitzer-linux-tage199/","title":"FSFE booth at the Chemnitzer Linux Tage","section":"Inductive Bias","content":" FSFE booth at the Chemnitzer Linux Tage # This year for the 11th time the \u0026ldquo;Linux Tage\u0026rdquo; were organized at the university of Chemnitz. Each year in March this means two days devoted to the topic of open and free software. It means an event that is very well organized by a pretty professional team of volunteers.\nFor the third time the FSFE had its booth at the event - this time run by Rainer Kersten, Uwe Zemisch and me. Recurring questions at the booth were\n\u0026ldquo;What the hack is FSFE and in which ways do you actually support free software?\u0026quot;\n\u0026ldquo;I am already fellow, you keep telling me there are these great fellowship meetups. Do you know whether there is one near my town? How do these events start? How are they organized?\u0026quot;\nIt was interesting to see that FSFE is one of the few organizations that try to fill the gap between those writing open source software and those actually making decisions that are relevant to the developers but know nothing of writing software whatsoever.\nBesides running the booth there was some time left for a few talks. I decided to go to the OpenMP talk. The idea is to develop a highlevel API for marking code passages for parallel execution. It is not designed for parallel programming on clusters but on multi core machines. Somewhat related to the Java concurrency package but far more high level.\nThe second talk I went to was on personal data protection laws in Germany. One funny piece of information: Even the ministry of justice was sued sucessfully for storing to much information on the visitors of its webpage.\nLast talk I went to was on Google Android. To me it looks like a nice mix of completly open source (like Open Moko) and completely closed source. If you need a phone you can use for making phone calls but still want to play with it and be root on the phone (399$ for the dev phone, sim unlocked, only available for registered developers, registration is 25,-$), Android G1 propably is the way to go. The phone is highly integrated with Google applications. The assumption when building it seems to have been, that people are online all the time with that phone.\nFor coding: Only a Java API is available, no C or C++. SDK is available for Lin/Mac/Win. The emulator does work, only thing it does not reflect is the real speed of the device itself. Each app gets its own VM, Dalvik supports process memory sharing that makes that less expensive. In case of memory shortage apps are killed in order of user impact (empty/precreated, background, service (mp3 player), visible apps, foreground apps). Idea is to kill those apps that are least visible to the user. The programmer needs to take care that apps constantly store state so restarting them gets them up in the same state they were in when killed.\nMore information online: http://www.htc.com; adoid.git.kernel.org;\nAll in all: Really nice weekend. Looking forwared to return next year.\n"},{"id":432,"href":"/books-i-found-particularly-helpful128/","title":"Books I found particularly helpful","section":"Inductive Bias","content":" Books I found particularly helpful # During the last few years I have quite a few books that one could easily file under the category \u0026ldquo;Hacking books\u0026rdquo;. Some of them were particularly interesting to me and have influenced the way I write code. The following list certainly is not complete at all - but it is a nice starting point.\nEffective C++ - I have comparably little experience with C++ but this book really helped understand some of the particularities.\nEffective Java - even though I have been developing in Java since a few years reading and revisiting Effective Java helps understanding and dealing with some of the quirks of the JVM.\nMythical Man Month - although classical literature for people dealing with software projects, although very well known, although easy to understand it is scaring to see that the exact same mistakes are still common in today\u0026rsquo;s software projects.\nConcurrent programming in Java - quick start on concurrent programming patterns - primarily focussed on Java. Fortunately no collection of recipes but thorough background information.\nWorking effectively with legacy code - I really like to have a look into this book from time to time. Shows great ways of untangling bad code, refactoring it and making it testable.\nXP books by Kent Beck - if you ever had any questions on what XP programming is and how you should implement it: These are the books to read. Don\u0026rsquo;t trust what people call XP in practice as long as they are not willing to refine and improve their \u0026ldquo;agile processes\u0026rdquo;. Keep on working on what stops you from delivering great code.\nWhy programs fail - a guide to systematic debugging - If you ever had to debug complex programs - and I bet you had - this is the book that explains how to do this systematically. How to even have fun along the way.\nZen and the art of motorcycle maintenance - Not particularly on Software Development but the techniques described match stunningly well on software development.\nRelease It! - just about to read that one. But already the first few pages are not only valuable and interesting but also entertaining.\nImplementation Patterns - forgot that yesterday.\nPresentation Zen - another one I forgot. Really helped me to make better presentations.\nThere are still quite a few good books on my list. If you have any recommendations - please leave them in the comments.\nThere are a few other book lists online in various blogs. Two examples are the ones below:\nhttp://www.codinghorror.com/blog/archives/000020.html\nhttp://www.joelonsoftware.com/navLinks/fog0000000262. html\n"},{"id":433,"href":"/scrum-roundtable-berlin344/","title":"Scrum Roundtable Berlin","section":"Inductive Bias","content":" Scrum Roundtable Berlin # Title: Scrum Roundtable Berlin\nLocation: DiVino Restaurant, Grünberger Str. 69, Friedrichshain\nLink out: Click here\u0026lt;br /\u0026gt;Start Time: 18:00\nDate: 2009-04-22\nThe next Scrum Roundtable is scheduled already. Thoralf, will present his speech from the Orlando ScrumGathering\nAgile Creation of Multi-Product Solutions\nMotivation for Network Solutions and their Agilility\nScaling Single Product Creation\nProduct Solutions using Scrum\nCustomizing Projects\nOutlook\nPlease let Marion know if you are coming to be able to organise the space.\nPlease find more information on upcoming events and the organization of the Scrum roundtable at: http://www.agile42.com/cms/blog/categories/scrumtisch/\n"},{"id":434,"href":"/erlang-user-group-scala169/","title":"Erlang User Group - Scala","section":"Inductive Bias","content":" Erlang User Group - Scala # What: Scala Presentation by Stefan Plantikow.\nWhere: Cockpit of the Box119 http://boxhagener119.de/ (Ring at UPSTREAM)\nWhen: Wednesday, 11.03.2009, 8:00 p.m. Yesterday the Erlounge, organised by Jan Lehnardt, took place in the Cockpit of Box119 in Berlin. Topic of the evening was an introduction to Scala.\nScala is a functional language that compiles to Java Bytecode and runs on the JVM. It tries to combine the best from two worlds: Object oriented languages and functional programming. So every function is an object and every object is a function.\nSome interesting bits of information: Scala is a statically typed language - but you can omit the types most of the times as type inference in the compiler is pretty good.\nEverything is an object - there is no difference in primitives and objects.\nThere are packages for distributed computing - spawning processes and sending messages is not as fast as in Erlang there is still room for improvement.\nThe developers are currently about to tidy up the syntax and take care of corner cases.\nIt is easy to start with Scala as you can start out with a subset of the language and extend your knowledge as you need.\nScala means Scalable language. Scalable in terms of projects and tasks you can accomplish with it.\nIf you want to see a second nice presentation that is slightly less focussed on comparing Scala to Erlang you might also find this year\u0026rsquo;s FOSEM presentation interesting: http://www.slideshare.net/Odersky/fosdem-2009-1013261 (video should be up soon as well).\n"},{"id":435,"href":"/basic-statistics-of-a-set-of-values110/","title":"Basic statistics of a set of values","section":"Inductive Bias","content":" Basic statistics of a set of values # Just in order to find that when searching for it yet another time:\nProblem: You have a set of values (for instance time it took to process various queries). You want a quick overview of how the values are distributed.\nSolution: Store the values in a file separated by newline, read the file with R and output summary statistics.\nR: times \u0026lt;- scan(\u0026ldquo;times\u0026rdquo;, list(0))\nRead 30000 records\nR: summary(times[[1]])\nMin. 1st Qu. Median Mean 3rd Qu. Max.\n6.00 12.00 13.00 16.75 14.00 8335.00\nThat\u0026rsquo;s it.\n"},{"id":436,"href":"/scrum-home340/","title":"Scrum @ Home","section":"Inductive Bias","content":" Scrum @ Home # Scrum has proven to be a suitable toolset for managing software projects: Large, unmanageable tasks are broken up into little pieces that can be easily estimated in terms of complexity or time necessary for implementation. During a fixed amount of time, several of these tasks are implemented. By looking back at previous iterations it is simple to predict exactly how many items can be expected to be ready after the next iteration. Given complexity is estimated it is also easy to evaluate the business value of each item.\nIf Scrum is a method suitable for managing tasks - Thilo and me asked ourselves: Why not use it to manage tasks such as \u0026ldquo;Shopping\u0026rdquo;, \u0026ldquo;Laundry\u0026rdquo;, \u0026ldquo;Prepare Slides\u0026rdquo;, \u0026ldquo;Vakuuming\u0026rdquo;? So we simply tried it out: We set up a whiteboard with the typical Scrum board layout. Each task to do for the upcoming week was noted on a little yellow post it, annotated with a complexity and sticked to the board.\nThe funny thing is: The experiment did work out really well. There are a few lessons learned as well:\nAnnotating the complexities helped answering the question \u0026ldquo;Where the hack did my freetime go to this week?\nAfter like three iterations (in our case iteration length is one week) we knew exactly when we had planned for too much and were able to throw out items of lower priority.\nMaking all those tiny little tasks visible suddenly helped in delegating tasks.\nHaving a board with all post its moved to the right does help motivation :)\nIn the meantime we found out it helps to color-code the post-it notes: Tasks are yellow, going out red, meetings with friends blue and so on and so forth. Otherwise it happens all too easy to forget about to relax. I included a foto of our Scrum board in this blog post. It was taken some time in the middle of the week. Currently, only the fun pieces are left for Sunday :) "},{"id":437,"href":"/hadoop-user-group-uk225/","title":"Hadoop User Group UK","section":"Inductive Bias","content":" Hadoop User Group UK # Title: Hadoop User Group UK\nLocation: London/ Sun Office\nLink out: Click here\nDate: 2009-04-14\u0026lt;br /\u0026gt;\nJohan Oskarsson is organising the second Hadoop User Group UK in London in April this year. The schedule is already up:\nTom White (Cloudera): Practical MapReduce\nMichael Stack (Powerset): Apache HBase\nIsabel Drost (ASF): Introducing Apache Mahout\nIadh Ounis and Craig Macdonald (University of Glasgow): Terrier\nPaolo Castagna (HP): \u0026ldquo;Having Fun with PageRank and MapReduce\u0026rdquo;\nTickets are free but limited. So better register soon. Looking forward to seeing you there.\n"},{"id":438,"href":"/fsfe-meetup-berlin201/","title":"FSFE Meetup Berlin","section":"Inductive Bias","content":" FSFE Meetup Berlin # Title: FSFE Meetup Berlin\nLocation: newthinking store\nStart Time: 19:00\nDate: 2009-03-12\nThe FSFE Berlin group is going to meet next Thursday. Topics to discuss at the next Get Together are\nDocument Freedom Day activities.\nThis year is the year of elections in Germany. How do we get the topic of Free Software into the discussion?\nFeel free to visit the wiki, drop by and take part in the discussions. After the meetup we will go to some restaurant close by for food and drinks. "},{"id":439,"href":"/scrum-discussions-in-berlin342/","title":"Scrum discussions in Berlin","section":"Inductive Bias","content":" Scrum discussions in Berlin # As software development cycles become ever shorter more and more companies adopt agile development methods. In many cases companies have switched to using Scrum or are still transitioning from less flexible approaches.\nSince last summer Marion Eickmann is organising a Scrum Roundtable in Berlin. The goal is to discuss advantages and problems when introducing and practicing Scrum. But discussions on Scrum itself are also welcome.\nLast Tuesday Thilo Fromm gave a presentation on his experiences introducing Scrum in his project. Thilo is project manager and developer at a company developing video surveillance hardware and software. He first gave an overview of the development process traditionally used in the company. Then he described how projects changed over the course of the last few years and how this caused new challenges to software development. Finally Thilo explained the status of implementing Scrum in one of his projects. He finished with (very positive reactions) from various teams in his company.\nSlides are available online: Thilo Fromm: Scrum in the Waterfall\nThe talk was followed by very interesting discussions on this exact implementation of Scrum as well as experiences of other participants using Scrum for their projects. Discussions were accompanied by really great italian food.\nThe next roundtable will be announced on the blog of Agile42.\n"},{"id":440,"href":"/hadoop-get-together-berlin283/","title":"March 2009 Hadoop Get Together Berlin","section":"Inductive Bias","content":" March 2009 Hadoop Get Together Berlin # Since last summer, newthinking store Berlin is hosting a Hadoop Meetup every quarter of the year. The scope of these user group meetings is not only on Hadoop projects but deals with technologies necessary with storing, processing and searching large amounts of data.\nThe meeting last Thursday featured a talk by Lars George on his experiences using HBase in customer projects as early as in 2007. His talk discussed his requirements for a distributed database. He then explained the basics of HBase and described his experiences using the software for customer projects. Bottom line for me is that although in a very early stage the project does provide a lot of value: Instead of re-implementing your own solution it is possible to benefit from the efforts of others. One thing I consider especially remarkable is the effort of the HBase community helping users in case they run into problems.\nThe second talk was from Jan Lehnardt on CouchDB. Jan explained the main design goals of the system. He detailed the architecture of CouchDB. Then he explained how Erlang made it possible to reach the goals in comparably short time.\nThe slides of the talks are both available online:\nLars George: HBase\nJan Lehnardt: CouchDB\nThe talks were followed by several questions and interesting discussions (with some beer in the Keyser Soze close by).\nThe next Get Together will be held in June 2009. Looking forward to see you in Berlin by then.\n"},{"id":441,"href":"/about6/","title":"About","section":"Inductive Bias","content":" About # This is the blog of Isabel. I am committer at Apache Mahout. In my free time I am working on Apache Mahout, organising the Apache Hadoop Get Together in Berlin and speaking at various conferences explaining the ins and outs of Hadoop in general and Mahout in particular.\nDisclaimer: I am writing this blog with my “Apache hat” on my head. The opinions expressed herein are my own personal opinions and do not represent my employer’s view in any way. Except when explicitly stated otherwise activities and events described are not related to my daytime job.\n"},{"id":442,"href":"/about/","title":"About","section":"Inductive Bias","content":" About this blog # This is the blog of Isabel. I\u0026rsquo;m a member of the Apache Software Foundation, co-founder of Apache Mahout and mentored several incubating projects. I\u0026rsquo;m interested in all things FOSS, search and text mining with a decent machine learning background. True to the nature of people living in Berlin I love having friends here for a brief visit - as a result I co-founded and am still one of the creative heads behind Berlin Buzzwords, a tech conference on all things search, scale and storage as well as FOSS Backstage.\n"}]