---
title: "FrOSCon 2018"
date: 2018-08-29T18:34:49+02:00
tags: [FrOSCon, conferences, Open Source]
categories: [Hacking people]
---

# FrOSCon 2018

A more general summary: https://tech.europace.de/froscon-2018/ of the
conference written in German. Below a more detailed summary of the keynote by
Lorena Jaume-Palasi.

In her keynote "Blessed by the algorithm - the computer says no!" Lorena detailed the intersection of ethics and 
technology when it comes to automated decision making systems. As much as humans with a technical training shy away 
from questions related to ethics, humans trained in ethics often shy away from topics that involve a technical layer. 
However as technology becomes more and more ingrained in everyday life we need people who understand both - tech and 
ethical questions.

Lorena started her talk detailing how one typical property of human decision making involves inconsistency, otherwise 
known as noise: Where machine made decisions can be either accurate and consistent or biased and consistent, human 
decisions are either inconsistent but more or less accurate or inconsistent and biased. Experiments that showed this 
level of inconsistency are plenty, ranging from time estimates for tasks being different depending on weather, mood, 
time of day, being hungry or not up to judges being influenced by similar factors in court.

One interesting aspect: While in order to measure bias, we need to be aware of the right answer, this is not necessary 
for measuring inconsistency. Here's where monitoring decisions can be helpful to palliate human inconsistencies.

In order to understand the impact of automated decision making on society one needs a framework to evaluate that - the 
field of ethics provides multiple such frameworks. Ethics comes in three flavours: Meta ethics dealing with what is 
good, what are ethical requests? Normative ethics deals with standards and principles. Applied ethics deals with 
applying ethics to concrete situations.

In western societies there are some common approaches to answering ethics related questions: Utilitarian ethics asks 
which outputs we want to achieve. Human rights based ethics asks which inputs are permissible - what obligations do we 
have, what things should never be done? Virtue ethics asks what kind of human being one wants to be, what does 
behaviour say about one's character? These approaches are being used by standardisation groups at e.g. DIN and ISO to 
answer ethical questions related to automation.

For tackling ethics and automation today there are a couple viewpoints, looking at questions like finding criteria 
within the context of designing and processing of data (think GDPR), algorithmic transparency, prohibiting the use of 
certain data points for decision making. The importance of those questions is amplified now because automated decision 
making makes it's way into medicine, information sharing, politics - often separating the point of decision making from 
the point of acting. One key assumption in ethics is that you should always be able to state why you took a certain 
action - except for actions taken by mentally ill people, so far this was generally true. Now there are many more 
players in the decision making process: People collecting data, coders, people preparing data, people generating data, 
users of the systems developed. For regulators this setup is confusing: If something goes wrong, who is to be held 
accountable? Often the problem isn't even in the implementation of the system but in how it's being used and deployed. 
This confusion leads to challenges for society: Democracy does not understand collectives, it understands individuals 
acting. Algorithms however do not understand individuals, but instead base decisions on comparing individuals to 
collectives and inferring how to move forward from there. This property does impact individuals as well as society.

For understanding which types of biases make it into algorithmic decision making systems that are built on top of human 
generated training data one needs to understand where bias can come from:

The uncertainty bias is born out of a lack of training data for specific groups amplifying outlier behaviour, as well 
as the risk for over-fitting. One-sided criteria can serve to reinforce a bias that is generated by society: Even 
ruling out gender, names and images from hiring decisions a focus on years of leadership experience gives an advantage 
to those more likely exposed to leadership roles - typically neither people of colour, nor people from poorer 
districts. One-sided hardware can make interaction harder - think face recognition systems having trouble identifying 
non-white humans, having trouble identifying non-male humans.

In the EU we focus on the precautionary principle where launching new technology means showing it's not harmful. This 
though proves more and more complex as technology becomes entrenched in everyday life.


What other biases do humans have? There's information biases, where humans tend to reason based on analogy, based on 
the illusion of control (overestimating oneself, downplaying risk, downplaying uncertainty), there's an escalation of 
committment (a tendency to stick to a decision even if it's the wrong one), there are single outcome calculations.

For cognitive biases are related to framing, criteria selection (we tend to value quantitative criteria over 
qualitative criteria), rationality. There's risk biases (uncertainties about positive outcomes typically aren't seen as 
risks, risk tends to be evaluated by magnitude rather than by a combination of magnitude and probability). There's 
attitude based biases: In experiments senior managers considered risk taking as part of their job. The level of risk 
taken depended on the amount of positive performance feedback given to a certain person: The better people believe they 
are, the more risk they are willing to take. Uncertainty biases relate to the difference between the information I 
believe I need vs. the information available - in experiments humans made worse decisions the more data and information 
was available to them.

General advise: Beware of your biases...
